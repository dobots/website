<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
<channel>
        <title>DoBots</title>
        <description>DoBots - Distributed Organisms B.V.</description>
        <link>https://dobots.nl
        <link>https://dobots.nl
        <lastbuilddate>2018-03-16T10:43:03+00:00</lastbuilddate>
        <pubdate>2018-03-16T10:43:03+00:00</pubdate>
        <ttl>1800</ttl>


        <item>
                <title>Dimming random devices with the Crownstone</title>
                <description>
&lt;h1 id="dimming-random-devices-with-the-crownstone"&gt;Dimming random devices with the Crownstone&lt;/h1&gt;

&lt;p&gt;Just before we started moving to the new office, we tortured some random devices. Instead of just switching them on and off, we wanted to see what happened if they are dimmed. The Crownstone dims via &lt;a href="https://www.arduino.cc/en/Tutorial/PWM"&gt;PWM&lt;/a&gt;, which is basically switching on and off very fast. In our case it was switched at 625 Hz.&lt;/p&gt;

&lt;p&gt;Unfortunately, we already threw away a lot of old devices as preparation for the move, but we still had a couple of devices to test on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Switching power supply&lt;/li&gt;
  &lt;li&gt;PC speakers&lt;/li&gt;
  &lt;li&gt;Monitor&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We made a short &lt;a href="https://youtu.be/y1rjwLoWbDg"&gt;video&lt;/a&gt; of the session.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/y1rjwLoWbDg?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;p&gt;To our surprise, none of the devices were permanently damaged. This is hopeful for us, as it means less chance that a user destroys their equipment by accidentally dimming a device that shouldn’t be dimmed (something that should not be possible, but you never know).&lt;/p&gt;

&lt;p&gt;The monitor seemed to have the most trouble with PWM; it started to make a high pitched noise, and at when going from off to 3%, random green pixels appeared on the screen. This green noise stayed until we rebooted both the Raspberry Pi and the monitor.&lt;/p&gt;

&lt;p&gt;Another interesting thing was the current consumption of the monitor. In normal operation, the monitor consumes current in short bursts, 100 times per second. You can see this in the image below, where a graph of voltage (blue) and current (yellow) is shown over 0.02 seconds (1 period of AC power).&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/pwm-monitor-100.png" alt="Voltage and current curve of a monitor in normal operation" title="Voltage and current curve of a monitor in normal operation" /&gt;&lt;/p&gt;

&lt;p&gt;For a duty cycle of 50%, you start to see very different behavior, you see more peaks of current consumption. It’s likely that the power supply in the monitor draws current each time PWM is in the &lt;em&gt;on&lt;/em&gt; state, which happens about 12 times as often as for the 50 Hz AC. Since the monitor still needs the same amount of power, the current peaks at higher levels (because power = voltage * current summed over time, see Pavg at &lt;a href="https://en.wikipedia.org/wiki/AC_power#Real_number_formulas"&gt;wikipedia&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/pwm-monitor-50.png" alt="Voltage and current curve of a monitor at 50% PWM" title="Voltage and current curve of a monitor at 50% PWM" /&gt;&lt;/p&gt;

&lt;p&gt;At 3% duty cycle, the peaks are even shorter.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/pwm-monitor-3.png" alt="Voltage and current curve of a monitor at 3% PWM" title="Voltage and current curve of a monitor at 3% PWM" /&gt;&lt;/p&gt;

&lt;p&gt;If you also performed similar experiments, or if you have better explanations, feel free to leave it in the comments!&lt;/p&gt;
</description>
                <link>https://dobots.nl/2016/04/20/dimming-random-devices-with-the-crownstone
                <guid>https://dobots.nl/2016/04/20/dimming-random-devices-with-the-crownstone</guid>
                <pubdate>2016-04-20T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Inertia energy balance management system</title>
                <description>
&lt;h1 id="load-balancing-on-the-grid"&gt;Load-balancing on the grid&lt;/h1&gt;

&lt;p&gt;&lt;a href="http://www.inertia-project.eu/inertia/"&gt;Inertia&lt;/a&gt; is an FP7 project that aims at expressing local energy use/production elasticity in a negotiation system that utilizes the published “flexibility” (=elasticity) to produce a balanced load to the energy production counterpart. The focus of the project is tertiary buildings, but the system is just as useful for any built environment and facilitates the changeover to more local production of electricity by home-owners and facilities alike.&lt;/p&gt;

&lt;p&gt;Flexibility can be aggregated from the level of a controlled light upto a complete set of buildings, and is expressed in four main categories to guard against too much privacy loss. The categories are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;HVAC (thermal and air comfort),&lt;/li&gt;
  &lt;li&gt;lighting,&lt;/li&gt;
  &lt;li&gt;RES (=Renewable Energy Resources) and&lt;/li&gt;
  &lt;li&gt;other.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The last category includes coffee machines, dish washers, etc. to make sure it all adds up.&lt;/p&gt;

&lt;p&gt;By providing a short horizon forecast on individual LCH level and a combined forecast of upto 12 hours for an MV-point the energy provider is assured a much more accurate figure upon which to base his production and or buying decisions. The MV-point aggregated level in the system is called the aggregator control hub and is the control system that coordinates an entire set of LCH’s that for the full load of an MV-point (Medium Voltage). In general an MV-point is between 0.5 and 2 MW.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/inertia1.png" alt="Switch loads" title="Switch loads to create a steady aggregated loads" /&gt;&lt;/p&gt;

&lt;p&gt;The experiments we have done have shown that the ability to switch loads in ms allows for a very steady aggregated load (see fig 1), The LCH’s can be instructed to use their control limits in such a way that they fulfill as much as possible their own predicted energy load. It is important to understand that load differs from energy consumption, as consumption is calculated over larger periods, which would still allow for large variations in load. These variations however are very detsabilizing for smaller subgrids with large contributions from locally generated energy such as PV and wind. The very fast reaction times of below 10ms and the tiered availability of flexibility both inside the LCH and between these LCH’s as facilitated by the ACH, are able to counteract the natural fluctuations that come with the use of PV and wind.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/inertia2.png" alt="Under noise" title="Heavy noise from random user events" /&gt;&lt;/p&gt;

&lt;p&gt;To illustrate the effectivity we have created the possibility to introduce random user events up to a scale that is normally impossible for humans to realize. (see fig 2).&lt;/p&gt;

&lt;p&gt;The aggregated flexibility of the well behaved LCH’s is easily enough to compensate (fig 3)&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/inertia3.png" alt="Aggregated flexibility" title="Aggregated flexibility" /&gt;&lt;/p&gt;

&lt;p&gt;We are more than happy to elaborate on the principles on your request 8-) and apologies for the rapid introduction of a lot of jargon…&lt;/p&gt;

&lt;p&gt;Oh yeah: The links on the page allow for accessing the APIs of the different LCH’s at a bus. A bus is identical to a single MV-point. As you can see I have used different buses to illustrate the behaviour.
All of the LCH’s in all buses are at the bottom of the page.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2016/01/28/inertia-energy-balance-management-system
                <guid>https://dobots.nl/2016/01/28/inertia-energy-balance-management-system</guid>
                <pubdate>2016-01-28T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Recognize that fridge!</title>
                <description>
&lt;h1 id="device-recognition"&gt;Device recognition&lt;/h1&gt;

&lt;p&gt;After thorough preparation, we have lift-off with the development of device classification for the Crownstone! For now I’m using second-hand data and starting with basic techniques, but with an accuracy in tests of 93.7% after just the first week of actual implementation and experimentation, the future looks bright!&lt;/p&gt;

&lt;p&gt;Here’s some stats and a confusion matrix of my latest cross-validation test, where you can see what kind of devices we can already identify. This classification method takes ‘traces’ of 24 hours as input, with a power level in Watts, each second. I extract 14 hand-implemented features from that data, and feed them through a &lt;a href="http://weka.sourceforge.net/doc.dev/weka/classifiers/meta/RandomCommittee.html"&gt;Random Committee classifier&lt;/a&gt; from the Weka set. RandomCommittee follows the same concept as &lt;a href="https://en.wikipedia.org/wiki/Random_forest"&gt;Random Forest&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The numbers correspond to classification results. The row pertains to the device from which the data was measured, and the column indicates the decision made by the classifier.&lt;/p&gt;

&lt;h2 id="stratified-cross-validation"&gt;Stratified cross-validation&lt;/h2&gt;

&lt;p&gt;Results are promising!&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Quantity&lt;/th&gt;
      &lt;th&gt;Percentage&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Correctly Classified Instances&lt;/td&gt;
      &lt;td&gt;1709&lt;/td&gt;
      &lt;td&gt;93.7466 %&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Incorrectly Classified Instances&lt;/td&gt;
      &lt;td&gt;114&lt;/td&gt;
      &lt;td&gt;6.2534 %&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Kappa statistic&lt;/td&gt;
      &lt;td&gt;0.9337&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Mean absolute error&lt;/td&gt;
      &lt;td&gt;0.0065&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Root mean squared error&lt;/td&gt;
      &lt;td&gt;0.0537&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Relative absolute error&lt;/td&gt;
      &lt;td&gt;14.792  %&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Root relative squared error&lt;/td&gt;
      &lt;td&gt;36.2652 %&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Total Number of Instances&lt;/td&gt;
      &lt;td&gt;1823&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id="confusion-matrix"&gt;Confusion Matrix&lt;/h2&gt;

&lt;p&gt;The confusion matrix shows which devices are easy and which ones are hard to distinguish from each other.&lt;/p&gt;

&lt;pre style="font-size: 9px"&gt;&lt;code&gt; a   b   c   d   e   f   g   h   i   j   k   l   m   n   o   p   q   r   s   t   u   v   w   x   y   z  aa  ab  ac  ad  ae  af  ag  ah  ai  aj  ak  al  am  an  ao  ap  aq   &amp;lt;-- classified as
 4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |   a = Alarmclock
 0  89   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |   b = Amplifier
 0   0  43   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |   c = BeanToCupCoffeemaker
 0   0   0  13   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |   d = Breadcutter
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0   0   0   0   0   0   0   0 |   e = CdPlayer
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |   f = Charger-PSP
 0   0   0   0   0   1   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |   g = Charger-Smartphone
 0   0   0   0   0   0   0  80   0   0   0   0   0   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |   h = Coffeemaker
 0   0   0   0   0   0   0   1  14   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0 |   i = Cookingstove
 0   0   0   0   0   0   0   0   0  23   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |   j = DigitalTvReceiver
 0   0   0   0   0   0   0   0   0   0  73   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0 |   k = Dishwasher
 0   0   0   0   0   0   0   0   0   0   0   3   0   0   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |   l = DvdPlayer
 0   0   0   0   0   0   0   0   0   0   0   0  28   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4   0   0   0   0   0   0   1   0   0   0   0   0   0   0 |   m = EthernetSwitch
 0   0   0   0   0   0   0   0   0   0   0   0   0   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |   n = Freezer
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |   o = Iron
 0   2   0   0   0   0   0   0   0   0   0   0   0   0   0  74   0   0   0   1   0   0   0   0   0   1   0   1   0   0   0   0   2   2   0   0   0   0   0   0   0   0   3 |   p = Lamp
 0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   7   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0 |   q = LaundryDryer
 0   0   0   0   0   0   0   3   0   0   0   0   0   0   0   1   0  56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |   r = MicrowaveOven
 0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  10  11   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |   s = Monitor-CRT
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  12 176   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0 |   t = Monitor-TFT
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  13   0   0   0   2   0   0   0   1   0   1   0   0   0   0   0   0   0   0   0   0   0   0 |   u = Multimediacenter
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0 150   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |   v = PC-Desktop
 0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   1  64   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |   w = PC-Laptop
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  11   0   0   0   0   0   0   0   0   0   3   0   0   0   0   0   0   0   0   0 |   x = Playstation3
 0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0  15   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |   y = Printer
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   7   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0 |   z = Projector
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 204   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0 |  aa = Refrigerator
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |  ab = RemoteDesktop
 0   0   0   0   0   0   0   0   0   0   0   0   4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  36   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |  ac = Router
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   1   0   0   0   0   0   0   0   5   0   0   0   0   0   0   0   0   0   0   0   0   0 |  ad = SolarThermalSystem
 0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0  25   0   0   0   0   0   0   0   0   0   0   0   0 |  ae = Subwoofer
 0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  22   0   0   0   0   0   0   0   0   0   0   0 |  af = Toaster
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   1   0   0   0   0   0   0   0   0   0   0  33   1   0   0   0   0   0   0   0   0   0 |  ag = TV-CRT
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   5   0   0   0   0   0   0   0   1   0   1   0   0   0   0   0   0   0 110   0   0   0   0   0   0   0   0   0 |  ah = TV-LCD
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0  28   0   0   0   0   0   0   0   0 |  ai = USBHarddrive
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  10   0   0   0   0   0   0   0 |  aj = USBHub
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |  ak = VacuumCleaner
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0  16   0   0   0   0   0 |  al = VideoProjector
 0   0   0   0   0   0   0   0   0   0   2   0   0   0   0   1   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  51   0   0   0   0 |  am = Washingmachine
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 |  an = WaterBoiler
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0  55   0   0 |  ao = WaterFountain
 0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 133   0 |  ap = WaterKettle
 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4 |  aq = XmasLights
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ideas for improvement are many, and this was just the first week. Things are looking up for classification! Cheers~&lt;/p&gt;
</description>
                <link>https://dobots.nl/2015/09/04/recognize-that-fridge
                <guid>https://dobots.nl/2015/09/04/recognize-that-fridge</guid>
                <pubdate>2015-09-04T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Human SLAM, Indoor localization using particle filters</title>
                <description>
&lt;h2 id="human-slam-indoor-localization-using-particle-filters"&gt;Human SLAM, Indoor localization using particle filters&lt;/h2&gt;

&lt;p&gt;A key problem (or challenge) within smart spaces is indoor localization: making estimates of users’ whereabouts. Without such information, systems are unable to react on the presence of users or, sometimes even more important, their absence. This can range from simply turning the lights on when someone enters a room to customizing the way devices interact with a specific user.&lt;/p&gt;

&lt;p&gt;Even more important for a system to know where users exactly are, is to know where users are relative to the devices it can control or use to sense the environment. This relation between user and device location is an essential input to these systems. A central question in this field is therefore:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What are the locations of devices in a smart space and what are the current locations of users relative to these devices?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;During my graduation project at DoBots I worked on an algorithm, called SLAC : Simultaneous Localization and Configuration, to solve this double localization problem: finding position of users and devices simultaneously.  With SLAC we aimed to simultaneously locate both the user and the devices of a system deployed in an indoor environment. The target platform was the Crownstone.&lt;/p&gt;

&lt;p&gt;The general idea of the project is to combine characteristics of devices with information from users that walk around in a building. The figure below illustrates this. We start without any knowledge (1), then we let users walk around and gather data (2), eventually we will learn the locations of each device in the building (3).&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/slac_project_overview.png" alt="SLAC project overview" title="SLAC project overview" /&gt;&lt;/p&gt;

&lt;p&gt;We use characteristics that are already available in many smart spaces: signal strength measurements (or RSSI) from devices and motion data from smart phones and other portable devices. These two inputs are combined in a system that can locate users and devices, respect individual users’ privacy and perform all estimations in real time. SLAC is based on a common technique from robotics, &lt;a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping"&gt;simultaneous localization and mapping&lt;/a&gt; (SLAM), and in particular the FastSLAM algorithm&lt;sup id="fnref:1"&gt;&lt;a href="#fn:1" class="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id="algorithm-overview"&gt;Algorithm overview&lt;/h3&gt;

&lt;p class="float-right" style="width: 400px; margin: 0px 0px 15px 20px;"&gt;&lt;img src="/attachments/slac_algorithm.svg" alt="SLAC algorithm" title="SLAC algorithm" /&gt;&lt;/p&gt;

&lt;p&gt;The SLAC algorithm uses two types of inputs: signal strength (RSSI) measurements to determine distances to devices and inertial measurement unit (IMU) data for pose estimations.&lt;/p&gt;

&lt;p&gt;The raw RSSI signal contains noise. To filter out large spikes, while trying to retain distance information, a (regular) &lt;a href="https://en.wikipedia.org/wiki/Kalman_filter"&gt;Kalman filter&lt;/a&gt; is used to filter incoming signal strength measurements.&lt;/p&gt;

&lt;p&gt;The second input is focussed on modeling motion. Two sensors are used to measure the motion of users: an accelerometer and a compass. These two sensors are present in almost any modern mobile device (including phones, tablets and wearables). The compass returns the current rotation or heading relative to the global north; this does not require any processing and can be used directly as an input. The acceleration is used as input for a pedometer. The pedometer is based on a design by Zhao (2010)&lt;sup id="fnref:2"&gt;&lt;a href="#fn:2" class="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt; and Ménigot (2014)&lt;sup id="fnref:3"&gt;&lt;a href="#fn:3" class="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Given the distance measurements and the motion data, we can map the SLAM problem to the domain of sensor networks and indoor localization. The robot’s controls, which are used for pose sampling, are replaced by our motion estimates. The observations, which are often 2D measurements, are replaced by 1D RSSI measurements similar to the approach of Sun et al. (2009)&lt;sup id="fnref:4"&gt;&lt;a href="#fn:4" class="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;. The flow and update rate of the SLAC algorithm is controlled by the pedometer: the algorithm is run after a new step has been detected.&lt;/p&gt;

&lt;p&gt;As device observations are 1D and signals propagate spherical it is impossible, given a single measurement, to determine the bearing of an observation. We therefore must first make an initial estimate of a beacon location before we can refine it using the default method of FastSLAM. We solve this by implementing a second separate particle filter that focusses solely on finding an initial estimate of a device’s location.  After each new measurement we update this filter by computing the importance weight and subsequently resampling the filter. When the variance between particles is low enough (given some threshold), we assume that a a devices’s location has been found.&lt;/p&gt;

&lt;p&gt;Our initialization filter is a separate component that uses the current best user estimate as input. So, to improve on our rough initial estimate we move the estimation from the global initalization filter to each individual particle. As it is impractical to update MxN particle filters (one particle filter per landmark per particle) we use an &lt;a href="https://en.wikipedia.org/wiki/Extended_Kalman_filter"&gt;extended Kalman filter&lt;/a&gt; (EKF) to estimate a landmark’s location (similar to the original FastSLAM implementation).&lt;/p&gt;

&lt;p&gt;When each observation is processed all particles have been updated and contain new importance weights. We then perform resampling. However, it does not make sense to resample after each step; there is just to little information for the resample process. In order to overcome this we utilize &lt;a href="https://en.wikipedia.org/wiki/Particle_filter#Sequential_importance_resampling_.28SIR.29"&gt;Sequential Importance Resampling&lt;/a&gt; (SIR).&lt;/p&gt;

&lt;h3 id="implementation"&gt;Implementation&lt;/h3&gt;

&lt;p class="float-right" style="width: 400px; margin: 0px 0px 15px 20px;"&gt;&lt;img src="/attachments/slac_devices_example.png" alt="Screenshots of application" title="Screenshots of application" /&gt;&lt;/p&gt;

&lt;p&gt;SLAC has been fully implemented in Javascript and more specifically using the &lt;em&gt;ECMAScript version 6/2015&lt;/em&gt; standard. Javascript has been chosen to support a large range of devices on which the algorithm can run; this includes web browsers and mobile phones. The &lt;a href="https://cordova.apache.org"&gt;Apache Cordova platform&lt;/a&gt; was used to access native API’s of mobile devices such as the Bluetooth radio and the motion sensors.&lt;/p&gt;

&lt;h3 id="simulations"&gt;Simulations&lt;/h3&gt;

&lt;p&gt;We evaluated the system using simulations first; this granted the opportunity to repeat the experiment and control the environment. In the simulations we emulated the world by building an environment of similar dimensions as one of our real world test beds.&lt;/p&gt;

&lt;p&gt;We varied the number of RSSI updates each device broadcasts between each consecutive algorithm step. As the signal strength is used to make range estimates the number of received messages could have an effect on the overall performance. The different settings are: 1, 2, 5, 10, 25, 50 and 100 updates per step. The results are shown in the figure below.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/slac_rssi_step.png" alt="Results of and simulations" title="Results of simulations" /&gt;&lt;/p&gt;

&lt;p&gt;We found that the number of RSSI updates per algorithm step has a very high effect on the performance of the system. This follows directly from our system: given more information the Kalman filter responsible for filtering the raw RSSI signal will be able to give a less noisy estimate of the current distance to a device. These distance estimates are vital in updating device positions and weighing particles. These results suggest that we need a high update rate for real world applications.&lt;/p&gt;

&lt;p&gt;In terms of performance, these simulations showed that within controlled environments (i.e. Gaussian noise, fixed paths of users, no obstacles), we can achieve an average localization error below .20m. This means that, after running the algorithm, the estimate of a device’s location will, on average, only be 20 centimeters away from its actual position. When the update frequency of devices is lowered, to a level similar to our live tests, the average localization error increased and resulted in an average error of .69 to 1.25m.&lt;/p&gt;

&lt;h3 id="real-world-tests"&gt;Real world tests&lt;/h3&gt;

&lt;p&gt;Simulating RSSI values and movement has its drawbacks: noise is predictable and there is less interference from events in the environment. In general it is hard to fully simulate all the factors of a real world environment. In order to asses the performance of the algorithm outside a simulated world, we also tested the algorithm in the wild at the DoBots/Almende building.&lt;/p&gt;

&lt;p&gt;While SLAC runs online and in real time, the data for this live test has been recorded and analyzed offline at a later stage. The algorithm did however run during the data collection to give feedback about the process. Each recorded data set consisted of the raw unprocessed and unfiltered motion data (i.e. acceleration and heading) and RSSI measurements. These datasets are played back several times to get an average performance. This is particular important as the algorithm is a random process: using the same input data twice will result in different outcomes.&lt;/p&gt;

&lt;p&gt;The results of our live tests are shown in the figure below (A_1 to A_4). Additionally, the simulations results of the two conditions (Sim 5 &amp;amp; Sim 10) which are comparable to a real world setup are displayed as a reference.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/slac_almende_results.png" alt="Results of live tests and simulations" title="Results of live tests and simulations" /&gt;&lt;/p&gt;

&lt;p&gt;All combined, our live tests showed a localization error of 2.3m, averaged over all devices. This result is good enough for room level accuracy, but there is room for improvement. These results where achieved by letting a single user walk around for one to two minutes (roughly 60 steps). All computations are done locally, i.e. running on users’ devices and without using prior information of the environment.&lt;/p&gt;

&lt;h3 id="demo"&gt;Demo&lt;/h3&gt;

&lt;p&gt;A video of a simulation of the algorithm can be seen in the video below. A user is walking inside a building/smart space filled with seven simulated devices (DoBeacons). The blue line describes the ground truth of the user’s path. The green line is the current best estimate of this path. Devices are displayed using squares; true positions are displayed in black and estimates in red.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/NzOi9uYiAOw?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;p&gt;An online demo can also be found on the website of &lt;a href="https://wouterbulten.nl/slacjs/"&gt;SLACjs&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id="code"&gt;Code&lt;/h3&gt;

&lt;p&gt;The full source code is available online on &lt;a href="https://github.com/wouterbulten/slacjs"&gt;GitHub&lt;/a&gt; and is licensed under the GNU Lesser General Public License v3.&lt;/p&gt;

&lt;h3 id="references"&gt;References&lt;/h3&gt;

&lt;div class="footnotes"&gt;
  &lt;ol&gt;
    &lt;li id="fn:1"&gt;
      &lt;p&gt;Michael Montemerlo, Sebastian Thrun, Daphne Koller, and Ben Wegbreit. FastSLAM: A factored solution to the simultaneous localization and mapping problem. Proc. of 8th National Conference on Artificial Intelligence/14th Conference on Innovative Applications of Artificial Intelligence, 68(2):593–598, 2002. &lt;a href="#fnref:1" class="reversefootnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id="fn:2"&gt;
      &lt;p&gt;Neil Zhao. Full-featured pedometer design realized with 3-Axis digital accelerometer. Analog Dialogue, 44(6), 2010. &lt;a href="#fnref:2" class="reversefootnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id="fn:3"&gt;
      &lt;p&gt;Sébastien Ménigot. Pedometer in HTML5 for Firefox OS and Firefox for Android, 2014. &lt;a href="http://sebastien.menigot.free.fr/index.php?option=com_content&amp;amp;view=article&amp;amp;id=93:pedometer-in-html5-&amp;amp;catid=46:web-application-for-firefox-os&amp;amp;Itemid=82"&gt;source&lt;/a&gt; &lt;a href="#fnref:3" class="reversefootnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id="fn:4"&gt;
      &lt;p&gt;Dali Sun, Alexander Kleiner, and Thomas M. Wendt. Multi-robot range-only SLAM by active sensor nodes for urban search and rescue. In Robocup 2008: Robot Soccer World Cup XII, volume 5399, pages 318–330, 2009. &lt;a href="#fnref:4" class="reversefootnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
                <link>https://dobots.nl/2015/09/03/human-slam-indoor-localization-using-particle-filters
                <guid>https://dobots.nl/2015/09/03/human-slam-indoor-localization-using-particle-filters</guid>
                <pubdate>2015-09-03T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>I'm Busy: the app that keeps up your status</title>
                <description>
&lt;h1 id="im-busy-the-app-that-keeps-up-your-status"&gt;I’m Busy: the app that keeps up your status&lt;/h1&gt;

&lt;p&gt;We made a simple &lt;a href="https://play.google.com/store/apps/details?id=nl.dobots.imbusy"&gt;app&lt;/a&gt; that can notify others when you’re busy.
It’s made for elderly people who, for example, don’t want to be interrupted while they’re in the bathroom.&lt;/p&gt;

&lt;p&gt;The app uses Bluetooth Smart beacons (which includes iBeacons) to localize itself. The iBeacon protocol is also
supported by our &lt;a href="https://crownstone.rocks"&gt;Crownstones&lt;/a&gt;, so we naturally used those!
When you’re near such a beacon, it will set your status to busy.
In the app you can see the status of your friends, and when you call someone who’s busy, it will notify you that he/she is busy.&lt;/p&gt;

&lt;p&gt;Here are some screenshots of the app:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/imbusy-screenshot-contacts.png" alt="Contact list" style="width: 280px" /&gt;
&lt;img src="/attachments/imbusy-screenshot-calling.png" alt="Calling a busy person" style="width: 280px" /&gt;&lt;/p&gt;

&lt;h1 id="technical-details"&gt;Technical details&lt;/h1&gt;

&lt;p&gt;In order to get your status to friends, we make use of XMPP, since it’s a nice protocol that has many (open source) implementations.
For the app, we used &lt;a href="http://www.igniterealtime.org/projects/smack/index.jsp"&gt;Smack&lt;/a&gt;, which now easily runs on Android (before, you had to use aSmack).
XMPP sped up development, as it already provides authorization, keeps up contact lists, and synchronizes the status.&lt;/p&gt;

&lt;p&gt;To scan for beacons, I used our own &lt;a href="https://github.com/dobots/bluenet-lib-android"&gt;bluenet android library&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And of course, you can find the source code on &lt;a href="https://github.com/dobots/imbusy-app"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
                <link>https://dobots.nl/2015/08/25/im-busy-the-app-that-keeps-up-your-status
                <guid>https://dobots.nl/2015/08/25/im-busy-the-app-that-keeps-up-your-status</guid>
                <pubdate>2015-08-25T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Crownstone. What is it? Students find out!</title>
                <description>
&lt;h1 id="students-and-the-crownstone"&gt;Students and the Crownstone&lt;/h1&gt;

&lt;p&gt;We have been not so talkative on the DoBots forums lately, because we are working a lot on a very new product, the
Crownstone! The Crownstone will be launched in a few weeks on Kickstarter! If you want to be one of the early birds,
go immediately to &lt;a href="http://crownstone.rocks"&gt;http://crownstone.rocks&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;The Crownstone is a smart power outlet that can react to your proximity as well as recognize devices, lights,
appliances, based on their consumption pattern.&lt;/p&gt;

&lt;p&gt;Students from the Hogeschool Rotterdam have been exploring a wide diversity of possibilities that the Crownstone opens
up. Below you see a wall with very short clips with people explaining in Dutch what their project was about. Each of
the projects has also a website linked to it.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href="http://project.cmi.hro.nl/2014_2015/emedia_mt_t1/"&gt;Team 1&lt;/a&gt; developed ChildLock. Devices are only turned
on when adults are in proximity.&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://project.cmi.hro.nl/2014_2015/emedia_mt_t2/"&gt;Team 2&lt;/a&gt; worked out Green Stone. A hotel visitor gets
information about being green using technology.&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://project.cmi.hro.nl/2014_2015/emedia_mt_t3/"&gt;Team 3&lt;/a&gt; developed Start VR. Virtual reality that
allows you to picture your own furniture in an Ikea store.&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://project.cmi.hro.nl/2014_2015/emedia_mt_t4/"&gt;Team 4&lt;/a&gt; introduced Never Lose. Lights indicate for
elderly people where they have lost items with iBeacons.&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://project.cmi.hro.nl/2014_2015/emedia_mt_t5/"&gt;Team 5&lt;/a&gt; used Crownstones to indicate the way to
store employees.&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://project.cmi.hro.nl/2014_2015/emedia_mt_t6/"&gt;Team 6&lt;/a&gt; worked on Tommy. An AI that analyzes patterns
of daily life to combat loneliness.&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://project.cmi.hro.nl/2014_2015/emedia_mt_t7/"&gt;Team 7&lt;/a&gt; developed Any Morning. Your phone using the
Crownstones guides you to your morning routines to make you leave your home on time.&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://project.cmi.hro.nl/2014_2015/emedia_mt_t8/"&gt;Team 8&lt;/a&gt; invented Crowns and Kingdoms. A game that
uses energy savings measured by Crownstones to be able to build your assets faster in your kingdom.&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://project.cmi.hro.nl/2014_2015/emedia_mt_t9/"&gt;Team 9&lt;/a&gt; played with Benergy. In a student house the
energy bill is brought down through awareness by Crownstones which directly results in more beer!&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://project.cmi.hro.nl/2014_2015/emedia_mt_t10/"&gt;Team 10&lt;/a&gt; implemented Tipspromenad. Kids have to find
objects in Ikea combined with solving puzzles for fun!&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://project.cmi.hro.nl/2014_2015/emedia_mt_t11/"&gt;Team 11&lt;/a&gt; developed SpotOn. In emergency situations
lights indicate how to flee a building.&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://project.cmi.hro.nl/2014_2015/emedia_mt_t12/"&gt;Team 12&lt;/a&gt; thought about using ambient lights and
scents to attract people to parts of a store.&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://project.cmi.hro.nl/2014_2015/emedia_mt_t13/"&gt;Team 13&lt;/a&gt; developed Ikea Bump. By bumping a phone
a product will be in your virtual cart and people are led to the right place in the warehouse.&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://project.cmi.hro.nl/2014_2015/emedia_mt_t14/"&gt;Team 14&lt;/a&gt; experimented with ideas on cleaning
hotel rooms.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following is a video wall with a project member of every team very briefly and on the spot explaining what their
project is about. It is in Dutch, but feel free to ask for any details. We will be happy to pass them on or explain
them in more detail ourselves.&lt;/p&gt;

&lt;video preload="metadata" controls="" class="some-css-class" style="width: 30%"&gt;&lt;source src="/attachments/hogeschool_rotterdam/team1.mp4" type="video/mp4; codecs=&amp;quot;avc1.42E01E, mp4a.40.2&amp;quot;" /&gt;&lt;/video&gt;
&lt;video preload="metadata" controls="" class="some-css-class" style="width: 30%"&gt;&lt;source src="/attachments/hogeschool_rotterdam/team2.mp4" type="video/mp4; codecs=&amp;quot;avc1.42E01E, mp4a.40.2&amp;quot;" /&gt;&lt;/video&gt;
&lt;video preload="metadata" controls="" class="some-css-class" style="width: 30%"&gt;&lt;source src="/attachments/hogeschool_rotterdam/team3.mp4" type="video/mp4; codecs=&amp;quot;avc1.42E01E, mp4a.40.2&amp;quot;" /&gt;&lt;/video&gt;
&lt;video preload="metadata" controls="" class="some-css-class" style="width: 30%"&gt;&lt;source src="/attachments/hogeschool_rotterdam/team4.mp4" type="video/mp4; codecs=&amp;quot;avc1.42E01E, mp4a.40.2&amp;quot;" /&gt;&lt;/video&gt;
&lt;video preload="metadata" controls="" class="some-css-class" style="width: 30%"&gt;&lt;source src="/attachments/hogeschool_rotterdam/team5.mp4" type="video/mp4; codecs=&amp;quot;avc1.42E01E, mp4a.40.2&amp;quot;" /&gt;&lt;/video&gt;
&lt;video preload="metadata" controls="" class="some-css-class" style="width: 30%"&gt;&lt;source src="/attachments/hogeschool_rotterdam/team6.mp4" type="video/mp4; codecs=&amp;quot;avc1.42E01E, mp4a.40.2&amp;quot;" /&gt;&lt;/video&gt;
&lt;video preload="metadata" controls="" class="some-css-class" style="width: 30%"&gt;&lt;source src="/attachments/hogeschool_rotterdam/team7.mp4" type="video/mp4; codecs=&amp;quot;avc1.42E01E, mp4a.40.2&amp;quot;" /&gt;&lt;/video&gt;
&lt;video preload="metadata" controls="" class="some-css-class" style="width: 30%"&gt;&lt;source src="/attachments/hogeschool_rotterdam/team8.mp4" type="video/mp4; codecs=&amp;quot;avc1.42E01E, mp4a.40.2&amp;quot;" /&gt;&lt;/video&gt;
&lt;video preload="metadata" controls="" class="some-css-class" style="width: 30%"&gt;&lt;source src="/attachments/hogeschool_rotterdam/team9.mp4" type="video/mp4; codecs=&amp;quot;avc1.42E01E, mp4a.40.2&amp;quot;" /&gt;&lt;/video&gt;
&lt;video preload="metadata" controls="" class="some-css-class" style="width: 30%"&gt;&lt;source src="/attachments/hogeschool_rotterdam/team10.mp4" type="video/mp4; codecs=&amp;quot;avc1.42E01E, mp4a.40.2&amp;quot;" /&gt;&lt;/video&gt;
&lt;video preload="metadata" controls="" class="some-css-class" style="width: 30%"&gt;&lt;source src="/attachments/hogeschool_rotterdam/team11.mp4" type="video/mp4; codecs=&amp;quot;avc1.42E01E, mp4a.40.2&amp;quot;" /&gt;&lt;/video&gt;
&lt;video preload="metadata" controls="" class="some-css-class" style="width: 30%"&gt;&lt;source src="/attachments/hogeschool_rotterdam/team12.mp4" type="video/mp4; codecs=&amp;quot;avc1.42E01E, mp4a.40.2&amp;quot;" /&gt;&lt;/video&gt;
&lt;video preload="metadata" controls="" class="some-css-class" style="width: 30%"&gt;&lt;source src="/attachments/hogeschool_rotterdam/team13.mp4" type="video/mp4; codecs=&amp;quot;avc1.42E01E, mp4a.40.2&amp;quot;" /&gt;&lt;/video&gt;

&lt;p&gt;Of course, these little clips don’t do right to what those students have been doing. Definitely check out their
websites to see much better movies and material they have created around their concepts!&lt;/p&gt;

</description>
                <link>https://dobots.nl/2015/07/24/crownstone-what-is-it-students-find-out
                <guid>https://dobots.nl/2015/07/24/crownstone-what-is-it-students-find-out</guid>
                <pubdate>2015-07-24T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Smart Product for Weight Loss</title>
                <description>
&lt;h1 id="a-smart-product-for-weight-loss-through-behavior-change"&gt;A smart product for weight loss through behavior change&lt;/h1&gt;

&lt;p class="some-css-class" style="width: 400px; float: right"&gt;&lt;img src="/attachments/bitslab_functions.png" alt="Bitslab functions" title="BitsLab functionality" /&gt;&lt;/p&gt;

&lt;p&gt;Though an increased caloric intake is the most influential cause of the high and rising prevalence of overweight and obesity, there are no proper technical solutions yet that address this problem. Monitoring intake has shown to be an effective method for reducing intake. However, manual tracking tends to be inaccurate. Additionally, manual tracking requires too much mental effort for prolonged use.&lt;/p&gt;

&lt;h2 id="design"&gt;Design&lt;/h2&gt;

&lt;p&gt;Within her graduation project, &lt;a href="/hall-of-fame/"&gt;Anne Bekker&lt;/a&gt; has created a design that automatically monitors the user’s eating patterns. The design, called the BitsLab, facilitates behavior change by helping people analyze and improve their food intake patterns. By changing habits, it focuses on long-term changes. The design consists of an add-on for smartphones and an accompanying app. The add-on is a chewing sensor, located on the zygomatic process (a facial bone).&lt;/p&gt;

&lt;h2 id="sensing-technology"&gt;Sensing technology&lt;/h2&gt;

&lt;p class="some-css-class" style="width: 400px; float: right; margin: 0px 0px 15px 20px;"&gt;&lt;img src="/attachments/chewing.png" alt="Talking, Chewing" title="Talking, Chewing" /&gt;&lt;/p&gt;

&lt;p&gt;The chewing sensor, a piezoelectric transducer, monitors chewing trough bone conducted audio. The following sample demonstrates a recording of speech followed by the chewing of a pepernoot.&lt;/p&gt;

&lt;p&gt;&lt;a href="/attachments/chewing.mp3"&gt;Audio data&lt;/a&gt; is sent from the add-on to a smartphone app using Bluetooth LE, where it is analyzed by an algorithm. This measured data will provide users with an unbiased overview of their intake pattern.&lt;/p&gt;

&lt;h2 id="app"&gt;App&lt;/h2&gt;

&lt;p&gt;The app (which is not yet developed) helps users to find out what it is they are actually craving at moments their craving for undesired eating strikes, and to find a substitute behavior that satisfies the same craving. Consecutively, the BitsLab helps users discover what triggers their craving by sending them prompts at times unplanned eating occurs. When filled in, these prompts gather categorized data on potential triggers which can be analyzed by the app and the user.&lt;/p&gt;

&lt;p class="some-css-class" style="width: 200px; float: left; margin: 0px 20px 0px 0px;"&gt;&lt;img src="/attachments/bitslab_app_screen.png" alt="App Screen" title="App Screen" /&gt;&lt;/p&gt;

&lt;p&gt;Once users have discovered what it is they are craving as well as what triggers this craving, they can start changing the habit. This consists of substituting the routine of unwanted eating by an alternative, healthy behaviour, which was found to satisfy the same craving.&lt;/p&gt;

&lt;p class="some-css-class" style="float: center;"&gt;&lt;img src="/attachments/bitslab_loop.jpg" alt="Loop" title="Habit-changing loop" /&gt;&lt;/p&gt;

&lt;p&gt;In addition to changing habits, the app will teach users how to form new habits. By taking very small steps, and executing them consistently at a set point within their established routine, a habit will form without requiring much effort.&lt;/p&gt;

&lt;p class="some-css-class" style="width: 35%; float: right; margin: 0px 0px 0px 20px;"&gt;&lt;img src="/attachments/bitslab_prototype.jpg" alt="Prototype" title="Prototype" /&gt;&lt;/p&gt;

&lt;h2 id="proof-of-concept-prototype"&gt;Proof of concept prototype&lt;/h2&gt;

&lt;p&gt;The prototype is able to detect chewing. The algorithm for the chewing recognition is based on the extraction and comparison of features of a pre-recorded chew to an unknown incoming sound stream. The movie below shows the chewing algorithm app in action. When the green rectangle appears, chewing is recognized. When the app detects that the user has stopped eating, it will turn back to black.&lt;/p&gt;

&lt;h2 id="a-small-demonstration"&gt;A small demonstration&lt;/h2&gt;

&lt;video preload="metadata" controls=""&gt;&lt;source src="/attachments/bitslab_movie.mp4" type="video/mp4; codecs=&amp;quot;avc1.42E01E, mp4a.40.2&amp;quot;" /&gt;&lt;/video&gt;

&lt;h1 id="literature"&gt;Literature&lt;/h1&gt;

&lt;p&gt;You can read more in Anne’s &lt;a href="/attachments/thesis/annebekker.pdf"&gt;master thesis (pdf)&lt;/a&gt;.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2015/05/26/smart-product-for-weight-loss
                <guid>https://dobots.nl/2015/05/26/smart-product-for-weight-loss</guid>
                <pubdate>2015-05-26T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Attaching a MicroView to the Crownstone</title>
                <description>
&lt;h1 id="microview-and-crownstone"&gt;MicroView and Crownstone&lt;/h1&gt;
&lt;p&gt;&lt;img src="/attachments/microview.JPG" alt="Crownstone with attached MicroView" title="Crownstone with attached MicroView" style="width: 680px" /&gt;&lt;/p&gt;

&lt;p&gt;Last week I figured it would be nice to attach the &lt;a href="http://learn.microview.io/intro/general-overview-of-microview.html"&gt;MicroView&lt;/a&gt; to the &lt;a href="https://dobots.nl/products/crownstone"&gt;Crownstone&lt;/a&gt;, so that it could display some info you normally see on your phone.
Since the Crownstone can only supply a low amount of power, the first thing I had to do is figure out how much power the MicroView consumes. I couldn’t find this online quick enough, so I simply measured it: about 12mA at 3.3V. This should be low enough for the Crownstone to supply!&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/microview_schematic.svg" alt="Schematic" style="width: 680px" /&gt;&lt;/p&gt;

&lt;h1 id="display-the-data"&gt;Display the data&lt;/h1&gt;
&lt;p&gt;Next, I experimented with the widgets, these are provided by the MicroView library and can display a number nicely. It turned out you can have two next to eachother plus some text above, perfect :).
I wrote a simple input parser that can parse arrays of integer. Then, I edited the code of the Crownstone to output temperature and RSSI values in the simple format.&lt;/p&gt;

&lt;p&gt;After this worked, I figured it was time to show a graph of the current consumption of the light bulb that is connected via the Crownstone. Since it can sample the current at about 10kHz, you can actually see the 50Hz AC curve that comes out of the &lt;a href="http://en.wikipedia.org/wiki/AC_power_plugs_and_sockets"&gt;wall socket&lt;/a&gt;.
To be able to switch between the different kinds of info, I added a simple push button to the MicroView.&lt;/p&gt;

&lt;h1 id="video"&gt;Video&lt;/h1&gt;
&lt;p&gt;In the video you see it all working. As soon as I connect to the Crownstone, the RSSI starts to update. Then I click the button and ask for a sample of the current graph.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/NiflbCiuCBE?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;h1 id="code"&gt;Code&lt;/h1&gt;
&lt;p&gt;You can find the code I made for the MicroView at our &lt;a href="https://github.com/dobots/crownstone-micro-view"&gt;github&lt;/a&gt;. And the Crownstone code is available under the &lt;a href="https://github.com/dobots/bluenet"&gt;bluenet repository&lt;/a&gt;.&lt;/p&gt;
</description>
                <link>https://dobots.nl/2015/03/30/attaching-a-microview-to-the-crownstone
                <guid>https://dobots.nl/2015/03/30/attaching-a-microview-to-the-crownstone</guid>
                <pubdate>2015-03-30T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>RPlidar vs kinect's laserscan with RTabmap on turtlebot</title>
                <description>
&lt;h2 id="rplidar-overview"&gt;RPlidar overview&lt;/h2&gt;

&lt;p&gt;The RPlidar is a 360° SLAM-ready laser scanner. It is amongst the cheapest of its kind and yet, it offers accurated measures.&lt;/p&gt;

&lt;p&gt;More info and where to find it: &lt;a href="http://www.robotshop.com/en/rplidar-360-laser-scanner.html"&gt;robotshop&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/rplidar-360-laser-scanner.jpg" alt="RPlidar laser scan" title="RPlidar laser scan" style="width: 140px" /&gt;&lt;/p&gt;

&lt;h2 id="rtabmap-overview"&gt;RTabmap overview&lt;/h2&gt;

&lt;p&gt;RTabmap is a real-time RGB-D graph SLAM approach developped by  M.Labbé and F.Michaud, from the IntRoLab.&lt;/p&gt;

&lt;p&gt;More info and where to find: &lt;a href="http://introlab.github.io/rtabmap/"&gt;RTabmap official website&lt;/a&gt;&lt;/p&gt;

&lt;h2 id="mounting-the-rplidar-on-the-turtlebot"&gt;Mounting the RPlidar on the turtlebot&lt;/h2&gt;

&lt;p&gt;First, you have to physically mount the laser on your turtlebot. Get your drill, a screwdriver and screw it on an empty plate! I fixed mine just above the kinect:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/rplidar_on_turtlebot.jpg" alt="RPlidar on turtlebot" title="RPlidar on turtlebot" style="width: 280px" /&gt;&lt;/p&gt;

&lt;p&gt;The next step is to update your turtlebot’s description files to include the laser.
We need to find which plate is the laser fixed to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;roscd turtlebot_description/urdf/stacks
gedit circles.urdf.xacro
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In my case, it was:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    &amp;lt;joint name="plate_3_joint" type="fixed"&amp;gt;
      &amp;lt;origin xyz="-0.01316 0 0.2063496" rpy="0 0 0" /&amp;gt;
      &amp;lt;parent link="plate_2_link"/&amp;gt;
      &amp;lt;child link="plate_3_link" /&amp;gt;
    &amp;lt;/joint&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;How to find it: the plate_3 was 20cm above plate_2 so I knew parent link=plate_2_link and Z=~0.20
Then, we need to adjust the position of the laser relative to its plate.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;roscd turtlebot_create/../create_description/urdf
gedit create.urdf.xacro
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modify: (coordinates in meters of the center of the rotating part of the laser, rotations in radians.)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	  &amp;lt;joint name="laser_joint" type="fixed"&amp;gt;
	    &amp;lt;origin xyz="[PositionX] [PositionY] [PositionZ]" rpy="[rotation/X] [rotation/Y] [rotation/Z]" /&amp;gt;
	    &amp;lt;parent link="[plate on which the laser is]" /&amp;gt;
	    &amp;lt;child link="laser" /&amp;gt;
	  &amp;lt;/joint&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;visualize Turtlebot on rviz to make sure your parameters are good, open two terminals and start:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;roslaunch turtlebot_bringup minimal.launch
rosrun rviz rviz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;in rviz: add -&amp;gt; robot model (you may have to change the fixed frame to base_link)
You should see something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/turtlebot_visualisation.jpg" alt="turtlebot visualisation" title="Turtlebot Visualisation" style="width: 280px" /&gt;&lt;/p&gt;

&lt;p&gt;Where the green rectangle represent the RPlidar.&lt;/p&gt;

&lt;h2 id="installing-ros-packages-for-the-rplidar"&gt;Installing ROS packages for the RPlidar&lt;/h2&gt;

&lt;p&gt;I installed it from source, which you can find here: &lt;a href="https://github.com/robopeak/rplidar_ros"&gt;https://github.com/robopeak/rplidar_ros&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd catkin_ws/src
git clone https://github.com/robopeak/rplidar_ros.git rplidar
cd ..
catkin_make
source devel/setup.bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You may have to change the ttyUSBX defined in rplidar.launch to have it working with along the turtlebot.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;roscd rplidar_ros/launch
gedit rplidar.launch
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id="configuring-the-turtlebot-launch-file"&gt;Configuring the turtlebot launch file&lt;/h2&gt;
&lt;p&gt;Now, we want to generate the scan topic from the RPlidar and the kinect to compare them with RTabmap.
Therefore, we have to change the name of the scan topic from the kinect.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;roscd turtlebot_bringup/launch
gedit 3dsensor.launch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;change&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;lt;!-- Laserscan topic --&amp;gt;
 	 &amp;lt;arg name="scan_topic" default="scan"/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;lt;!-- Laserscan topic --&amp;gt;
  &amp;lt;arg name="scan_topic" default="scan_kinect"/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id="configuring-rtabmap-launch-file"&gt;Configuring RTabmap launch file&lt;/h2&gt;

&lt;p&gt;Now that we have a working rplidar publishing on /scan and a kinect publishing an emulated scan on /scan_kinect, we have to modify our rtabmap launch file to include one or another.
The easiest and most convenient way is to define an argument scan_topic which can be either /scan or /scan_kinect.&lt;/p&gt;

&lt;p&gt;Edit your favorite launch file this way (e.g. rgbd_mapping.launch):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;roscd rtabmap_ros/launch
gedit rgbd_mapping.launch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add this line under the other arguments:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   &amp;lt;arg name="scan_topic" default="/scan"/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then change the line remapping the scan_topic in the visual SLAM and the visulation node to&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      &amp;lt;remap from="scan" to="$(arg scan_topic)"/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make sur you have set the subscribe_laserScan to true in those nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      &amp;lt;param name="subscribe_laserScan" type="bool" value="true"/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now your turtlebot is ready to use rtabmap with one or the other laser ! By default it will use the RPlidar, if you want to start rtabmap with the kinect laser, use this line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;roslaunch rtabmap_ros rgbd_mapping.launch scan_topic:=/scan_kinect
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id="rplidar-vs-kinect-laserscan"&gt;RPlidar vs kinect laserscan&lt;/h2&gt;

&lt;p&gt;In order to compare both methods in the exact same conditions, we have to record our ros_topic during the run, and provide those recorded runs to rtabmap.&lt;/p&gt;

&lt;p&gt;To record them as a rosbag:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd bagfiles
rosbag record -O turtlebotrun /tf /odom /camera/depth_registered/camera_info /camera/depth_registered/image_raw camera/rgb/camera_info /camera/rgb/image_rect_color /scan /scan_kinect
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To play the recorded bagfile:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rosbag play turtlebotrun --clock
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is an example of run. This is what the room we used look like I.R.L.:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/testroom.jpg" alt="Testroom" title="Testroom" style="width: 560px" /&gt;
&lt;img src="/attachments/testroom2.jpg" alt="Testroom2" title="Testroom2" style="width: 560px" /&gt;&lt;/p&gt;

&lt;p&gt;Turtlebot ran around the pool table. The RTabmap run footage: (The multicolor voxels represent the rplidar scan)&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/SEjo1diBlro?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;p&gt;Here are the two point clouds extracted from the laser scan and the kinect laser scan during a the fast run showed above:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/rplidarvskinect_pc.jpg" alt="rplidar vs kinect point cloud" title="rplidar vs kinect point cloud" style="width: 840px" /&gt;&lt;/p&gt;

&lt;p&gt;The red line is the path followed by the turtlebot.
As we can see, the results are similar except that the pool table is detected with the rplidar and not the kinect, because it was not in the kinect’s field of vision.&lt;/p&gt;

&lt;p&gt;The graphview (occupancy grid) made from rtabmap looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/rplidarvskinect.jpg" alt="rplidar vs kinect occupancy grid" title="occupancy grid" style="width: 840px" /&gt;&lt;/p&gt;

&lt;p&gt;A few details about the zone:
There were some holes in the cardboard around the pool table. This is why the pool table is partially filled in the rplidar scan. Also, the mess on the top corner on both screen is due to the presence of a radiator.&lt;/p&gt;

&lt;p&gt;To compare both maps, we used different criterias such as the number of missing walls, the unknown surface in known area, the straightnes of the walls…
We evaluated that, on a short run, the rplidar gave better result. Its wider range gives him more information and we don’t have to look in every direction to map the area. However, if we did so, the results were similar with the kinect.
But in a real environment, and depending to the exploration algorithm used, we are more likely to be in a “fast-run” setup.&lt;/p&gt;
</description>
                <link>https://dobots.nl/2015/02/05/rplidar-with-rtabmap-on-turtlebotmd
                <guid>https://dobots.nl/2015/02/05/rplidar-with-rtabmap-on-turtlebotmd</guid>
                <pubdate>2015-02-05T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Programming the nRF51822 with the ST-link</title>
                <description>
&lt;h1 id="segger-connector"&gt;Segger connector&lt;/h1&gt;

&lt;p&gt;If your device has a connector that is meant for The Segger JLink programmer, you can find the pin layout at
&lt;a href="https://www.segger.com/jlink-adapters.html#CM_9pin"&gt;https://www.segger.com/jlink-adapters.html#CM_9pin&lt;/a&gt;. The default programmer with the Nordic development kit comes with
the J-Link 9-pin Cortex-M adapter.&lt;/p&gt;

&lt;p&gt;The ones that you will need to break out are four wires that at the top:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. VTref
2. SWDIO
3. GND
4. SWCLK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first one is optional! It does not provide power, it is used to &lt;strong&gt;measure&lt;/strong&gt; if the target board does have power.&lt;/p&gt;

&lt;h1 id="st-link-programmer"&gt;ST-Link programmer&lt;/h1&gt;

&lt;p&gt;The ST-Link programmer is not easy obtainable, just as the J-Link programmer is actually hard to come by as a separate
product.&lt;/p&gt;

&lt;p&gt;For that reason it is worth to check into the STM32FDISCOVERY board. You can get the &lt;a href="http://no.mouser.com/ProductDetail/STMicroelectronics/STM32F4DISCOVERY/?qs=J2qbEwLrpCGdWLY96ibNeQ%3D%3D"&gt;STM32FDISCOVERY at Mouser&lt;/a&gt; for € 13,47. This board is anyway fun, it has a
lot of GPIO (pins) and it has an ST-Link on-board.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/stm32f4_discovery.jpg" alt="STM32F Discovery Board" title="STM32F Discovery Board" /&gt;&lt;/p&gt;

&lt;p&gt;On the STM32 board you see a connector with the label &lt;code&gt;SWD&lt;/code&gt;. We need to pins from there. From the top (where there is
the USB connection), we need the second pin and connect it to &lt;code&gt;SWCLK&lt;/code&gt;. From the top we need also the fourth pin, and
connect it to &lt;code&gt;SWDIO&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The Crownstone (or RFduino, or any other nRF51822 board) needs then also to be powered and grounded. The power can be
fed to the device also from the STM32 board. You can connect one of the pins labeled &lt;code&gt;3V&lt;/code&gt; on the right of the board
to the corresponding pin on your device. Ground can be obtained from many pins, pick one labeled &lt;code&gt;GND&lt;/code&gt;. The VTref pin
from the J-Link does not have a corresponding VTref pin on the ST-Link. On the STM32FDiscovery board probably 
somewhere internally the reference voltage is measured. So, you’ll only have to break out three pins of the cable to
connect it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2. SWDIO
3. GND
4. SWCLK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Use something like a logic analyzer to see if you do things wrong. This is a very, very useful tool that can save you
a lot of time.&lt;/p&gt;

&lt;h1 id="scripts"&gt;Scripts&lt;/h1&gt;

&lt;p&gt;Normally the Crownstone we program with the J-Link from Segger, but if you want to use this cheaper solution, we also
created some files for you at the &lt;a href="https://github.com/dobots/bluenet/tree/master/scripts"&gt;BlueNet repository at github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="combination"&gt;Combination&lt;/h2&gt;

&lt;p&gt;First of all you can combine all the required binaries into one big binary. This is done by the script &lt;code&gt;combine.sh&lt;/code&gt;.
Before you use it, you will need to install &lt;code&gt;srec_cat&lt;/code&gt; on your system. On Ubuntu:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install srecord
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you call the script it basically just runs &lt;code&gt;srec_cat&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./combine.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And you will see that it runs something like if you only want the SoftDevice and the Crownstone for example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;srec_cat /opt/softdevices/s110_nrf51822_7.0.0_softdevice.hex -intel crownstone.bin -binary -offset 0x00016000 -o combined.bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You have to adjust that file on the moment manually to switch between softdevices.&lt;/p&gt;

&lt;h2 id="openocd"&gt;OpenOCD&lt;/h2&gt;

&lt;p&gt;Subsequently, we are gonna set up OpenOCD. You should &lt;strong&gt;not&lt;/strong&gt; use the one from the Ubuntu repository: it is too old. In case you installed it before:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; sudo apt-get remove --purge openocd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Download fom github and compile the source:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd /opt
git clone https://github.com/ntfreak/openocd
sudo aptitude install libtool automake libusb-1.0-0-dev expect
cd openocd
./bootstrap 
./configure --enable-stlink
make
sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will see that there are several files on &lt;a href="https://github.com/dobots/bluenet/tree/master/scripts/openocd"&gt;github&lt;/a&gt; 
that you can use. There is a &lt;code&gt;49-stlinkv2.rules&lt;/code&gt; file that you can use for &lt;code&gt;udev&lt;/code&gt; so that no superuser rights are 
required to use the &lt;code&gt;ST-Link&lt;/code&gt;. There is also an &lt;code&gt;openocd.cfg&lt;/code&gt; file that sets the defaults for the hardware we are
gonna use (the &lt;code&gt;nrf51&lt;/code&gt; series).&lt;/p&gt;

&lt;p&gt;To start OpenOCD you can use our scripts again:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./flash_openocd.sh init
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will start the daemon that subsequently can be talked to over telnet. You can try that if you want to (but this
is not necessary):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./flash_openocd.sh connect
&amp;gt; help
&amp;gt; exit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To actually upload the binaries you have created, you can use:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./flash_openocd.sh upload combined.hex
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will upload the binary you have previously composed to the target. Ready you are!&lt;/p&gt;

&lt;h2 id="debugging"&gt;Debugging&lt;/h2&gt;

&lt;p&gt;If your target does not have pins to break out UART, it might be worth to first try and start experimenting with the
RFduino. That board has enough pins broken out, to see what for example the Crownstone code is actually doing. For
example if you combine a SoftDevice with the wrong bootloader, you will see proper error messages. If you know how
to hook up &lt;code&gt;gdb&lt;/code&gt; over OpenOCD, please feel free to file an issue at &lt;a href="https://github.com/dobots/bluenet/"&gt;https://github.com/dobots/bluenet/&lt;/a&gt; and I will
be happy to update this guide.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2015/01/23/programming-the-nrf51822-with-the-st-link
                <guid>https://dobots.nl/2015/01/23/programming-the-nrf51822-with-the-st-link</guid>
                <pubdate>2015-01-23T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>FeltYou prototype</title>
                <description>
&lt;h1 id="feltyou"&gt;FeltYou&lt;/h1&gt;

&lt;p&gt;The name “FeltYou” is a literal translation of the Dutch word “viltje”, which is what we call chair glides: the things that prevent the chair from scratching the floor.
We figured it would be nice to give the chair glides some weighing functionality, so that you can detect if someone is sitting on the chair. This can come in handy to see if elderly people are not sitting in a chair when they normally are; an indication that something might be wrong. It’s not as intrusive as carrying an accelerometer with you all day for example.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/feltyou_prototype_1.jpg" alt="FeltYou prototype" style="width: 280px" /&gt;
&lt;img src="/attachments/feltyou_prototype_2.jpg" alt="FeltYou prototype" style="width: 280px" /&gt;&lt;/p&gt;

&lt;h2 id="hardware"&gt;Hardware&lt;/h2&gt;
&lt;p&gt;&lt;img src="/attachments/feltyou_circuit.png" alt="Voltage divider" class="float-right" style="width: 200px" /&gt;
For the first prototype I used an &lt;a href="http://www.rfduino.com/product/rfd22301-rfduino-ble-smt/"&gt;rfd module&lt;/a&gt;, since we were &lt;a href="/2014/03/05/rfduino-without-rfduino-code/"&gt;already programming&lt;/a&gt; those. Next, I added a thin &lt;a href="https://www.sparkfun.com/products/11207"&gt;force sensor&lt;/a&gt; and connected that via a voltage divider to the rfd module.
To keep the prototype a bit small, we used paper with a &lt;a href="http://agic.cc/"&gt;silver ink marker&lt;/a&gt; to connect to the battery. The casing is 3D printed and should provide enough protection for the electronics not to be sqeezed.&lt;/p&gt;

&lt;h2 id="code"&gt;Code&lt;/h2&gt;
&lt;p&gt;I slightly modified the code we had for the &lt;a href="http://dobots.nl/products/crownstone.html"&gt;crownstone&lt;/a&gt;, such that it samples the voltage over the pressure sensor on request and puts the result in another characteristic. The code can be found &lt;a href="https://github.com/vliedel/bluenet/tree/vilt"&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2014/11/28/feltyou-prototype
                <guid>https://dobots.nl/2014/11/28/feltyou-prototype</guid>
                <pubdate>2014-11-28T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>The making of the Virtual Memo</title>
                <description>
&lt;h1 id="virtual-memo"&gt;Virtual Memo&lt;/h1&gt;

&lt;p&gt;The virtual memo (or “virtuele memo”) is a gadget created by a group of companies for the &lt;a href="http://wots.nl"&gt;Wots&lt;/a&gt; 
conference. On the moment you can read a bit more on the &lt;a href="http://wots.nl/gadget-virtuele-memo/"&gt;Wots page&lt;/a&gt;, but 
those websites tend to be removed after the event, so this blog post will be a sticky reference to what this thing is
about and what you can do with it.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/memo_logo.jpg" alt="Memo logo" class="float-right" style="width: 500px" /&gt;&lt;/p&gt;

&lt;h2 id="section-a-winning-the-gadget"&gt;Section A: Winning the gadget&lt;/h2&gt;

&lt;p&gt;The app for the virtual memo does have two separate functionalities integrated in one application. The first part will
help the visitor of the conference to win this very gadget! This is done in the form of a quest that shows another
facet of the virtual memo. Namely that of an iBeacon.&lt;/p&gt;

&lt;p&gt;The visitor gets a code, say &lt;code&gt;JV_Q5D8T6P7A9W&lt;/code&gt; which he or she has to enter on the registration page of the smartphone 
application. Subsequently a route is calculated that allows the visitor to, indeed, visit stands of the companies that 
participated in the creation of the virtual memo. At each stand the app proposes an ice breaking question that might
help you to initiate the conversation with the stand owner. Let’s hope it is not “Ah, this app sucks!”. :-) On his or 
her turn the stand owner will give you a passcode that you can fill in that is coupled to your unique participant code
entered at the beginning. Where the virtual memo enters the picture here is in the form of devices at each stand. When
you are near a stand one of the round blobs will change color to indicate that you are close. This is done through
iBeacon advertisements and is not necessary for the quest.&lt;/p&gt;

&lt;p&gt;Because pictures say more than thousand words, you will see here some screenshots of the virtual memo application:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/memo_guide_wots1.png" alt="Memo guide" style="width: 280px" /&gt;
&lt;img src="/attachments/memo_guide_wots2.png" alt="Memo guide" style="width: 280px" /&gt;
&lt;img src="/attachments/memo_guide_wots3.png" alt="Memo guide" style="width: 280px" /&gt;
&lt;img src="/attachments/memo_register.png" alt="Memo registration" style="width: 280px" /&gt;&lt;/p&gt;

&lt;p&gt;And the actual functionality that allows the visitor to fulfill the assignment:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/memo_list.png" alt="Memo list" style="width: 280px" /&gt;
&lt;img src="/attachments/memo_question_dobots.png" alt="Memo question" style="width: 280px" /&gt;
&lt;img src="/attachments/memo_list_filled.png" alt="Memo list filled" style="width: 280px" /&gt;&lt;/p&gt;

&lt;h2 id="section-b-the-gadget-at-home"&gt;Section B: The gadget at home&lt;/h2&gt;

&lt;p&gt;At home you can follow the guide for home usage. You can glue the gadget to your fridge and use it as conventional
Post-it’s®.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/memo_guide_home1.png" alt="Home guide" style="width: 280px" /&gt;
&lt;img src="/attachments/memo_guide_home2.png" alt="Home guide" style="width: 280px" /&gt;
&lt;img src="/attachments/memo_guide_home3.png" alt="Home guide" style="width: 280px" /&gt;&lt;/p&gt;

&lt;p&gt;The memo part is very rudimentary on the moment. The priority of the app development has been in providing the proper
functionality at the one-time Wots event. In the following screenshots you see the actual memo notes. You can navigate 
through them with the arrows at the top-right. Note that for this functionality a working link to the CommonSense
database is required somewhere in the past. The virtual memo does namely not have the functionality to actually store
the messages. This would have been neat by the way, and might be something we will provide later. When a memo is
discovered the screen shows an elaborate string below the note that indicates the id of this memo. Moreover, shortly
an alert will light up the LEDs on the memo. Of course, a virtual memo can only be seen if it is nearby and the “on”
button has been pressed.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/memo_note.png" alt="Note" style="width: 280px" /&gt;
&lt;img src="/attachments/memo_note_magnet.png" alt="Note" style="width: 280px" /&gt;&lt;/p&gt;

&lt;p&gt;You can navigate to other memos, if you are so lucky to have multiple, by navigating to “Alle memos” in the menu:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/memo_menu.png" alt="Menu" style="width: 280px" /&gt;
&lt;img src="/attachments/memo_overview.png" alt="Menu" style="width: 280px" /&gt;&lt;/p&gt;

&lt;p&gt;If you select the “okay” button at the top-right of a memo, it will briefly blink and send a sound alert. Then it will
be selected in the software and navigate to the stack of memo notes corresponding to that device.&lt;/p&gt;

&lt;h2 id="for-nerds"&gt;For nerds&lt;/h2&gt;

&lt;p&gt;There are a lot of improvements possible! Please, help us in whatever way you can! This project was not paid for by the
organizers (but we are happy we can present ourselves in this way). It is not finished!&lt;/p&gt;

&lt;p&gt;The original developers for the iOS application didn’t find the time to do so. Hence, this part of the app development
was a learning experience for us. Let’s say it like this, one of our Apple fanboys didn’t like iOS development so much
afterwards. :-) It is really long-winded to get into the Apple store. Something we anticipated and pushed for it, 
although not all bugs were yet pletted. What this means, is that the iOS app will always lag behind the Android one,
where an application shows up in a few hours after submission.&lt;/p&gt;

&lt;p&gt;You can find all the code at &lt;a href="https://github.com/almende/virtuele-memo"&gt;github&lt;/a&gt;! This code runs, although not smoothly,
on both Android and iOS. This means you might also be able to use parts of the code if you want to, for different 
projects. Perhaps even parts of it are useful in the browser. The application is a Cordova application (the commercial
variant is called PhoneGap).&lt;/p&gt;

&lt;p&gt;You will find for example &lt;a href="https://github.com/almende/virtuele-memo/blob/master/cordova/com.almende.VirtualMemo/www/js/senseapi.js"&gt;code&lt;/a&gt; to communicate with the CommonSense API or you can find out easily which &lt;a href="https://github.com/almende/virtuele-memo/tree/master/cordova/com.almende.VirtualMemo"&gt;plugins&lt;/a&gt; work both on Android and iOS with respect to Bluetooth Low-Energy or iBeacon support.&lt;/p&gt;

&lt;p&gt;Please, help us by submitting issues to &lt;a href="https://github.com/almende/virtuele-memo/issues"&gt;github&lt;/a&gt;.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2014/09/16/the-making-of-the-virtual-memo
                <guid>https://dobots.nl/2014/09/16/the-making-of-the-virtual-memo</guid>
                <pubdate>2014-09-16T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>European webshops for DIY robots</title>
                <description>
&lt;h1 id="sourcing-components"&gt;Sourcing components&lt;/h1&gt;

&lt;p&gt;Robots are fun to build yourself. And although there are a lot of Do-It-Yourself websites describing circuits and alike, it might be tough to find websites actually selling the components you need. This is a short review of different websites that help you in actually getting those components when you live in Europe. Companies like &lt;a href="https://www.adafruit.com/"&gt;Adafruit&lt;/a&gt;, &lt;a href="http://www.mouser.com"&gt;Mouser&lt;/a&gt; (see &lt;a href="http://forums.adafruit.com/viewtopic.php?f=26&amp;amp;p=84744"&gt;this forum post&lt;/a&gt; on saving on shipping costs), &lt;a href="https://www.sparkfun.com/"&gt;SparkFun&lt;/a&gt;, &lt;a href="http://www.digikey.com/"&gt;Digi-Key&lt;/a&gt;, &lt;a href="http://nl.farnell.com/"&gt;Farnell&lt;/a&gt;, &lt;a href="http://www.newark.com/"&gt;Newark&lt;/a&gt; and the somewhat lesser known &lt;a href="http://www.pololu.com/"&gt;Pololu&lt;/a&gt;, &lt;a href="http://www.robotshop.com/"&gt;RobotShop&lt;/a&gt;, and &lt;a href="alliedelec.com"&gt;Allied&lt;/a&gt; are cool but they ship from the other side of the Atlantic ocean. There are also distributors operating from China, such as &lt;a href="http://www.seeedstudio.com/"&gt;Seeed Studio&lt;/a&gt; and &lt;a href="http://www.dfrobot.com/"&gt;DFRobot&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="european-webshops"&gt;European webshops&lt;/h2&gt;

&lt;p&gt;One of the companies which is based in France is &lt;a href="www.lextronic.fr"&gt;Lextronic&lt;/a&gt;, another one is &lt;a href="http://www.cooking-hacks.com/"&gt;Cooking Hacks&lt;/a&gt; based in Portugal, and in Germany there is of course &lt;a href="http://www.conrad.com/"&gt;Conrad&lt;/a&gt; and lesser known &lt;a href="http://www.pollin.de"&gt;Pollin&lt;/a&gt;. In Belgium we have &lt;a href="http://www.kidlogic.be/"&gt;KidLogic&lt;/a&gt; and &lt;a href="http://www.rato.be/"&gt;Rato&lt;/a&gt;. In Switzerland there is &lt;a href="https://www.distrelec.com"&gt;Distrelec&lt;/a&gt;. In Bulgeria there is &lt;a href="https://www.olimex.com"&gt;Olimex&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We are a Dutch company, so of course, we would like to give some pointers about how to source components in Holland. Even in our little country, you will find several resellers, such as &lt;a href="http://www.antratek.nl" target="webshop"&gt;Antratek&lt;/a&gt;, &lt;a href="https://www.iprototype.nl/" target="webshop"&gt;iPrototype&lt;/a&gt;, &lt;a href="http://sossolutions.nl/" target="webshop"&gt;Sos Solutions&lt;/a&gt;, &lt;a href="http://www.eztronics.nl/" target="webshop"&gt;EZtronics&lt;/a&gt;, &lt;a href="http://www.hobbyelectronica.nl/" target="webshop"&gt;HobbyElectronica&lt;/a&gt;, &lt;a href="http://www.vanallesenmeer.nl/" target="webshop"&gt;VanAllesEnMeer&lt;/a&gt;, &lt;a href="http://www.hackerstore.nl/" target="webshop"&gt;Hackerstore&lt;/a&gt;, &lt;a href="http://www.kiwi-electronics.nl" target="webshop"&gt;Kiwi Electronics&lt;/a&gt;, &lt;a href="http://www.okaphone.com/" target="webshop"&gt;Okaphone&lt;/a&gt;, &lt;a href="http://www.htfelectronics.nl" target="webshop"&gt;HTF Electronics&lt;/a&gt;, &lt;a href="http://www.budgetronics.nl/" target="webshop"&gt;BudgeTronics&lt;/a&gt;, &lt;a href="http://mycom.nl/" target="webshop"&gt;MyCom&lt;/a&gt;, &lt;a href="http://www.voc-electronics.com/" target="webshop"&gt;VOC Electronics&lt;/a&gt;, &lt;a href="https://www.tinytronics.nl" target="webshop"&gt;TinyTronics&lt;/a&gt;, &lt;a href="http://www.eoo-bv.nl/" target="webshop"&gt;Electronica Onderdelen Online&lt;/a&gt;, &lt;a href="http://www.kent-electronics.nl/" target="webshop"&gt;Kent Electronics&lt;/a&gt;, &lt;a href="http://www.dil.nl/" target="webshop"&gt;DIL Elektronika&lt;/a&gt;, &lt;a href="http://www.hecke.com/" target="webshop"&gt;Hecke&lt;/a&gt;, &lt;a href="http://brigatti.nl/" target="webshop"&gt;Brigatti&lt;/a&gt;, &lt;a href="http://www.aduis.nl/technische-accessoires-pg33.aspx" target="webshop"&gt;Aduis&lt;/a&gt;. If you click on any of these it will open in a new tab, but the same one for all, which I labelled “webshop”.&lt;/p&gt;

&lt;p&gt;If you want to search for them yourself, it is smart not to choose “robot” as the keyword. Use for example “Arduino” or “Raspberry”. A company like &lt;a href="http://www.sunbedded.nl/en/"&gt;sunbedded&lt;/a&gt; or &lt;a href="http://www.duinos.nl/"&gt;Duinos&lt;/a&gt; specializes in Arduino-based products for example. A website like &lt;a href="http://www.raspberrystore.nl/"&gt;Raspberry Store&lt;/a&gt; or &lt;a href="http://www.minifo.com/"&gt;Minifo&lt;/a&gt; specializes in the Raspberry PI, as does &lt;a href="http://www.gadgetpark.nl/"&gt;GadgetPark&lt;/a&gt; although they include mediaplayers in general as well. A combination with home automation is also possible as for example is the case for the &lt;a href="http://www.domotica-shop.nl/"&gt;domotica-shop&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;At times you might also find components on general computer webshops such as &lt;a href="http://www.informatique.nl/"&gt;Informatique&lt;/a&gt;, or other websites in the Pricewatch of &lt;a href="http://tweakers.net"&gt;Tweakers&lt;/a&gt;. But, they tend to be quite expensive and have a very small inventory. However, if the only thing you need is a &lt;a href="http://www.raspberrypi.org/"&gt;Raspberry PI&lt;/a&gt;, they can provide!&lt;/p&gt;

&lt;h2 id="pcb-assembly"&gt;PCB assembly&lt;/h2&gt;

&lt;p&gt;Also, when you want to go from breadboards to PCBs, the company &lt;a href="http://www.eurocircuits.com/"&gt;EuroCircuits&lt;/a&gt; can not be unmentioned. For around 100 bucks you can have your own PCB and you will only need to solder the components yourself. Regretfully, placing components, PCB assembly, is still incredibly expensive and we have not found any company who is willing to do this for under a 1000 euros for a few prototypes. An example of a company providing a turnkey solution to PCB assembly is &lt;a href="http://www.nexpcb.com/collections/pcba-turnkey-solution/products/prototype-pcba-turnkey-solution"&gt;NexPCB&lt;/a&gt; which offers it for $1199,- for 10 PCBs. However, it is much likely that a service like &lt;a href="http://www.seeedstudio.com/propagate/index.php?controller=estimation&amp;amp;action=calculate"&gt;Seeed Studio Propagate&lt;/a&gt;, &lt;a href="http://www.circuitology.com/assembly/"&gt;Circuitology&lt;/a&gt;, &lt;a href="http://www.elecrow.com/pcb-assembly-p-366.html"&gt;Elecrow&lt;/a&gt;, &lt;a href="http://smart-prototyping.com/"&gt;Smart Prototyping&lt;/a&gt;, or &lt;a href="http://imall.iteadstudio.com/open-pcb/pcb-prototyping.html"&gt;ITead&lt;/a&gt; is much cheaper. Now you know some names it is easier to come across blogs and forum posts comparing the different services such as for example on &lt;a href="http://www.youritronics.com/seeed-studio-vs-itead-studio-vs-osh-park/"&gt;YourITronics&lt;/a&gt;. If you have good experiences with any of these companies for PCB assembly, we would love to hear!&lt;/p&gt;

&lt;p&gt;If you want any (or your) company to be added, feel free to drop an email or comment. We are happy to provide a link to your business as long as it resides in Europe! A little bit of a positive discrimination for a change. :-)&lt;/p&gt;

</description>
                <link>https://dobots.nl/2014/08/02/european-webshops-for-diy-robots
                <guid>https://dobots.nl/2014/08/02/european-webshops-for-diy-robots</guid>
                <pubdate>2014-08-02T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Linux and BLE</title>
                <description>
&lt;h1 id="linux-and-bluetooth-low-energy"&gt;Linux and Bluetooth Low-Energy&lt;/h1&gt;

&lt;p&gt;Although not strictly Linux, Android being a VM on top of a Linux kernel, and iOS being based on top of Darwin, the Unix-like OS from Apple, I just add for your convenience.&lt;/p&gt;

&lt;h2 id="android"&gt;Android&lt;/h2&gt;

&lt;p&gt;On Android you definitely need the &lt;a href="https://play.google.com/store/apps/details?id=no.nordicsemi.android.mcp&amp;amp;hl=en"&gt;nRF Master Control Panel&lt;/a&gt;. This is a great tool which you can even use to upload new firmware to a device. It is very often updated and seems to be used internally at Nordic as well. Bugs pop up so now and then, but they plet them at a fast rate.&lt;/p&gt;

&lt;h2 id="ios"&gt;iOS&lt;/h2&gt;

&lt;p&gt;A convenient tool on iOS is &lt;a href="https://itunes.apple.com/us/app/ble-utility/id606210918?mt=8"&gt;BLE Utility&lt;/a&gt;. It is graphical nice and clean and it shows all services and characteristics you need. You can easily set a characteristic that you have defined yourself on a custom BLE device.&lt;/p&gt;

&lt;h2 id="ubuntu"&gt;Ubuntu&lt;/h2&gt;

&lt;p&gt;There are plenty of BLE dongles you can buy if your laptop (or raspberry pi) does not support Bluetooth Low-Energy, Bluetooth 4, Bluetooth Smart, or iBeacon (whatever they call it nowadays) out of the box. We ordered a few at &lt;a href="http://www.miniinthebox.com/nl/3mbps-bluetooth-mvo-usb-dongle-met-20m-werkbereik_p367687.html"&gt;MiniInTheBox&lt;/a&gt; that have &lt;a href="http://www.csr.com/products/technology/low-energy"&gt;CSR&lt;/a&gt; (Cambridge Silicon Radio) as label.&lt;/p&gt;

&lt;p&gt;Type in &lt;code&gt;hciconfig&lt;/code&gt; before and after you plug in the new dongle.&lt;/p&gt;

&lt;p&gt;You probably see something along the lines:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hci1:	Type: BR/EDR  Bus: USB
	BD Address: 00:1A:7D:DA:71:13  ACL MTU: 310:10  SCO MTU: 64:8
	UP RUNNING PSCAN 
	RX bytes:33907 acl:14 sco:0 events:1406 errors:0
	TX bytes:1524 acl:18 sco:0 commands:63 errors:0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And another device that represents your good old-fashioned standard Bluetooth radio.&lt;/p&gt;

&lt;p&gt;We can reach all new fancy BLE functionality through an old, familiar tool, &lt;code&gt;hcitool&lt;/code&gt; with function that are now prepended with &lt;code&gt;le&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;✗ sudo hcitool -i hci1 lescan
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;LE Scan ...
CF:72:4E:70:A6:DB Crown
CF:72:4E:70:A6:DB (unknown)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here you see one of our &lt;a href="/products/crownstone"&gt;Crownstone&lt;/a&gt;s.&lt;/p&gt;

&lt;p&gt;Connecting to it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;✗ sudo gatttool -i hci1 -b CF:72:4E:70:A6:DB -I
[   ][CF:72:4E:70:A6:DB][LE]&amp;gt; connect
Connecting... connect error: Connection refused (111)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fails… And now you get a time saving command from me. You first tell that you don’t care about your own address, which can be random:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo hcitool -i hci1 lecc --random CF:72:4E:70:A6:DB
Connection handle 70

✗ sudo gatttool -i hci1 -b CF:72:4E:70:A6:DB --interactive
[   ][CF:72:4E:70:A6:DB][LE]&amp;gt; connect
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can get the services:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[CON][CF:72:4E:70:A6:DB][LE]&amp;gt; primary
attr handle: 0x0001, end grp handle: 0x0007 uuid: 00001800-0000-1000-8000-00805f9b34fb
attr handle: 0x0008, end grp handle: 0x000b uuid: 00001801-0000-1000-8000-00805f9b34fb
attr handle: 0x000c, end grp handle: 0xffff uuid: 00002220-0000-1000-8000-00805f9b34fb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And the characteristics:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[CON][CF:72:4E:70:A6:DB][LE]&amp;gt; characteristics
handle: 0x0002, char properties: 0x0a, char value handle: 0x0003, uuid: 00002a00-0000-1000-8000-00805f9b34fb
handle: 0x0004, char properties: 0x02, char value handle: 0x0005, uuid: 00002a01-0000-1000-8000-00805f9b34fb
handle: 0x0006, char properties: 0x02, char value handle: 0x0007, uuid: 00002a04-0000-1000-8000-00805f9b34fb
handle: 0x0009, char properties: 0x20, char value handle: 0x000a, uuid: 00002a05-0000-1000-8000-00805f9b34fb
handle: 0x000d, char properties: 0x0e, char value handle: 0x000e, uuid: 00002201-0000-1000-8000-00805f9b34fb
handle: 0x0011, char properties: 0x0e, char value handle: 0x0012, uuid: 00000125-0000-1000-8000-00805f9b34fb
handle: 0x0015, char properties: 0x0e, char value handle: 0x0016, uuid: 00000124-0000-1000-8000-00805f9b34fb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Of course &lt;code&gt;help&lt;/code&gt; works. Let us now try to turn on/off the Crownstone with the &lt;a href="https://github.com/mrquincle/bluenet"&gt;BlueNet software running&lt;/a&gt;. I know that it should be uuid 0x0124 for the characteristic, but let us figure out which handle that is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[CON][CF:72:4E:70:A6:DB][LE]&amp;gt; char-desc 0x0015
handle: 0x0015, uuid: 2803
handle: 0x0016, uuid: 0124
handle: 0x0017, uuid: 2901
handle: 0x0018, uuid: 2904
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Apparently handle 0x0016, it is empty on read:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[CON][CF:72:4E:70:A6:DB][LE]&amp;gt; char-read-hnd 0x0016
Characteristic value/descriptor: 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, we can write to it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[CON][CF:72:4E:70:A6:DB][LE]&amp;gt; char-write-cmd 0x0016 0xFF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And the switch turns on!&lt;/p&gt;

</description>
                <link>https://dobots.nl/2014/07/23/linux-and-ble
                <guid>https://dobots.nl/2014/07/23/linux-and-ble</guid>
                <pubdate>2014-07-23T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>BLE, doBeacon, a Virtual Memo</title>
                <description>
&lt;h1 id="what-is-this-ibeacon-stuff"&gt;What is this iBeacon stuff?&lt;/h1&gt;

&lt;p&gt;Bluetooth Low Energy is becoming largely available. At DoBots we have always promoted technology that can hitchhike on the success of smartphones and other personal devices around smartphones. The use of Bluetooth Low Energy (BLE), also called Bluetooth Smart, is therefore one of our core technologies.&lt;/p&gt;

&lt;p&gt;It is quite remarkable that the Bluetooth consortium allowed Apple to claim an extremely impoverished subset of the BLE standard and call it iBeacon. This term &lt;a href="https://en.wikipedia.org/wiki/IBeacon"&gt;iBeacon&lt;/a&gt; is a trademark and cannot be used without an &lt;a href="https://developer.apple.com/ibeacon/"&gt;iBeacon License&lt;/a&gt;. These kind of products are in the market at 3 for €100,- at &lt;a href="http://www.beaconic.nl/buy-beacons/"&gt;Beaconic&lt;/a&gt;, 3 for $100,- at &lt;a href="http://estimote.com/"&gt;Estimote&lt;/a&gt;, 3 for $116,- at &lt;a href="http://todhq.com/purchase.html"&gt;Tōd&lt;/a&gt;. This is exactly the same technology as for example the crowdsource funded &lt;a href="https://www.sticknfind.com/store.aspx"&gt;stick-n-find&lt;/a&gt;, which comes with 4 in a package at around $90,-.&lt;/p&gt;

&lt;p&gt;Perhaps it is worth to explain how little Apple did. The standard BLE specification divides the spectrum into channels. There are 37 channels for data communication, and there are 3 channels for advertising. The advertising channels are spread over the spectrum to be resilient against interference on the crowded 2.4GHz band (where also normal Bluetooth and Wi-Fi resides). There are actually two modes of operation, let’s call the first mode &lt;a href="https://en.wikipedia.org/wiki/Simplex_communication"&gt;simplex&lt;/a&gt; and the second mode &lt;a href="http://bit.ly/1qcD5RX"&gt;duplex&lt;/a&gt; according to the telecommunication tradition. In simplex mode there is a broadcasting party and a receiving party. Broadcasting in BLE is called &lt;strong&gt;advertising&lt;/strong&gt;, and receiving is called &lt;strong&gt;scanning&lt;/strong&gt;. The duplex case is handled by classical master-slave roles. Now, we encounter a nice property of BLE: A device might only support an advertising mode and hence only requires circuitry for the transmission of messages. This is exactly an iBeacon with respect to &lt;em&gt;hardware&lt;/em&gt; required. Subsequently, the content of the message that is sent in the BLE-dictated advertising mode is up to the user. Here Apple got a brilliant invention on par with the wheel, the steam engine, the laser, the transistor… Namely, sending an &lt;em&gt;identifier&lt;/em&gt; within an advertising message. :scratching_head_icon:&lt;/p&gt;

&lt;p&gt;Okay! We can do that of course. Our code for the &lt;a href="https://github.com/mrquincle/bluenet"&gt;nRF51822&lt;/a&gt; builds on top of code by Christopher Mason, is open-source, and can do already much more. You can start experimenting with this code if you buy a &lt;a href="https://www.nordicsemi.com/eng/Products/Bluetooth-R-low-energy/nRF51822-Development-Kit"&gt;Nordic development kit for the nRF51822&lt;/a&gt;, which is around €100,-. Note, that you absolutely do not need that if you only want to communicate with iBeacons!&lt;/p&gt;

&lt;p&gt;Our super-elaborate technology to be able to use advertising for proximity purposes, we call &lt;strong&gt;doBeacon&lt;/strong&gt;. This is one of the functionalities you will find in our &lt;a href="http://dobots.nl/products/crownstone.html"&gt;crownstone&lt;/a&gt; product.&lt;/p&gt;

&lt;h1 id="virtual-memo"&gt;Virtual Memo&lt;/h1&gt;

&lt;p&gt;The &lt;a href="http://wots.nl"&gt;WOTS&lt;/a&gt;, World of Technology &amp;amp; Science, conference, will be held in Utrecht, the Jaarbeurs. It is &lt;a href="https://registration2.n200.com/survey/0o33vcrxufeeo?check=1"&gt;free&lt;/a&gt; and allows you to see our technology at our stand! With a consortium of companies we are creating a gadget for visitors, which is called the &lt;a href="http://wots.nl/gadget-virtuele-memo"&gt;Virtuele Memo&lt;/a&gt;. It is meant as an electronic Post-it® that you can use to leave messages as on say a fridge, such as “Please, don’t forget the milk, honey!”, or a reminder for yourself such as “Hairdresser time!”.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/e6o2Mrdqt5g?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;p&gt;A very short clip with flashing LEDs. This is to be expected of a movie shot by an engineer. :-)&lt;/p&gt;

&lt;p&gt;The Virtual Memo (or Virtuele Memo in Dutch), surprise!, uses BLE functionality. It is exactly a beacon as described before. So, how can you leave messages? This is done through the internet. Your phone comes in close proximity to the fridge, and a note that you want to attach to it, is uploaded to a server. By the way, this is is the &lt;a href="http://www.sense-os.nl/developers"&gt;CommonSense&lt;/a&gt; internet-of-things infrastructure from Sense, the sister company of DoBots. Okay, so, suppose now your roommate is coming home. Her phone will come in proximity with the Virtuele Memo on the fridge. It will then contact the server and download new messages for her. The app on the phone can be very energy-efficient in this way. It only needs contact with a server when it gets a trigger over BLE.&lt;/p&gt;

&lt;p&gt;DoBots is responsible for developing the smartphone application. It is based on &lt;a href="https://cordova.apache.org/"&gt;Cordova&lt;/a&gt;, so it uses HTML, CSS, and Javascript on the phone. Cordova is the codebase at the Apache Software Foundation, and originally developed by Adobe/Nitobi under the name PhoneGap (which still exists under its own name). See for the entire story this blog post at &lt;a href="http://phonegap.com/2012/03/19/phonegap-cordova-and-what%E2%80%99s-in-a-name/"&gt;PhoneGap&lt;/a&gt;. This means it can run on Android and iOS both! We decided upon making our application open source, so you can find it at &lt;a href="https://github.com/almende/virtuele-memo"&gt;https://github.com/almende/virtuele-memo&lt;/a&gt;. This software makes use of e.g. the &lt;a href="https://github.com/senseobservationsystems/commonsense-javascript-lib"&gt;commonsense-javascript-lib&lt;/a&gt; and a &lt;a href="https://github.com/mrquincle/BluetoothLE"&gt;BluetoothLE&lt;/a&gt; plugin for Cordova.&lt;/p&gt;

&lt;h1 id="ble-and-beyond"&gt;BLE and Beyond&lt;/h1&gt;

&lt;p&gt;DoBots does not believe that the IP (intellectual property) resides on the level of such applications. But again, our technology offering is around services that can make use of BLE technology on a large scale, such as the &lt;a href="http://dobots.nl/products/crownstone.html"&gt;crownstone&lt;/a&gt;, which has the potential to become ubiquitous. This means namely that interesting algorithms, hence, nice IP, are required, for example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;recognition of &lt;strong&gt;activities of daily living&lt;/strong&gt; (ADLs) to support independent living (only anomalies are communicated to family, and finally caretakers, see also the &lt;a href="http://ppats.wordpress.com/"&gt;PPATS project&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;recognition of devices, &lt;strong&gt;standby power killers&lt;/strong&gt;, but also standby for an &lt;strong&gt;entire building&lt;/strong&gt; without turning off the fridge!&lt;/li&gt;
  &lt;li&gt;indoor localization, the use of multiple doBeacons for more &lt;strong&gt;accurate positioning&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For indoor localization often scenarios are described that are a bit fancy. However, in a business context, knowing which facilities are in use, really saves money! Cleaning unused offices should be a lot cheaper!&lt;/p&gt;

&lt;p&gt;If you want to know in general why DoBots puts so much effort in consumer electronics such as BLE, come also to one of the &lt;a href="http://wots.nl/elektronicatrendsindtoepassing/"&gt;seminars&lt;/a&gt; at WOTS. Anne van Rossum is going to talk about &lt;a href="http://wots.nl/slimme-fabriek/"&gt;smart factories&lt;/a&gt; in the &lt;a href="http://wots.nl/elektronicatrendsindtoepassing/"&gt;industry&lt;/a&gt; where also connections will be made with Almende projects such as &lt;a href="http://www.almende.com/arum"&gt;ARUM&lt;/a&gt; which focus on agile manufacturing.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2014/07/15/ble-dobeacon-a-virtual-memo
                <guid>https://dobots.nl/2014/07/15/ble-dobeacon-a-virtual-memo</guid>
                <pubdate>2014-07-15T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>A nonparametric Bayesian lesson</title>
                <description>
&lt;h1 id="bayesian"&gt;Bayesian&lt;/h1&gt;

&lt;p&gt;It is quite hard to withstand the reasoning of Jaynes (it is a pity he couldn’t finish his book) in “&lt;a href="http://www.amazon.com/Probability-Theory-The-Logic-Science/dp/0521592712"&gt;Probability Theory&lt;/a&gt;”. In chapter 5 Jaynes comes with a beautiful example that shows that exactly the same information can lead to totally different conclusions. It is very illustrative considering religious, ethical, or political views. I can only do justice by using an extended quote (forgive me!):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The new information &lt;script type="math/tex"&gt;D&lt;/script&gt; is ‘Mr &lt;script type="math/tex"&gt;N&lt;/script&gt; has gone on TV with a sensational claim that a commonly used drug is unsafe’ and three viewers, Mr &lt;script type="math/tex"&gt;A&lt;/script&gt;, Mr &lt;script type="math/tex"&gt;B&lt;/script&gt;, and Mr &lt;script type="math/tex"&gt;C&lt;/script&gt;, see this. Their prior probabilities that the drug is safe are &lt;script type="math/tex"&gt;(0.9, 0.1, 0.9)&lt;/script&gt;, respectively; i.e. initially Mr &lt;script type="math/tex"&gt;A&lt;/script&gt; and Mr &lt;script type="math/tex"&gt;C&lt;/script&gt; were believers in the safety of the drug, Mr &lt;script type="math/tex"&gt;B&lt;/script&gt; a disbeliever. 
But they interpret the information D very differently, because they have different views about the reliability of Mr &lt;script type="math/tex"&gt;N&lt;/script&gt;. They all agree that, if the drug had really been proved unsafe, Mr &lt;script type="math/tex"&gt;N&lt;/script&gt; would be right there shouting it; that is, their probabilities &lt;script type="math/tex"&gt;P(D\vert\overline{D}I)&lt;/script&gt; are &lt;script type="math/tex"&gt;(1,1,1)&lt;/script&gt;; but Mr &lt;script type="math/tex"&gt;A&lt;/script&gt; trusts his honesty while Mr &lt;script type="math/tex"&gt;C&lt;/script&gt; does not. Their probabilities &lt;script type="math/tex"&gt;P(D\vert SI)&lt;/script&gt; that, if the drug is safe, Mr &lt;script type="math/tex"&gt;N&lt;/script&gt; would say that it is unsafe, are &lt;script type="math/tex"&gt;(0.01, 0.3, 0.99)&lt;/script&gt;, respectively.&lt;/p&gt;

  &lt;p&gt;Applying Bayes theorem &lt;script type="math/tex"&gt;P(S\vert DI)=P(S\vert I)P(D\vert SI)/P(D\vert I)&lt;/script&gt; […] we find their posterior probabilities that the drug is safe to be &lt;script type="math/tex"&gt;(0.083, 0.032, 0.899)&lt;/script&gt;, respectively. Put verbally, they have reasoned as follows:&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;
      &lt;p&gt;Mr &lt;script type="math/tex"&gt;A&lt;/script&gt;: ‘Mr &lt;script type="math/tex"&gt;N&lt;/script&gt; is a fine fellow, doing a notable public service. I had thought the drug to be safe from other evidence, but he would not knowingly misrepresent the facts; therefore hearing his report leads me to change my mind and think that the drug is unsafe after all. My belief in safety is lowered […] so I will not buy any more’&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Mr &lt;script type="math/tex"&gt;B&lt;/script&gt;: ‘Mr &lt;script type="math/tex"&gt;N&lt;/script&gt; is an erratic fellow, inclined to accept adverse evidence too quickly. I was already convicned that the drug is unsafe; but even if it is safe he might be carried away into saying otherwise. So, hearing his claim does strengthen my opinion, but only [a bit]. I would never under any circumstances use the drug.’&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Mr &lt;script type="math/tex"&gt;C&lt;/script&gt;: ‘Mr &lt;script type="math/tex"&gt;N&lt;/script&gt; is an unscrupulous rascal, who does everything in his power to stir up trouble by sensational publicity. The drug is probably safe, but he would almost certainly claim it is unsafe whatever the facts. So hearing his claim has proactically no effect on my confidence that the drug is safe. I will continue to buy it and use it.’&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2 id="nonparametric-bayesian"&gt;Nonparametric Bayesian&lt;/h2&gt;

&lt;p&gt;This Bayesian prelude makes it clear that the structure of the prior is very important. Very interesting problems can be solved if the prior gets sufficient structure. A nice workshop has been held at Como, Italy, on Applied Bayesian Nonparametrics, from the Applied Bayesian Statistics School, &lt;a href="http://www.mi.imati.cnr.it/conferences/abs14.html"&gt;ABS14&lt;/a&gt;. The lecturers were &lt;a href="http://www.cs.berkeley.edu/~jordan/"&gt;Michael Jordan&lt;/a&gt; and &lt;a href="http://www.stats.ox.ac.uk/~caron/"&gt;Francois Caron&lt;/a&gt; who talked about Dirichlet processes and Beta processes, respectively. If you don’t know this Michael Jordan, you might instead have heard one time or another from one of his students, under which e.g. Yoshua Bengio, Tommi Jaakkola, Andrew Ng, Emanuel Todorov, and Daniel Wolpert. But, enough small talk, let’s face the truth.&lt;/p&gt;

&lt;h3 id="the-chinese-restaurant-process"&gt;The Chinese Restaurant Process&lt;/h3&gt;

&lt;p&gt;It sounds a bit childish, but the Chinese Restaurant Process is an actual stochastic process with great expressive power. Imagine a restaurant with a seamingly infinite number of tables. The first customer enters the restaurant and picks the first table. A second customer enters the restaurant and sits with the first customer with probability &lt;script type="math/tex"&gt;\frac{n_{table}}{\alpha+n}&lt;/script&gt; and at a new table with probability &lt;script type="math/tex"&gt;\frac{\alpha}{\alpha+n-1}&lt;/script&gt;. Here &lt;script type="math/tex"&gt;\alpha&lt;/script&gt; is a so-called concentration parameter which causes more or less tables to occupied with respect to the number of customers. For example, &lt;script type="math/tex"&gt;\alpha=1&lt;/script&gt; gives a probability of opening a new table for the second person &lt;script type="math/tex"&gt;n_i=2&lt;/script&gt; equal to &lt;script type="math/tex"&gt;1/2&lt;/script&gt;. The peculiarity of this process is that if you check the probability of a random two people, say customer &lt;script type="math/tex"&gt;n_i=64&lt;/script&gt; and &lt;script type="math/tex"&gt;n_j=94&lt;/script&gt;, the probability they sit at the same table is also exactly that, a &lt;script type="math/tex"&gt;1/2&lt;/script&gt;! The number of people sitting at the same table as the first customer is on average the same number of people sitting at the same table as the last customer! These properties have to do with the fact that this is an exchangeable process. You can hone your intuition with the &lt;a href="http://www.stats.ox.ac.uk/~caron/code/abs2014/html/BNP_clustering.html"&gt;assignments from Francois&lt;/a&gt; if you like.&lt;/p&gt;

&lt;p&gt;It is proven by &lt;a href="https://en.wikipedia.org/wiki/De_Finetti%E2%80%99s_theorem"&gt;de Finetti&lt;/a&gt; that a process with exchangeable observations can be written in such way that there are underlying hidden (latent) variables that are i.i.d. (independent and identically distributed) according to some unknown distribution. It is interesting (and not trivial) that such a distribution can be found! The distribution that corresponds to the Chinese Restaurant Process is one of the most used processes in nonparametric Bayesian methods, namely the &lt;a href="https://en.wikipedia.org/wiki/Dirichlet_process"&gt;Dirichlet process&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id="hierarchy"&gt;Hierarchy&lt;/h3&gt;

&lt;p&gt;Why are these stochastic processes so interesting? With a Dirichlet process it is possible to model a generative procedure in which data generates clusters ad infinitum. New data can always need to new clusters. These kind of mechanisms might exist in other machine learning methods. For example, in &lt;a href="http://www.scholarpedia.org/article/Adaptive_resonance_theory"&gt;ART&lt;/a&gt; new category nodes are added during the learning process. However, nonparametric Bayesian hierarchies is the first method that describes how to this in a full Bayesian setting. This means no adhoceries. The inference can be done over the structure of the model and the parameters of the model simultaneously. In other words, the model reasons over the parameter per table, as well as the number of tables at the same time.&lt;/p&gt;

&lt;h3 id="examples"&gt;Examples&lt;/h3&gt;

&lt;p&gt;To explain how inference proceeds is something for another blog post (check the &lt;a href="https://github.com/mrquincle/aim_modules/tree/master/DirichletModule"&gt;code&lt;/a&gt;). It is more interesting for now to see how these methods have been applied already.&lt;/p&gt;

&lt;p&gt;An illustrative example is from Del Pero et al. in &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Del_Pero_Understanding_Bayesian_Rooms_2013_CVPR_paper.pdf"&gt;Understanding bayesian rooms using composite 3d object models&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src="/slides/abs14/images/delpero.png" alt="Chairs" title="Chairs" /&gt;`&lt;/p&gt;

&lt;p&gt;One of the chairs you can see very well, but the other is occluded by the table. To perform inference over this structure requires a presentation of a chair and the ability of reasoning over multiple of those composed objects. Only then it becomes feasible to infer the full chair behind this table.&lt;/p&gt;

&lt;p&gt;Another example is speaker diarization. Imagine an organized meeting, there are people talking, supposedly not at the same time, but sequentially. How do we perform inference on the number of speakers present and how can we build an inference engine that benefits from Bob talking at time &lt;script type="math/tex"&gt;t[0] \cdots t[30]&lt;/script&gt;, and him talking at time &lt;script type="math/tex"&gt;t[120] \cdots t[180]&lt;/script&gt;? The system should all the time improve from Bob talking!&lt;/p&gt;

&lt;p&gt;This can be solved by a so-called sticky Hierarchical Dirichlet Process defined on a very well-known model structure, a Hidden Markov Model (see &lt;a href="http://projecteuclid.org/download/pdfview_1/euclid.aoas/1310562215"&gt;A sticky HDP-HMM with application to speaker diarization&lt;/a&gt;). A Hidden Markov Model, is built on a Markovian assumption, hence its name. This means that it is impossible to store long-term dependencies as required by this application: a person talking at the beginning and the end of a conversation. Remarkably is that this model actually performed on par with state-of-the-art algorithms that were very specific to this application!&lt;/p&gt;

&lt;h3 id="slides"&gt;Slides&lt;/h3&gt;

&lt;p&gt;The problems you will encounter by applying nonparametric Bayesian methods in the real world are manifold. Foremost, the sampling methods to approximate a full Bayesian method, are not yet adapted to these hierarchical schemes. Although it might seem that having an abstraction of say, a chair, would allow you to move around, copy, and delete this entire chair through an inference process, these kind of abstraction handling have not been handed to the sampling algorithms themselves. All approximation methods, be it variational methods, or sampling methods, such as slice sampling or Gibbs sampling, are suddenly quite non-Bayesian in nature! To incorporate a prior (such as a hierarchical structure) in the sampling procedure itself, I have yet to see it. The methods are general, in the sense, that the sampling is able to sample any probability density function and is not able to take advantage of prior knowledge about this function (except for approximating a Bayesian formulation that entails this structure).&lt;/p&gt;

&lt;p&gt;You can find my presentation with my questions about how to apply nonparametric Bayesian methodology to simple line estimation as in RANSAC or Hough &lt;a href="/slides/abs14/nonparametric.html"&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2014/06/25/a-nonparametric-bayesian-lesson
                <guid>https://dobots.nl/2014/06/25/a-nonparametric-bayesian-lesson</guid>
                <pubdate>2014-06-25T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Sportopia, friends are your best coach!</title>
                <description>
&lt;h1 id="motivation"&gt;Motivation&lt;/h1&gt;

&lt;p&gt;What if our smartphone would be a means for our friends and family to reach us to keep us motivated in exercising well? Wouldn’t that help much better than setting our goals!? Everybody knows that taking up a sports, even only running, which does not need much of additional gear, is very difficult. You can motivate yourself by listening to nice music in the meantime, by reminding yourself of the nice feeling afterwards, or trying to measure an improvement in your physical condition. However, it is hard to listen to yourself at times. We are wired in such a way that we do not take ourselves too seriously, we do not admit it, but we know of ourselves that we make mistakes.&lt;/p&gt;

&lt;p&gt;Friends! If we tell others of our achievements or if we can compete with them, this already changes a lot! Now we can talk about it, we think of it naturally in a nice social setting, others can compliment us, and so much more. Ideally we would not only communicate to each other after we have exercised, but before! Students at the University of Applied Sciences (in Dutch, the &lt;a href="http://hva.nl"&gt;Hogeschool van Amsterdam&lt;/a&gt;, or HvA), have created a technological means to do so! A smartphone application that you can use to motivate others in the form of &lt;strong&gt;challenges&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id="sportopia"&gt;Sportopia&lt;/h2&gt;

&lt;p&gt;The application developed by Kevin Groen, Aster Schultz, and Dennis Reep is called Sportopia and is available on &lt;a href="https://play.google.com/store/apps/details?id=alm.motiv.AlmendeMotivator"&gt;Google Play&lt;/a&gt;. It hooks up to your Facebook contacts, so it nicely integrates with your social network. It then enables you to send so-called challenges to your friends. For example, you can ask your roommate to get beer from the supermarket. A perhaps questionable activity with respect to exercise overall, but an activity nevertheless! As proof you can ask for evidence in the form of pictures. In the meantime you can try to motivate him with text messages.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/friends.png" alt="Sportopia Friends" style="width: 300px" /&gt;
&lt;img src="/attachments/challenges.png" alt="Sportopia Challenges" style="width: 300px" /&gt;
&lt;img src="/attachments/challenge_closed.png" alt="Sportopia Challenge closed" style="width: 300px" /&gt;&lt;/p&gt;

&lt;h2 id="machine-learning"&gt;Machine learning&lt;/h2&gt;

&lt;p&gt;The reason we as DoBots are interested in this is because of the algorithms that are used for recommender systems are not so different from algorithms used for inference in general. In the &lt;a href="http://www.commit-nl.nl/projects/sensei-sensor-based-engagement-for-improved-health"&gt;Sensei project&lt;/a&gt;, our mother company &lt;a href="http://almende.com"&gt;Almende&lt;/a&gt;, is concerned with improving health using sensor-based engagement. The data kindly requested through the use of the Sportopia application can be used to fed into a recommender system to achieve exactly that. The creation of a virtual coach that is able to motivate people like we motivate our friends. There is very little data available on motivation except in-house in advertisement-driven companies (like Google). Concerning health, which is so much more important, than getting the “right” advertisement about some commercial product, we regretfully have no such datasets available. If you want to contribute to science, please download the app, and start to play!&lt;/p&gt;

&lt;p&gt;The machine learning algorithms we are designing are so-called nonparameteric Bayesian methods, in this case applied to the collaborative filtering problem. The motivation texts people sent each other are distinguished according to several classes. Each class comes with a certain factor of failure or success. By looking at a lot of texts we can cluster what types of motivation type works for what type of people. The algorithms we use for clustering can be found on &lt;a href="https://github.com/mrquincle/aim_modules"&gt;github&lt;/a&gt;. Due to the fact that clustering is such a general concept, it is quite straightforward to realize that these types of modules can also be used in totally different contexts, from computer vision to robotics. These types of algorithms all end up in the freely accessible platform &lt;a href="http://www.dodedodo.com/"&gt;Dodedodo&lt;/a&gt;. If you consider yourself a bit of a machine learning expert, you definitely should take a look also at &lt;a href="https://dobots.github.io/aim/"&gt;our documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="open-source"&gt;Open-source&lt;/h2&gt;

&lt;p&gt;The Sportopia application can be found on &lt;a href="https://github.com/almende/motivator"&gt;github&lt;/a&gt;. Copyrights belong to the students and us. If you want to use it in another context, please, contact us! We probably can work something out that makes everybody happy!&lt;/p&gt;

</description>
                <link>https://dobots.nl/2014/05/28/sportopia-friends-are-your-best-coach
                <guid>https://dobots.nl/2014/05/28/sportopia-friends-are-your-best-coach</guid>
                <pubdate>2014-05-28T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Best European Startup</title>
                <description>
&lt;h1 id="the-robot-launch-competition"&gt;The Robot Launch competition&lt;/h1&gt;

&lt;p&gt;&lt;img src="/attachments/best_european_startup.png" alt="Best European Startup" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;It’s an exciting time for us at DoBots. We are very proud to be part of the six finalists of the &lt;a href="http://robotlaunch.com/"&gt;Robot Launch competition&lt;/a&gt;, selected out of 60 startups in robotics in total! Movies from the finalists can be found at &lt;a href="http://robotlaunch.com/2014/05/robot-launch-2014-finalists-announced/"&gt;Robot Launch&lt;/a&gt;. We can’t do justice to the products and services of the finalists, but let us describe them briefly:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href="http://www.robosoftsystems.co.in/"&gt;RoboSoft Systems&lt;/a&gt;, a duct inspection robot&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://www.tandemech.com/"&gt;Tandemech Engineering&lt;/a&gt;, a wall-climbing robot&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://weareleka.com/en/"&gt;Leka&lt;/a&gt;, a robotic interactive sphere as a toy, made for children with autism&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://dobots.nl"&gt;DoBots&lt;/a&gt;, &lt;strong&gt;yes, us&lt;/strong&gt;, an autopilot for an industrial scrubber/dryer robot&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://www.oddiopdx.com/"&gt;Odd I/O&lt;/a&gt;, an interactive robot reacting on music and sounds&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://www.robotictechtn.com/"&gt;Robotics Technologies of Tennessee&lt;/a&gt;, mobile platforms for welding and inspection in the shipbuilding industry&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are happy we were in such good company, but of course, it makes the competition fierce!&lt;/p&gt;

&lt;h2 id="competition"&gt;Competition&lt;/h2&gt;

&lt;p&gt;A rich panel of judges assembled by Andra Keay from &lt;a href="http://robohub.org/author/andrakeay/"&gt;RoboHub&lt;/a&gt; formed a balanced view on the pros and cons of the different robotic technologies presented. There is Glenn Luinenburg, partner at &lt;a href="http://www.wilmerhale.com/"&gt;WilmerHale law firm&lt;/a&gt;, Rich Mahoney, director of robotics at &lt;a href="http://www.sri.com/"&gt;SRI International&lt;/a&gt;, Roger Chen from &lt;a href="http://oatv.com/"&gt;OATV&lt;/a&gt;, Cyril Ebersweiler from &lt;a href="http://haxlr8r.com/"&gt;Haxlr8r&lt;/a&gt;, Kate Drane, the hardware lead at &lt;a href="https://www.indiegogo.com/"&gt;Indiegogo&lt;/a&gt;, and Shahin Farshchi from &lt;a href="http://www.luxcapital.com/"&gt;Lux Capital&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can see the entire Google Hangout, including technical glitches ;-) on YouTube:&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/TPVug6lkRlA?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;p&gt;However, our presentation can not be seen in this Hangout, so to save one hour and a half of your life, this is a short presentation (of 4 minutes) with a voice over:&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/wmyU8BeGG4I?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;p&gt;If you are impatient, forward to 3:15. Here two clips start with moving robots (and Bart).&lt;/p&gt;

&lt;h2 id="award"&gt;Award&lt;/h2&gt;

&lt;p&gt;We are the proud receiver of the prize of the “Best European Startup”! This prize is really relevant to us! We won namely a meeting with &lt;a href="http://www.robolutioncapital.com"&gt;Robolution Capital&lt;/a&gt; in Paris! To describe Robolution Capital in their own words:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;"Robolution Capital is the first private equity fund dedicated to service robotics in the world. Robolution Capital's mission is to invest in innovative companies of the fast growing service robotics market, mainly within Europe. Robolution Capital is managed by Orkos Capital SAS, an experienced and well known private equity management company."
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;An investment party that is dedicated to service robotics in Europe, is a perfect partner for us!&lt;/p&gt;

&lt;p&gt;You all want to know of course who became the overall winner! The overall “Grand Champion” is Leka, the interactive robot for children with autism. Congratulations, Leka!&lt;/p&gt;

&lt;h2 id="future"&gt;Future&lt;/h2&gt;

&lt;p&gt;Future editions of this competitions might lead to questions to us. Feel free to ask us. We also suggested a Google+ community around it, which can be &lt;a href="https://plus.google.com/u/1/communities/113625439039934216202"&gt;found&lt;/a&gt;. Ask to be a member if you want to come in contact with other startups, like us, in robotics.&lt;/p&gt;

&lt;p&gt;Very important, in case you are considering investing in robotics, please, &lt;a href="about-us.html"&gt;contact us&lt;/a&gt;! We are down to earth, and make products and services that really are of added value to consumers or businesses. The industrial cleaning robot will be there in the near future. This is no science fiction anymore.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2014/05/23/best-european-startup
                <guid>https://dobots.nl/2014/05/23/best-european-startup</guid>
                <pubdate>2014-05-23T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Asus Xtion on UDOO with OpenNI2 and ROS</title>
                <description>
&lt;h1 id="asus-xtion-on-udoo-with-openni2-and-ros"&gt;Asus Xtion on UDOO with OpenNI2 and ROS&lt;/h1&gt;

&lt;p&gt;&lt;img src="/attachments/udoo,xtion,openni2.png" alt="Asus Xtion on UDOO" title="Asus Xtion on UDOO" /&gt;&lt;/p&gt;

&lt;p&gt;This step by step tutorial shows how to setup the &lt;a href="http://www.udoo.org/"&gt;UDOO&lt;/a&gt; in order to use an Asus Xtion with ROS. If ROS is not necessary, follow &lt;a href="http://feilipu.me/2013/11/09/udoo-ubuntu-12-04-guide/"&gt;this&lt;/a&gt; tutorial alternatively and go directly to step 2.&lt;/p&gt;

&lt;h2 id="step-1-install-ros-on-udoo"&gt;Step 1: Install ROS on UDOO&lt;/h2&gt;

&lt;p&gt;To use ROS on the UDOO set up an sdcard with an ubuntu core, following the steps described &lt;a href="http://wiki.ros.org/hydro/Installation/UDOO"&gt;here&lt;/a&gt; with these modifications:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of downloading the kernel, u-boot and modules seperately, download the Update Package found under the tab Driver &amp;amp; Tools on the udoo download page &lt;a href="http://www.udoo.org/downloads/"&gt;here&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Depending on the udoo cpu, use the following command to flash the boot sector: (replace by u-boot-q.imx for the quad)&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; sudo dd if=u-boot-d.imx of=/dev/sdb bs=512 seek=2 status=noxfer
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fix hostname problem. first add a correct hostname &lt;HOST_NAME&gt;&lt;/HOST_NAME&gt;&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; sudo nano /etc/hostname
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;then add that hostname to the file /etc/hosts&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; sudo nano /etc/hosts
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;by adding this line (Replace &lt;HOST_NAME&gt; with the hostname defined above)&lt;/HOST_NAME&gt;&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; 127.0.0.1 	localhost &amp;lt;HOST_NAME&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;then reboot.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To use the WiFi module, the firmware has to be installed seperately and can be found &lt;a href="https://packages.debian.org/sid/firmware-ralink"&gt;here&lt;/a&gt;&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; sudo dpkg -i firmware-ralink_0.41_all.deb
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id="step-2-install-openni2-driver"&gt;Step 2: Install OpenNI2 Driver&lt;/h2&gt;

&lt;h3 id="minimal-install"&gt;Minimal Install&lt;/h3&gt;

&lt;p&gt;To use OpenNI2 with ROS, we only need the shared OpenNI2 libraries and the Drivers. Java is not necessary and doesn’t need to be compiled. So to avoid installing and using up unneccessary space, I describe here first the minimal solution, and later the additions necessary to compile the java code and create an archive.&lt;/p&gt;

&lt;p&gt;Install the following dependencies:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install git g++ make python libusb-1.0-0-dev libudev-dev pkg-config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;clone OpenNI2&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/OpenNI/OpenNI2
cd OpenNI2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;edit ThirdParty/PSCommon/BuildSystem/Platform.Arm&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nano ThirdParty/PSCommon/BuildSystem/Platform.Arm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and replace&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CFLAGS += -march=armv7-a -mtune=cortex-a9 -mfpu=neon -mfloat-abi=softfp #-mcpu=cortex-a8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CFLAGS += -march=armv7-a -mtune=cortex-a9 -mfpu=neon -mfloat-abi=hard
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then run make to compile the OpenNI2 drivers and libraries&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PLATFORM=Arm make
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note: Because of the missing java compiler, the compilation will fail with OpenNI.java, and javac not found. This is ok since the drivers and shared libraries are already compiled at this point. The only thing missing are the samples.&lt;/p&gt;

&lt;p&gt;Once the compilation is done, run the linux install script&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd /Packaging/Linux
sudo ./install.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;copy libraries and includes to the system paths&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo cp -r Include /usr/include/openni2
sudo cp -r Bin/Arm-Release/OpenNI2 /usr/lib/
sudo cp Bin/Arm-Release/libOpenNI2.* /usr/lib/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;create a package config file&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo nano /usr/lib/pkgconfig/libopenni2.pc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and fill it with this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prefix=/usr
exec_prefix=${prefix}
libdir=${exec_prefix}/lib
includedir=${prefix}/include/openni2

Name: OpenNI2
Description: A general purpose driver for all OpenNI cameras.
Version: 2.2.0.0
Cflags: -I${includedir}
Libs: -L${libdir} -lOpenNI2 -L${libdir}/OpenNI2/Drivers -lDummyDevice -lOniFile -lPS1080.so
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;this will enable ubuntu to find the location of the drivers, libraries and include files.&lt;/p&gt;

&lt;p&gt;To make sure it is correctly found, run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pkg-config --modversion libopenni2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which should give the same version as defined in the file above (2.2.0.0)&lt;/p&gt;

&lt;p&gt;Now the Xtion is ready to be used. Plug it in (if it is already, unplug it first), then run the sample program&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./Bin/Arm-Release/SimpleRead
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note: For the minimal install, the samples need to be compiled first. Add the following line to the makefile at the end&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;core_samples: $(CORE_SAMPLES)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PLATFORM=Arm make core_samples
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to compile the samples.&lt;/p&gt;

&lt;p&gt;The SimpleRead returns the readings of middle point in the sensor data&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./Bin/Arm-Release/SimpleRead
	[00066738]      631
	[00100107]      641
	[00133477]      654
	[00166846]      662
	[00200215]      670
	[00233584]      677
	[00266954]      679
	[00300323]      678
	[00333692]      675
	[00367061]      669
	[00400431]      657
	[00433800]      644
	[00467169]      629
	[00500538]      611
	[00533908]      592
	...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id="full-install"&gt;Full Install&lt;/h3&gt;

&lt;p&gt;in addition to the dependencies of the minimal install, the default-jdk package is needed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install default-jdk
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;now running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PLATFORM=Arm make
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;will compile the drivers, shared libraries, jar files and samples.&lt;/p&gt;

&lt;p&gt;If a compressed archive with the drivers and libraries is needed, install the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install doxygen
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd Packaging
./ReleaseVersion.py Arm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which creates the archive OpenNI-Linux-Arm-2.2.tar.bz2 in Packaging/Final/&lt;/p&gt;

&lt;h2 id="step-3-install-ros-packages-to-run-the-asus-xtion"&gt;Step 3: Install ROS Packages to run the Asus Xtion&lt;/h2&gt;

&lt;p&gt;The packages used to run the Asus Xtion on the UDOO are not availbale over apt yet, so they need to be compiled from source&lt;/p&gt;

&lt;p&gt;First install the following dependencies, which will take some time and space (~900MB):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install ros-hydro-ros ros-hydro-rostopic ros-hydro-nodelet ros-hydro-camera-info-manager ros-hydro-roscpp ros-hydro-sensor-msgs ros-hydro-dynamic-reconfigure ros-hydro-image-transport ros-hydro-image-proc ros-hydro-depth-image-proc ros-hydro-tf 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then create a catkin workspace as described &lt;a href="http://wiki.ros.org/catkin/Tutorials/create_a_workspace"&gt;here&lt;/a&gt;, and check out the following packages in the src folder of the catkin workspace:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/ros-drivers/openni2_camera
git clone https://github.com/ros-drivers/openni2_launch
git clone https://github.com/ros-drivers/rgbd_launch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now the ros packages checked out above to the catkin workspace can be compiled with catkin_make&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ~/catkin_ws
catkin_make
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the packages are compiled, the Xtion is ready for use with ROS with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;roslaunch openni2_launch openni2.launch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Performance on the UDOO is good with 30 Hz at 640x480, compared to the Raspberry Pi that can only provide 30 Hz at 320x240 under no other load.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2014/05/05/asus-xtion-using-openni2-and-ros-on-udoo
                <guid>https://dobots.nl/2014/05/05/asus-xtion-using-openni2-and-ros-on-udoo</guid>
                <pubdate>2014-05-05T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>RFduino without RFduino code</title>
                <description>
&lt;h1 id="rfduino-without-rfduino-code"&gt;RFduino without RFduino code&lt;/h1&gt;

&lt;p&gt;The &lt;a href="http://www.rfduino.com/"&gt;RFduino&lt;/a&gt; was a successfull &lt;a href="https://www.kickstarter.com/projects/1608192864/rfduino-iphone-bluetooth-40-arduino-compatible-boa"&gt;Kickstarter project&lt;/a&gt;. Thanks to the fact that it basically is not much more than the nRF51822 chip by Nordic, its form factor is very small. Additional buttons, leds, sensors, etc. can be added by boards that precisely fit its connectors.&lt;/p&gt;

&lt;p&gt;The advantage of the RFduino is that it on top of the nRF51822 provides FCC and CE certification, has a thought out antenna design, and has so many boards that function as shields. It is created by the company &lt;a href="http://rfdigital.com/"&gt;RF Digital&lt;/a&gt; under the module name RFD51822 and later the RFD22102. Note that there are &lt;a href="http://www.nordicsemi.com/eng/Products/3rd-Party-Bluetooth-low-energy-Modules"&gt;other 3rd Party Bluetooth Low Energy vendors&lt;/a&gt; listed on the Nordic site, that have these advantages as well. Besides that, for example the MBDT40 from Raytac Corp has all the 31 GPIO available, while the RFduino only has 7 GPIO pins.&lt;/p&gt;

&lt;p&gt;Anyway, interestingly, the RFduino code is not open-source, or most of it is not open-source. This is awkward, given the fact that the guys from RF Digital, started the website &lt;www.opensourcerf.com&gt; to indicate their involvement. The RFduino first came with a simple means to upload a binary to the RFduino, namely using the same tool as with most of the other Arduino boards: __avrdude__. Regretfully, after learning that changes to the open-source code in avrdude must be made open-source as well, the developer behind RFduino took all his sites offline (github as well as the forums), and came back online again with a proprietary tool to do so. A very strange move. The bootloader that resides on the RFduino is made proprietary as well, it is not even available in binary form. Interestingly, I saw some wrapper code between the Arduino code and the SoftDevice libraries from Nordic. If anything, these header files should be kept private. On the Nordic forums the employees do not know of any reason why the flash tool or the bootloader has to be proprietary. It seems to run counter any business sense. The more ways the RFD22102 boards can be programmed, the more will be sold from them.&lt;/www.opensourcerf.com&gt;&lt;/p&gt;

&lt;h2 id="programming-it-ourselves"&gt;Programming it ourselves…&lt;/h2&gt;

&lt;p&gt;There is a lot of information on programming the nRF51822 on the Nordic website and the forums. Most of it however can only be obtained by buying a development or evaluation kit. The development kit comes with a so-called J-Link programmer from Segger, the J-Link LITE CortexM to be precise. Connecting it to the RFduino is not hard. In the following picture you can see how a little breadboard is enough. Here I just took a 9 pins FTSH Samtec connector we had lying around from a previous project (FireSwarm, a swarm of flying robots to find a dune fire). And there is no color coding whatsoever here!&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/rfduino.jpg" alt="RFduino" title="RFduino connected" /&gt;&lt;/p&gt;

&lt;p&gt;The J-Link comes with a connector with 9 pins, this means one pin is removed (pin 7) to give some asymmetry, very convenient! The pin layout is like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;VTref 1 * * 2  SWDIO / TMS
GND   3 * * 4  SWCLK / TCK
GND   5 * * 6  SWO / TDO
--    7 o * 8  TDI
NC    9 * * 10 nReset
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, power the RFduino from an external source on 3.3V. The 3V and GND pins are nicely indicated on the RFduino. And then there are only three pins you have to connect. The VTref measures if the RFduino has actually enough power and must be connected to the 3V pin. The two other pins to connect are the SWDIO and the SWCLK pins. The SWDIO is connected to the RESET pin on the RFduino, the SWDCLK to the FACTORY pin.&lt;/p&gt;

&lt;p&gt;Now, if you download the code at &lt;a href="https://github.com/mrquincle/bluenet"&gt;github&lt;/a&gt; you will get a project with a Makefile that calls scripts in the &lt;code&gt;scripts&lt;/code&gt; directory. Most of the code is thanks to Christopher Mason, only the adaptations to support RFduino are mine. To program with the J-Link, you will need the JLinkExe binary and for debugging the JLinkGDBServer binary. You can download them from &lt;a href="http://www.segger.com/jlink-software.html"&gt;segger.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The current code requires a lot more love, but the beginning is there. In this movie you can see how the LED on the RGB RFduino shield reacts on the signal strength with an Android smartphone.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/9rnCdWl4mto?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;p&gt;Note, that if you use this code and flash the RFduino, there are two things you will have to keep in mind. First, the SoftDevice, in this case the S110, is proprietary. It comes with the development kit (of 100 bucks) from Nordic. You will not be able to use Bluetooth without it. I would not recommend starting to program your RFduinos without buying it. Second, you won’t be able to get back to the standard RFduino software. This would require the RFduino people to make certain information public, especially where it expects the SoftDevice and how it interfaces with it. This is not the same as providing open-source software, but also this information is not available. So, consider this a one-way direction. :-)&lt;/p&gt;

&lt;p&gt;Now we are in full control of our RFduinos. We can create our own Bluetooth characteristics, services, etc. We know which timers are used. We can take full advantage of the system that ARM uses to have peripheral devices communicate with each other without using the CPU for example. They use besides the well known interrupts, entities like “events” and “tasks” to do this, pretty neat.&lt;/p&gt;

&lt;p&gt;For us, we’d like to experiment with the new SoftDevice, the S120. Contrary to the S110, this SoftDevice allows mixing central and observer roles. This means it becomes possible to develop wireless sensor networks type of functionality. What is also really interesting is its support for wireless charging. More can be read in &lt;a href="http://www.prnewswire.com/news-releases/nordic-semiconductor-announces-bluetooth-smart-solution-for-rezence-wireless-charging-and-the-s120-8-link-central-role-bluetooth-low-energy-softdevice-for-the-nrf51822-enabling-charge-pads-with-support-for-simultaneous-charging--236655491.html"&gt;Nordic’s press release&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So, why using the RFduino at all? Its advantages are still there: certification, many extension boards, and a nice antenna design. We would like to concentrate on very rapid prototyping of services, such as a &lt;strong&gt;Lost &amp;amp; Found service&lt;/strong&gt;, rather than spending too much time on the electronics itself.&lt;/p&gt;

&lt;h2 id="crownstone"&gt;Crownstone&lt;/h2&gt;

&lt;p&gt;Make sure you take a look at our &lt;a href="/products/crownstone"&gt;Crownstone&lt;/a&gt; offering. This is directly based on the nRF51822 and open-source for real. :-) So, this uses the code at github for &lt;a href="https://github.com/mrquincle/bluenet"&gt;BlueNet&lt;/a&gt; as indicated above. If you want to have more details on how to program the different SoftDevice versions from Nordic etc., feel free to file an issue there. Also, look around if you want to get more information on Bluetooth Low-Energy in general, as for example in this blog post about &lt;a href="/2014/07/23/linux-and-ble/"&gt;Linux and BLE&lt;/a&gt; or on the &lt;a href="/2014/07/15/ble-dobeacon-a-virtual-memo/"&gt;iBeacon-type of device&lt;/a&gt; we build (with respect to software!) for &lt;a href="http://wots.nl"&gt;WOTS&lt;/a&gt;. To be clear, the services on top of the Crownstone that require a larger part of machine learning and artificial intelligence will not be open-source. If you think we do not communicate that properly, feel also free to suggest improvements in wording!&lt;/p&gt;

</description>
                <link>https://dobots.nl/2014/03/05/rfduino-without-rfduino-code
                <guid>https://dobots.nl/2014/03/05/rfduino-without-rfduino-code</guid>
                <pubdate>2014-03-05T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Dotty update</title>
                <description>
&lt;h1 id="dotty-update"&gt;Dotty update&lt;/h1&gt;

&lt;p&gt;It’s been a while since we’ve shown the dotty and the progress we’ve made in the meantime. Dotty has particularly changed aesthetically, but we’ve also made some changes under the hood. Mostly to enable easy interaction with a smartphone.&lt;/p&gt;

&lt;h2 id="shaping-dotty"&gt;Shaping dotty&lt;/h2&gt;

&lt;p&gt;&lt;img src="/attachments/baseDotty1.jpg" alt="Dotty's base platform" title="Base platform" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;Right now the dotty platform is made up of two distinct parts. There’s the base platform and the body. The base platform serves as a standard hub to which all electronics and motors are connected. This part mainly exists to be be sturdy and support the body, which is why we added supporting ridges to it.&lt;/p&gt;

&lt;p&gt;The body is less standard and can be varied to suit particular needs we may have or develop. One example may be to attach a smartphone as extra brain for Dotty. In such a case, the body must be the structure to support and fasten the phone. One of the first bodies we made was a little pirate ship where the smartphone would act as a sail. In the future we intend to make multiple types of bodies available, one of which will be the ship, another in the works is a bumper car.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/pirateDotty.jpg" alt="Pirate Dotty" title="Pirate ship Dotty" class="float-right" /&gt;&lt;/p&gt;

&lt;h2 id="electronics"&gt;Electronics&lt;/h2&gt;

&lt;p&gt;Not much can be said about the electronics at this point. We’re researching options to have Dotty be self sustaining by means of a charging station. This way Dotty can go on without any external interventions required. This recharging is intended to be done wirelessly, so no metal prongs or plates required. Expect more on Dotty in the coming months.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2014/02/26/dotty-update
                <guid>https://dobots.nl/2014/02/26/dotty-update</guid>
                <pubdate>2014-02-26T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Painting Quadcopters</title>
                <description>
&lt;h1 id="painting-quadcopters"&gt;Painting Quadcopters&lt;/h1&gt;

&lt;p&gt;It started with a simple question half a year ago: Besides capturing video and images from above, what can quadcopters actually do autonomously? More or less practical use cases haven been in the media already: &lt;a href="http://edition.cnn.com/2013/10/18/tech/innovation/zookal-will-deliver-textbooks-using-drones/"&gt;delivering books&lt;/a&gt; or &lt;a href="http://matternet.us/"&gt;medicines&lt;/a&gt; and &lt;a href="http://www.youtube.com/watch?v=W18Z3UnnS_0"&gt;constructing buildings&lt;/a&gt; for example.&lt;/p&gt;

&lt;p&gt;However, the use case that is most appealing to &lt;em&gt;us&lt;/em&gt; has not received much attention so far: simple maintenance tasks at places that are difficult to reach. We believe such tasks are a prime candidate for robotization with quadcopters. Therefore, we set out to build a simple demonstrator model of a quadcopter that can do paintjobs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;This demonstrator is not finished yet!&lt;/strong&gt; Nevertheless, we (in particular Juan Pablo Forero Cortés) made some good progress over the last half year and solved some of the challenges for this particular application. Watch the video to see how the work has been coming along!&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/UOgi7UkqzCc?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;h3 id="technical-details"&gt;Technical Details&lt;/h3&gt;

&lt;p&gt;The Parrot AR Drone was first extended with brightly coloured markers and with an Arduino with a Bluetooth module. The smartphone’s camera is then pointed towards the quadcopter so that an app can find its location by searching for the markers. The app then connects to the Parrot over WiFi, so that it can give the standard navigation commands to move the drone to the right position. The next step is that the app connects to the Arduino over Bluetooth in order to give the painting commands. The Arduino relays the paint command to a tiny servo that pushes at a spraycan, and voila, a black blob!&lt;/p&gt;

&lt;p&gt;For our future versions, there are two major improvements to be made. First of all, the flying behaviour of the drone near a wall is not nearly as good as it should be. This can be improved by using &lt;a href="http://wiki.ros.org/tum_ardrone"&gt;existing ROS libraries&lt;/a&gt; and should increase the precision enough to create pretty wallpaintings with our demonstrator. Secondly, in a final application it should not be necessary to have camera’s pointed at the quadcopter: it should be able to localize itself, in particular with respect to the wall.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2014/02/25/painting-quadcopters
                <guid>https://dobots.nl/2014/02/25/painting-quadcopters</guid>
                <pubdate>2014-02-25T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Dodedodo Christmas lights</title>
                <description>
&lt;h1 id="dodedodo-christmas-lights"&gt;Dodedodo Christmas lights&lt;/h1&gt;

&lt;p&gt;Lately, we’ve been busy trying to develop a way for us to easily interact
between devices in a modular way. This resulted in Dodedodo.&lt;/p&gt;

&lt;p&gt;Dodedodo allows anyone to install modules on different devices and easily
connect them by using the Dodedodo website. Just create an account at &lt;a href="http://www.dodedodo.com"&gt;dodedodo.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As a test, we had our christmas tree display messages from an android phone in
morse code. Another phone stood ready to decipher the message. Check the video
below to see how this works.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/6qtAuwOP9bs?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;
</description>
                <link>https://dobots.nl/2013/12/24/dodedodo-christmas-lights
                <guid>https://dobots.nl/2013/12/24/dodedodo-christmas-lights</guid>
                <pubdate>2013-12-24T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Romo, the surveillance robot</title>
                <description>
&lt;h1 id="romo-the-surveillance-robot"&gt;Romo, the surveillance robot&lt;/h1&gt;

&lt;p&gt;As a graduation project, I investigated the possibility of a surveillance robot with visual sensors that could learn what its environment looks like and how its body works, with as little prior information as possible. For example, when rotating right, the camera pixels would shift to the left. These relations between action and perception are learned using a specific type of local linear model, and the actions are chosen based on the principle of maximising the learning progress. The latter is explained in a &lt;a href="http://www.dobots.nl/blog/-/blogs/self-learning-robots"&gt;previous blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In that blog, it was also explained that prediction can be used to learn something about the body and environment of the robot. At each timestep t, the robot uses the action ut and the sensor data st to predict the sensor data of the next timestep, &lt;script type="math/tex"&gt;s_{t+1}&lt;/script&gt;. How this can be done will be explained shortly in this blog.&lt;/p&gt;

&lt;h2 id="shuffled-linear-model"&gt;Shuffled Linear Model&lt;/h2&gt;

&lt;p&gt;The relation between st+1 and (ut, st) is in general not a linear relation. So we cannot write &lt;script type="math/tex"&gt;s_{t+1} = A s_t + B u_t&lt;/script&gt; for all (&lt;script type="math/tex"&gt;u_t&lt;/script&gt;, &lt;script type="math/tex"&gt;s_t&lt;/script&gt;). However, around a local point  (ut(i), st(i)), that is, for a specific action and a specific location in the environment, the relation is linear. For example, when rotating right, while looking at a painting, all camera pixels shift to the left by the same amount, which can be written down by using a translation matrix which has 1’’s above the main diagonal. But for other locations, for example when looking at objects of greater or smaller depth, and for other actions like standing still or moving backwards, a different linear relation exists.&lt;/p&gt;

&lt;p&gt;Since there is an infinite number of possibilities for the sensor data and the actions available, it is not possible to find the right linear relation for all of these possibilities. However, the Shuffled Linear Model  can be seen as a Monte Carlo simulation for this problem by using a finite number of these possibilities, randomly. A finite number of sensor data and actions is chosen beforehand, randomly, and these are assigned to the same number of linear models. Then the linear models are summed together smoothly by multiplying them with Gaussian functions. The resulting formula would be something like this:&lt;/p&gt;

&lt;script type="math/tex; mode=display"&gt;s_{t+1} = \sum_t e^{-(||s_t-v_i||^2+||u_t-w_i||^2)}(A_i s_t+B_i u_t)&lt;/script&gt;

&lt;p&gt;Here, the &lt;script type="math/tex"&gt;v_i&lt;/script&gt; and &lt;script type="math/tex"&gt;w_i&lt;/script&gt; are chosen randomly. Linear regression can then be used to determine the matrices &lt;script type="math/tex"&gt;A_i&lt;/script&gt; and &lt;script type="math/tex"&gt;B_i&lt;/script&gt;. Details can be found in the &lt;a href="http://arxiv.org/abs/1308.6498"&gt;Shuffled Linear Model paper&lt;/a&gt; and in my &lt;a href="http://repository.tudelft.nl/view/ir/uuid%3A3f12dd7f-8761-4738-a224-95de36f7b53d"&gt;master thesis&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="model-learned-by-romo"&gt;Model learned by Romo&lt;/h2&gt;

&lt;p&gt;So what does it look like when using these techniques? Can Romo understand its body and environment? To a certain extend, yes. The robot learned that rotating right corresponds to a linear model with larger values above the main diagonal:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/romo_model.png" alt="Romo model" title="Romo linear model" /&gt;&lt;/p&gt;

&lt;p&gt;Similar relations were found for rotating left, and at different speeds, and for standing still (which had mainly large values near the main diagonal, like the identity matrix).&lt;/p&gt;

&lt;h2 id="noticing-changes-to-the-environment"&gt;Noticing changes to the environment&lt;/h2&gt;

&lt;p&gt;So now we have a robot that can learn some simple things about itself and the environment. How can we use this as a surveillance robot? Well, imagine that someone wants to break into a room where the robot has finished learning the model. Assuming that the robot can predict every timestep what sensor data it should receive, if someone breaks into a room this should cause the robot to get different sensor data from what it predicted. This prediction error can be measured, and a high prediction error should indicate something different to the environment or to the robot itself (for example someone moving the robot).&lt;/p&gt;

&lt;p&gt;Some of this can be found in the following movie.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/qHFX0Mr_RoA?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;p&gt;In conclusion, the robot can detect some changes to the environment, like humans entering the scene or bright objects that were not there before. Other things like dark objects or changes near a part of the environment that is particularly noisy, are harder. So this robot is not a real surveillance robot yet, but it certainly has potential for the future.&lt;/p&gt;
</description>
                <link>https://dobots.nl/2013/11/24/romo-the-surveillance-robot
                <guid>https://dobots.nl/2013/11/24/romo-the-surveillance-robot</guid>
                <pubdate>2013-11-24T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Replicator Robots Final</title>
                <description>
&lt;h2 id="finale"&gt;Finale&lt;/h2&gt;

&lt;p&gt;The project Replicator reached its final deadline in September. A European
project this scale - together with Symbrion a staggering 17 partners - has its
own dynamics. What did we learn? What can be done better next time?&lt;/p&gt;

&lt;p&gt;The field of modular robots did not have a robot that could move on its own.
The &lt;a href="http://replicators.eu/"&gt;Replicators&lt;/a&gt; are the first of their kind in that
they are fully autonomous. A single robot has screw drives (the so-called
Backbone robot), three wheels (the ActiveWheel), or tracks (the Scouts). They
all serve different purposes. The 3D hinge of the Backbones is particularly
strong. They can lift three times their own weight and are hence fully
equipped to fulfill a role as spine in a robot organism. The ActiveWheel can
carry two robots of the other type in a very simple way, requiring little
control. The Scouts are most like a normal robot and use a tank drive. All of
these robots can however be mixed and matched together to form complicated
structures. In the following section you see the preparations for the final
demo of Replicator.&lt;/p&gt;

&lt;h2 id="movie"&gt;Movie&lt;/h2&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/fGCRjWBzEpo?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;h2 id="lessons-learnt"&gt;Lessons learnt&lt;/h2&gt;

&lt;p&gt;So, what should be done differently next time?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Although the biologists in the project already warned about this from the start, the robots become way too complicated. The proper word to describe the robots is “over-engineered”. The number of sensors the engineers put on the robot (infrared, cameras, microphones, current sensors, ambient light sensors, accelerometers) lead together with the many different motors (four docking units, two motors for locomotion, a motor for the 3D hinge) and peripheral functions such as ethernet, power management, zigbee, and indoor positioning chips, to a proliferation of around 20 microcontrollers per robot! The complex robots require a lot of software development on the MSPs, Cortex, and Blackfin processors. Software development that has to do with communication, drivers, and the operating system, each taking time away for higher level functions. Simpler robots, perhaps ball-shaped, with omnidirectional vision through a fisheye lens, one docking unit, only a few motors, would have been so much better.&lt;/li&gt;
  &lt;li&gt;The reuse of technology. Although the smartphone world hadn’t kicked in gear yet in March 2008, when the Replicator project kicked off, in later years the non-anticipated rise of smartphones has not been corrected in the project. In case the robots would have been equipped with smartphone technology each year a better “brain” could have been embedded in the robots. Although in the first years a smartphone had only a low-resolution camera, microphone, wifi and some simple sensors like an accelerometer and a compass, in current years, you will have one with two cameras, a gyroscope, NFC, USB on-the-go, recharging through induction, low-energy Bluetooth, 3G, GPS, etc. etc. The project might have an excuse with respect to timing… However, new European projects that do not leverage smartphone technology should simply not be funded.&lt;/li&gt;
  &lt;li&gt;The lack of interest for existing software and frameworks. Although it is understandable that to obtain a PhD or MSc it is easier to write yet another robot architecture from scratch, this impedes progress. There are already frameworks like ROS and YARP which would be excellent candidates for an implementation on a robotic swarm. This is not a trivial matter and would allow a much larger community to profit from the results in the project. It would be great if there is in the academic world larger emphasis on reuse of software. Again, if that would be the case, there is more time for the development of algorithms and higher-level conceptualisation.&lt;/li&gt;
  &lt;li&gt;Last point. The modularity of the modular robots has not been extended towards the internals of the robots. Each robot has been designed as a monolithic block. It is not possible to add a little sensor block with a camera to one of the modules, or achieve heterogeneity in another lego block like manner. Worse, there has not been any attempt to create something like a “robot heart”. Almost every European robotic project fails to use smartphone technology. Moreover, they invent the wheel themselves time afte rtime. The core of a robot, that is, the processor, a few sensors, and wireless communication, and power management for these items is a recurring aspect of every robot. Still, none of the European projects tries to create something like a “robot heart” that can be reused by other projects. Although the actuators and power management for the motors are highly intertwined with the mechanics and electronics, the “payload” as it is called in unmanned aerial vehicles, should undergo some standardization. Modularity and standardization in academic robotics are absent.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notwithstanding the critical remarks, the project has been fun for Almende and
inspired a lot of technology at DoBots. Let’s take these lessons at heart!&lt;/p&gt;

</description>
                <link>https://dobots.nl/2013/10/01/replicator-robots-final
                <guid>https://dobots.nl/2013/10/01/replicator-robots-final</guid>
                <pubdate>2013-10-01T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Self-learning robots</title>
                <description>
&lt;h1 id="self-learning-robots"&gt;Self-learning robots&lt;/h1&gt;

&lt;p&gt;Autonomous robots are supposed to perform their tasks without human guidance.
One way to make this happen is by implementing some reward or penalty inside
the robot; something similar to pleasure and pain in humans and animals. If
the robot performs its task correctly, it receives a reward, if not, it
receives a penalty, and the robot will choose its actions in such a way that
it maximises its reward. This is studied in the area of &lt;a href="http://en.wikipedia.org/wiki/Reinforcement_learning"&gt;reinforcement learning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Suppose an autonomous robot ‘wakes up’ in an unknown body and in an unknown
environment, and its task is to understand itself and the environment. This
is, of course, a huge task that can even take some of us humans a lifetime to
learn, so we will start with a simple version of this task: the robot has to
predict what it will perceive when it performs a simple action, like moving
one step forward. If it can succesfully do this in every possible situation
and for every possible action, we could say that the robot has some sort of
understanding of itself and the environment.&lt;/p&gt;

&lt;p&gt;Now the problem of the creators of the robot is to implement a reward that
will cause the robot to predict correctly in every possible situation. This
can be divided in two goals: predict correctly, and explore new situations.
This is similar to the exploration vs. exploitation trade-off in reinforcement
learning, but a big difference is that we cannot say that moving towards a
specific location in the environment for example, should give a high reward,
since the environment is unknown. The only available information is the action
&lt;script type="math/tex"&gt;a&lt;/script&gt; of the robot, and what the robot perceives: sensory data &lt;script type="math/tex"&gt;s&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Let’s say that every timestep &lt;script type="math/tex"&gt;t&lt;/script&gt; the robot can predict sensory data &lt;script type="math/tex"&gt;s_{t+1}&lt;/script&gt; by
using current action &lt;script type="math/tex"&gt;a_t&lt;/script&gt; and current sensory data &lt;script type="math/tex"&gt;s_t&lt;/script&gt; and a predictor
&lt;script type="math/tex"&gt;P(s_t,a_t)&lt;/script&gt;. The predictor can ‘learn’ by using any machine learning algorithm,
for example by training a neural network. In general the predictor will not be
perfect, so we define a prediction error &lt;script type="math/tex"&gt;e_t=|s_{t+1}-P(s_t,a_t)|&lt;/script&gt;. This error
can be used to let the robot achieve its two goals of predicting correctly and
explore new situations.&lt;/p&gt;

&lt;h2 id="minimise-prediction-error"&gt;Minimise prediction error&lt;/h2&gt;

&lt;p&gt;If the robot receives a reward for minimising the prediction error, the
expected behaviour is that the robot will choose those actions that are
easiest to predict, like standing still, to receive rewards easily. This makes
it achieve one goal, namely to predict correctly, but it will not cause the
robot to explore new situations.&lt;/p&gt;

&lt;h2 id="maximise-prediction-error"&gt;Maximise prediction error&lt;/h2&gt;

&lt;p&gt;If the robot receives a reward for high prediction errors, this will generally
lead to explorative behaviour. The robot will move towards situations where it
expects to have a high error, thus receiving rewards. This should work if the
predictor can learn all of these situations, but in general it takes some time
for a predictor to learn something. Moreover, there might be situations where
the predictor cannot really learn to predict, like the noise on a television
screen or if the robot moves too fast. Then the robot will move towards these
situations and stay there because it will keep getting a high reward for this.&lt;/p&gt;

&lt;h2 id="maximise-learning-progress"&gt;Maximise learning progress&lt;/h2&gt;

&lt;p&gt;A better solution is to give the robot a reward when it &lt;em&gt;decreases&lt;/em&gt; the
prediction error. A decrease in error is also called ‘learning progress’. When
it maximises this learning progress, the robot might start simple by just
standing still. At first, the prediction error will be high because it is the
first time ever that the robot stands still. But it is not difficult to
predict what the robot will perceive in this situation, so the prediction
error will decrease. At some point, the error cannot decrease anymore (for
example, because it might be zero), so then the robot has learned the
situation and will move on to reach a situation where the error can still
decrease. This solves both the ‘predicting correctly’ part and the ‘explore
new situations’ part. If the robot ever encounters a situation where it cannot
learn to predict correctly, then the error will not decrease and the robot
will also move to a different situation.&lt;/p&gt;

&lt;p&gt;Although there are still some drawbacks even to this last method, it is a good
method to let the robot predict correctly and also explore new situations. For
further reading, see:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Oudeyer, Pierre-Yves, and Frederic Kaplan. “What is intrinsic motivation? a typology of computational approaches.” &lt;em&gt;Frontiers in neurorobotics&lt;/em&gt; 1 (2007).&lt;/li&gt;
  &lt;li&gt;Oudeyer, Pierre-Yves, et al. “The playground experiment: Task-independent development of a curious robot.” &lt;em&gt;Proceedings of the AAAI Spring Symposium on Developmental Robotics&lt;/em&gt;. Stanford, California, 2005.&lt;/li&gt;
  &lt;li&gt;Schmidhuber, Jürgen. “Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts.” &lt;em&gt;Connection Science&lt;/em&gt; 18.2 (2006): 173-187.&lt;/li&gt;
&lt;/ol&gt;

</description>
                <link>https://dobots.nl/2013/07/26/self-learning-robots
                <guid>https://dobots.nl/2013/07/26/self-learning-robots</guid>
                <pubdate>2013-07-26T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>IRC, XMPP and WhatsApp</title>
                <description>
&lt;h1 id="irc-xmpp-and-whatsapp"&gt;IRC, XMPP and WhatsApp&lt;/h1&gt;

&lt;h2 id="irc"&gt;IRC&lt;/h2&gt;

&lt;p&gt;IRC, short for Internet Relay Chat, is a protocol, created in 1988, for
instant messaging. There are many IRC client programs and many networks to
connect to, and most of the chat happens in channels (chat rooms), while
private messages are also possible. The nice thing about IRC is that most
servers are non commercial (w.r.t. privacy concerns), and requires very little
traffic. Since channels have a name and topic, it’s easy to find a channel
that meets your interests, while you can also create your own channel for
friends. Channels have operators, who can manage a channel: kick or ban users,
set modes and inviting other people. Many more handy commands are available
for users, like setting yourself as being away or ignoring someone.&lt;/p&gt;

&lt;p&gt;There is quite some development done around IRC: networks improve their
&lt;a href="http://hg.quakenet.org"&gt;servers&lt;/a&gt;, client programs are improved, BNCs (a proxy
to an IRC server that remain connected to the server when you go offline) are
getting more and more features. Next to that, many people create bots that are
connected to IRC to make live easier: bots that can lookup information, create
statistics, or control (game) servers.&lt;/p&gt;

&lt;h2 id="whatsapp"&gt;WhatsApp&lt;/h2&gt;

&lt;p&gt;At some point, WhatsApp became popular, and I didn’t get why. It was just
another way to message people and it uses a customized protocol, with poor
&lt;a href="http://en.wikipedia.org/wiki/Whatsapp#Security"&gt;security&lt;/a&gt;. Furthermore: all
traffic goes via a single, commercial, company, which often makes it tempting
to sell users privacy (like Facebook and Google do).
This made me think: why not replace WhatsApp with a nice app that uses IRC?
The problem with IRC is that it isn’t very noob friendly: it is made by and
for people who aren’t scared of computers, with many options, commands and
modes. So the app would have to hide all this and show an easy interface for
the user. I’ve been talking to IRC developers about this and there are a
couple of problems: IRC server do not store messages your receive when you’re
offline (you’d need a BNC for that) and sending something to a certain nick
does not guarantee that this nick is actually the person you think it is.&lt;/p&gt;

&lt;h2 id="xmpp"&gt;XMPP&lt;/h2&gt;

&lt;p&gt;Recently I’ve been reading myself into XMPP (for&lt;a href="http://www.dodedodo.com"&gt;http://www.dodedodo.com&lt;/a&gt;) and figured that this would
is perfect for the app, and of course it turns out that WhatsApp simply uses a
customized XMPP protocol. XMPP basically works like email: everyone can setup
a server and users get an address (JID) in the form of:
user@server.com/device. When you send a message to another user, the message
goes via your server, to the other user’s server and ends up at the other
user. Because XMPP always uses a login it does not have the same problem that
IRC has. Furthermore, there is an XMPP standard
&lt;a href="http://xmpp.org/extensions/xep-0013.html"&gt;extension&lt;/a&gt; that enables message
storage when a user is offline, and retrieval by the user when he connects
again.&lt;/p&gt;

&lt;p&gt;So I was looking for XMPP &lt;a href="https://play.google.com/store/search?q=xmpp"&gt;apps&lt;/a&gt;
and it seems that most apps assume you already have an account, and don’t look
very nice. I think that if there was an app that uses standard XMPP, it would
be a very nice replacement of WhatsApp.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2013/07/19/irc-xmpp-and-whatsapp
                <guid>https://dobots.nl/2013/07/19/irc-xmpp-and-whatsapp</guid>
                <pubdate>2013-07-19T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Introduction to One-Class Support Vector Machine</title>
                <description>
&lt;h1 id="introduction-to-one-class-support-vector-machine"&gt;Introduction to One-Class Support Vector Machine&lt;/h1&gt;

&lt;p&gt;Traditionally, many classification problems try to solve the two or multi-
class situation. The goal of the machine learning application is to
distinguish test data between a number of classes, using training data. But
what if you only have data of one class and the goal is to test new data and
found out whether it is alike or not like the training data? A method for this
task, which gained much popularity the last two decades, is the One-Class
Support Vector Machine. This (quite lengthly) blog post will give an
introduction to this technique and will show the two main approaches.&lt;/p&gt;

&lt;p&gt;At &lt;a href="http://rvlasveld.github.io/blog/2013/07/12/introduction-to-one-class-support-vector-machines/"&gt;this blog&lt;/a&gt; the post continues.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2013/07/12/introduction-to-one-class-support-vector
                <guid>https://dobots.nl/2013/07/12/introduction-to-one-class-support-vector</guid>
                <pubdate>2013-07-12T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Two websites</title>
                <description>
&lt;h1 id="two-websites"&gt;Two websites&lt;/h1&gt;

&lt;p&gt;We launched two websites that show a bit what we are doing at DO:&lt;/p&gt;

&lt;h2 id="rest4phone"&gt;Rest4phone&lt;/h2&gt;

&lt;p&gt;If you followed our work, you saw that we have been working on a special
robot, the Dotty. This robot is a smartphone-based robot, but different from
the ones that are now on the market, it is able to recharge itself by making
use of the USB connection through the smartphone. The newest smartphones are
able to recharge themselves via QI or other means. We have first experimented
with different methods ourselves, for example with Witricity which you might
now from the &lt;a href="http://www.ted.com/talks/eric_giler_demos_wireless_electricity.html"&gt;famous Ted talk by Eric Giler&lt;/a&gt;. However, by using the inate
functionality of the wireless charging smartphone, we won’t be limited to one
provider of wireless charging functionality, which is ideal in our case.&lt;/p&gt;

&lt;p&gt;We would like to give the Dotty to the world, but before we can do so, we have
to experiment with all the different forms and options that are out there. The
course exchange your old smartphone for money. However, you won’t get much,
especially not if the screen is broken for example. So, why not give it to us!
Smartphones turned into robots is of course not the only idea imaginable.
That’s why we set ourselves to redesign our website if there are other ideas
for a second life for your smartphone. We have already two incoming ideas.
Mats Lundgren from SilverLine uses smartphones for the elderly. Blair Palmen
from HopePhones for Medic Mobile. If you don’t have a second phone to spare,
feel free to tell us on &lt;a href="http://rest4phone.com"&gt;http://rest4phone.com&lt;/a&gt; if you have another great
idea.&lt;/p&gt;

&lt;h2 id="dodedodo"&gt;Dodedodo&lt;/h2&gt;

&lt;p&gt;A funny name, with a lot of DO (from DoBots) in it. That’s our website
very well that we now have market places or app stores to get applications for
our smartphones. However, smartphones are not the only devices in our life.
Especially from our perspective of robotics, we want to have a seamless
experience across all your devices. You should be able to install applications
across your smartphone, your home automation device, your mediabox, your
laptop, even your online server park with drag-and-drop actions.&lt;/p&gt;

&lt;p&gt;The &lt;a href="http://dodedodo.com"&gt;http://dodedodo.com&lt;/a&gt; website is a hub that brings together a lot of
software packages build by many developers. On the moment you can add your own
repository to add your own modules to the hub. To build these modules a lot of
helper code has been created. It would take too far to go in detail here, but
the code itself is open-source and a description on how to use it can be found
at &lt;a href="http://mrquincle.github.io/aim-bzr/"&gt;http://mrquincle.github.io/aim-bzr/&lt;/a&gt;. These utilities allow a developer to program certain functionality once
and subsequently wrapper code will be generated to be able to use it in many
different middlewares or frameworks. The currently supported middlewares are
YARP, ROS, ZeroMQ, and NodeJS. Moreover, and important for us, robotici, the
code can generate binaries for multiple architectures, the blackfin processor,
the RaspBerry PI, etc. The modules are described by a general interface that
uses a “port” abstraction. On &lt;a href="http://dodedodo.com"&gt;http://dodedodo.com&lt;/a&gt; it will become possible to
drag-and-drop blocks and draw lines between them to implement a distributed
application across your smartphone, a laptop, and a robot for example to
implement a “remote-presence robot” yourself.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2013/05/27/two-websites
                <guid>https://dobots.nl/2013/05/27/two-websites</guid>
                <pubdate>2013-05-27T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Robot Remote Control with ØMQ</title>
                <description>
&lt;h1 id="robot-remote-control-with-ømq"&gt;Robot Remote Control with ØMQ&lt;/h1&gt;

&lt;p&gt;In earlier blog posts we presented the Swarm Control App where we added
different robots to an Android app in order to control them remotely and
display their sensor data on the phone. However, this was only the first step;
what we want is to connect the robots to the Cloud where we process the sensor
data and send back remote commands to the robot.&lt;/p&gt;

&lt;p&gt;As a little showcase we took a Romo 1.0, equipped it with an Android
smartphone and wrote an app which publishes the video from it’s camera and
listens to remote commands. The camera video is sent to a server which
forwards the video stream to subscribed client(s). The clients in turn can
send remote commands back to the server which relays it to the Romo. The
message passing is done with &lt;a href="http://www.zeromq.org/"&gt;ØMQ&lt;/a&gt;, a high
performance, asynchronous message system for many different languages which
provides message queues, N-to-N connections, different messaging patterns, and
several messaging transports. On the server we are running node.js to forward
the ØMQ messages and provide the website for the web client.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/wCX6XXzhrCM?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;h2 id="romo-brain"&gt;Romo Brain&lt;/h2&gt;

&lt;p&gt;The source code of the app running on Romo can be found
&lt;a href="https://github.com/eggerdo/RomoBrain"&gt;here&lt;/a&gt; and uses the
&lt;a href="https://github.com/Romotive/Romo-SDK-Gen2"&gt;RomoSDK&lt;/a&gt; provided by Romotive. It
compresses the camera frames as 640 x 480 JPEG images and sends them encoded
in ØMQ messages to the server. It also subscribes to ØMQ messages containing
remote control commands which are decoded to control the Romo.&lt;/p&gt;

&lt;h2 id="robot-server"&gt;Robot Server&lt;/h2&gt;

&lt;p&gt;On the server side we are running node.js. The node provides two channels for
video and two channels for command messages. The incoming channels uses the
Push-Pull, the outgoing channel the Pub-Sub pattern. Messages received on the
incoming socket are forwarded to all subscribers on the outgoing channel. In
addition, the node provides another outgoing channel on which every incoming
video message is published encoded as Base64. This stream was added to support
web clients and will be described in more detail later on. The source code can
be found &lt;a href="https://github.com/eggerdo/robot_server.node"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The Romo establishes two connections with the Robot Server. It connects to the
incoming video channel of the server to push video messages and subscribes to
the outgoing command channel to receive command messages. The messages are
designed such that the Subscriber can filter messages so that it only receives
messages it is interested in. The server on the other hand will process every
message.&lt;/p&gt;

&lt;h2 id="remote-control-client"&gt;Remote Control Client&lt;/h2&gt;

&lt;p&gt;In order to control the robot and to show the flexibility of ØMQ we
implemented two different clients: an Android app and a web client. However,
there is no limit to the possible clients, since ØMQ comes in over 30
different programming languages.&lt;/p&gt;

&lt;h3 id="android-client"&gt;Android Client&lt;/h3&gt;

&lt;p&gt;The Android app to control the Romo over ØMQ can be found
&lt;a href="https://github.com/eggerdo/RoboTalk-User"&gt;here&lt;/a&gt;. It provides the possibility
to drive the Romo around and displayes the video stream of the Romo’s camera
on the screen. It subscribes to the outgoing video channel of the Robot
Server, decodes the video message and displays it on the screen. In turn,
remote commands are encoded as a JSON string in a ØMQ message and pushed to
the server.&lt;/p&gt;

&lt;h3 id="web-client"&gt;Web Client&lt;/h3&gt;

&lt;p&gt;The Web Client used to control the Romo from a Web Browser is composed of a
node.js node which serves the HTML web page from a server and a bridge between
WebSockets and ØMQ. The web site uses JavaScript to encode command messages
and decode video messages. Because a website cannot make TCP connections,
WebSockets are used to send the ØMQ messages to the server serving the
website. There a python script provides a bridge between WebSockets and ØMQ to
connect to the Robot Server. The code for the web server and ZmqWebBridge can
be found &lt;a href="https://github.com/eggerdo/robot_ctrl.node"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="performance"&gt;Performance&lt;/h2&gt;

&lt;p&gt;Performance of our setup looks quite well. To control the Romo we used an LG
Optimus 2x which delivered a video stream of about 20 fps. The RTT is about
100ms for a video message, and 60 ms for a command message. (This was measured
with an internet connection of 40 Mpbs down and 36 Mbps uplink and includes
encoding and decoding of the data in a ØMQ message as well as transmission to
and back from the server). As a result we get a frame rate of 20 fps on the
receiving side as well. However, using a slower internet connection will
significantly reduce the frame rate on the receiving side and reducing the
image size might be a good idea.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2013/04/26/robot-remote-control-with-%C3%B8mq
                <guid>https://dobots.nl/2013/04/26/robot-remote-control-with-ømq</guid>
                <pubdate>2013-04-26T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Extreme Learning Machines</title>
                <description>
&lt;h1 id="extreme-learning-machines"&gt;Extreme Learning Machines&lt;/h1&gt;

&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Neural_network"&gt;Artificial neural networks&lt;/a&gt; are
a common technique in the field of machine learning. Inspired by biology, they
are used in, among others, function approximation, time series prediction,
classification and pattern recognition. There exist many variations in the
type of network, the types of neurons used, and in the learning algorithms.
The most common learning algorithm for a feedforward neural network is
&lt;a href="https://en.wikipedia.org/wiki/Backpropagation"&gt;backpropagation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Although the backpropagation algorithm is a very popular learning algorithm,
there are some drawbacks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A small learning rate can lead to slow convergence.&lt;/li&gt;
  &lt;li&gt;A large learning rate can lead to divergence.&lt;/li&gt;
  &lt;li&gt;The algorithm might get stuck at local minima.&lt;/li&gt;
  &lt;li&gt;It is possible to overtrain the network, reducing the generalisation performance.&lt;/li&gt;
  &lt;li&gt;The algorithm can be very time-consuming, especially for large networks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In 2004, a new alternative to backpropagation for learning feedforward neural
networks has been proposed by Huang: Extreme Learning Machine (ELM). This
algorithm is easy to implement and does not suffer from the drawbacks above. I
will first show a summary of the theory behind this algorithm, and then
provide a supervised learning example implemented in MATLAB.&lt;/p&gt;

&lt;p&gt;Note that this technique is very similar to Reservoir Computing techniques for
recurrent neural networks. See &lt;a href="/2012/07/06/echo-state-networks/"&gt;Remco’s blog about Echo State Networks&lt;/a&gt; for a description.&lt;/p&gt;

&lt;h2 id="theory"&gt;Theory&lt;/h2&gt;

&lt;p&gt;Suppose we want to train a feedforward neural network with one hidden layer in
a supervised learning setting by providing input &lt;script type="math/tex"&gt;x&lt;/script&gt; and desired output &lt;script type="math/tex"&gt;y&lt;/script&gt;.
If the hidden layer contains synaptic weights &lt;script type="math/tex"&gt;SH&lt;/script&gt; and bias &lt;script type="math/tex"&gt;BH&lt;/script&gt; and the
hyperbolic tangent as a sigmoid activation function, the output &lt;script type="math/tex"&gt;H&lt;/script&gt; of the
hidden layer can be computed as &lt;script type="math/tex"&gt;H = tanh(-BH + SH*x)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;For the output layer, we use a linear activation function (though we could
also use a sigmoid function here) and weights &lt;script type="math/tex"&gt;S&lt;/script&gt; and no bias. Then the output
&lt;script type="math/tex"&gt;O&lt;/script&gt; of the whole neural network can be computed as &lt;script type="math/tex"&gt;O = S*H&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Now we want the neural network to produce an output &lt;script type="math/tex"&gt;O&lt;/script&gt; that minimizes the
error &lt;script type="math/tex"&gt;||y-O||&lt;/script&gt;, given input data &lt;script type="math/tex"&gt;x&lt;/script&gt;. In the backpropagation algorithm this
is done by performing a gradient descent on the error to update the hidden
weights &lt;script type="math/tex"&gt;S*H&lt;/script&gt; and the output weights &lt;script type="math/tex"&gt;S&lt;/script&gt;. In the ELM approach, however, the
hidden weights are initialised randomly and remain fixed; only the output
weights are adapted.&lt;/p&gt;

&lt;p&gt;Suppose the weights and biases of the hidden layer are fixed, then we can
compute the outputs of the hidden layer for all the training samples at once.
This gives a matrix &lt;script type="math/tex"&gt;\bf{H}&lt;/script&gt; of dimension &lt;script type="math/tex"&gt;h*T&lt;/script&gt;, where &lt;script type="math/tex"&gt;h&lt;/script&gt; is the amount of
hidden neurons and &lt;script type="math/tex"&gt;T&lt;/script&gt; the amount of training samples. Since the desired
output &lt;script type="math/tex"&gt;y&lt;/script&gt; is also known for all the training samples, for the network to
produce the desired output we need to solve the linear matrix equation &lt;script type="math/tex"&gt;S*\bf{H} = y&lt;/script&gt;. This equation does not necessarily have an exact solution, but the best (smallest norm least-squares) solution of this equation is &lt;script type="math/tex"&gt;S = y*\bf{H^+}&lt;/script&gt;, where &lt;script type="math/tex"&gt;\bf{H^+}&lt;/script&gt; is the &lt;a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse"&gt;Moore-Penrose pseudo-inverse&lt;/a&gt; of &lt;script type="math/tex"&gt;\bf{H}&lt;/script&gt;. Even with fixed weights going from input to hidden layer, using this &lt;script type="math/tex"&gt;S&lt;/script&gt; for the output weights gives good results in theory and in practice.
For more information, see &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.217.3692&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. “Extreme learning machine: theory and applications.” Neurocomputing 70.1 (2006): 489-501&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="practice"&gt;Practice&lt;/h2&gt;

&lt;p&gt;The above approach is easy to implement in MATLAB, or in any environment that
can handle a pseudo-inverse (or you can implement a pseudo-inverse yourself).
Here, an example is shown to let a feedforward neural network with one hidden
layer learn the XOR function.&lt;/p&gt;

&lt;p&gt;First, set the amount of training and generalisation samples and the input and
output dimensions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;T = 100; %amount of
training samples gen = round(0.2*T); %amount of generalisation samples id = 2;
%input dimension od = 1; %output dimension 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the input, random values from the set &lt;script type="math/tex"&gt;\{0,1\}^2&lt;/script&gt; are picked. And since the
function we want to learn is actually a known function, we can easily compute
the desired output values:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x = round(rand(id,T+gen));
%input data y = zeros(od,T+gen); %output data for t=1:T+gen y(:,t) =
xor(x(1,t),x(2,t)); %function to be learned end 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Of course, in a more realistic application &lt;script type="math/tex"&gt;y&lt;/script&gt; might not be given as a
function of &lt;script type="math/tex"&gt;x&lt;/script&gt; so easily, since that is exactly what the neural network needs
to learn. But in this example the function to be learned is known explicitly.
We also know the different possible values of the input data, so only 4 hidden
nodes will suffice for the network. In practice, one might need to find the
amount of hidden nodes just by trial and error or by using an incremental
algorithm.&lt;/p&gt;

&lt;p&gt;Initialise the neural network:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;h = 4; %amount of hidden nodes
SH = rand(h,id); %input-to-hidden synaptic weights, fixed
BH = rand(h,1)*ones(1,T+gen); %hidden layer bias, fixed
S = zeros(od,h); %hidden-to-output synaptic weights, to be adapted 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The hidden layer bias and weights will be fixed during the whole algorithm,
while the output weights will be adapted. It is important that the randomly
initialised weights and biases are drawn from a continuous probability
distribution, but it does not matter which one.&lt;/p&gt;

&lt;p&gt;The outputs of the hidden layer can be computed for every training sample:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;H = tanh(-BH + SH*x); %Calculate hidden layer output matrix
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When using your own inputs and outputs, sometimes the outputs of the hidden
layer will all be 1 or -1, or close to it. This could give problems in the
learning phase. You can try to normalise the input or the weights and biases
in this case.&lt;/p&gt;

&lt;p&gt;The learning phase is now only one line of code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S = y(:,1:T)*pinv(H(:,1:T)); %adjust hidden-to-output synaptic weights during learning phase
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Only the output weights are adapted, the hidden weights remain fixed. The
neural network has now learned the XOR function in one step, by only adapting
the output weights. The next code can be used to visualise this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;O = S*H; %output
plot(y,'b*'); %desired output hold on;
plot(1:T,O(:,1:T),'r.'); %output during learning phase
hold on; plot(T+1:T+gen,O(:,T+1:T+gen),'g.'); %output during generalisation phase
hold off;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src="/attachments/extreme_learning_machines.png" alt="Extreme learning machines" title="Extreme learning machines" /&gt;&lt;/p&gt;
</description>
                <link>https://dobots.nl/2013/04/19/extreme-learning-machines
                <guid>https://dobots.nl/2013/04/19/extreme-learning-machines</guid>
                <pubdate>2013-04-19T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Visualizing accelerometer data</title>
                <description>
&lt;h1 id="visualizing-accelerometer-data"&gt;Visualizing accelerometer data&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;Every major application or breakthrough starts with just a simple idea. But
an idea that has developed in the mind of one needs to tested to the real
world, and preferably in an easy and fast manner. During a research project on
recognizing human activities using accelerometer data, the need for quick
visualizations of collected data emerged. Using a simple setup this is made
possible, for everybody with an Android smartphone.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We are working on a system that can be seen as a part of a pipeline process of
recognizing human activities using smartphone data. The hypotheses is that by
segmenting the gathered sensory data before applying classification, as is
often done directly, a richer and better understanding of the data can be
obtained. To try out this idea, one can work with artificial data or, as we
liked to do, with real world data. In our first attempt we used free and open
available datasets with labeled activity data; the dataset of &lt;a href="http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones"&gt;Human Activity Recognition Using Smarthphones from the UCI Machine Learning Repository&lt;/a&gt;
and the &lt;a href="http://www.cis.fordham.edu/wisdm/dataset.php"&gt;WISDM dataset&lt;/a&gt;.
Although it is a luxury to have labeled data, any uncertainty about performed
activities and conditions is still a drawback.&lt;/p&gt;

&lt;p&gt;To overcome this problem of unknown factors, we want to gather our own data.
Because we will do small runs of a few consecutive performed activities, such
as sitting, walking and taking the stairs, manual annotation would suffice and
there is no need for precise labeling of each measurement. The requirements
are to quickly perform an activity (without an extensive environment setup)
and visualise the data for human inspection. This way we are able to get a
&lt;em&gt;feel&lt;/em&gt; of the data, before formalizing the idea.&lt;/p&gt;

&lt;h2 id="android-apps"&gt;Android Apps&lt;/h2&gt;

&lt;p&gt;Although the implementation of an Android app to gather the accelerometer data
is not that complicated, it is easier to choose an existing solution. This
implementation simplicity is reflected in the number of apps available
performing the task of recording sensory data. After a quick review of
available apps in the Google &lt;a href="http://play.google.com/"&gt;Play store&lt;/a&gt;, our weapon
of choice became &lt;a href="https://play.google.com/store/apps/details?id=com.kzs6502.sensorlogger"&gt;Sensor Logger&lt;/a&gt;. Its benefits are the adjustable sensor rate, the
number of (virtual) sensors (raw accelerometer, linear acceleration,
orientation, rotation, etc) and the easy csv format in which data is logged. A
drawback is the lack of gps recording, but due the nature of your recordings
(small in- or outdoor activities), this is not a big problem for now.&lt;/p&gt;

&lt;h2 id="data-gathering"&gt;Data gathering&lt;/h2&gt;

&lt;p&gt;The Sensor Logger app logs the sensor data to eight separate csv files on the
SD-card of the smartphone. These files can be transmitted to a PC, e.g. by USB
or bluetooth. Because our wish is fast data- gathering, &lt;a href="https://pl
ay.google.com/store/apps/details?id=kr.pe.meinside.DropSpace"&gt;DropSpace&lt;/a&gt; is used to
mirror the SD-card directory to our computer, using the
&lt;a href="http://www.dropbox.com/"&gt;Dropbox&lt;/a&gt; system.&lt;/p&gt;

&lt;h2 id="visualising-the-data"&gt;Visualising the data&lt;/h2&gt;

&lt;p&gt;&lt;img src="/attachments/visualizing_accelerometer_data.png" alt="Accelerometer data" title="Accelerometer data" style="width: 800px" /&gt;
When the data is gathered on the computer, it needs to be transformed and plotted. For
this we use a combination of Ruby and Octave scripts. The first step of this
process is to strip the csv files from comments and prepare them for an easy
Octave &lt;a href="http://www.gnu.org/software/octave/doc/interpreter/Simple-File-I_002fO.html#doc_002dload"&gt;load&lt;/a&gt; call. This is done by a Ruby script, which scans a
directory for Sensor Logger created files.&lt;/p&gt;

&lt;p&gt;The next step is to plot the data and save the graphs is png format. For this
the directory with the transformed csv files is passed on to a Octave script.
This script will create a graph for each measurement and one file with a few
graphs accumulated.&lt;/p&gt;

&lt;p&gt;These steps can be executed by using the command line script:
    $ ./transform_logs_to_directory.rb logs/set- stand-walk-stand-sit
An installed Ruby and Octave environment is required.&lt;/p&gt;

&lt;h2 id="results"&gt;Results&lt;/h2&gt;

&lt;p&gt;An example of a gathered plot can be seen
&lt;a href="https://github.com/rvlasveld/accelerometer_plotting/blob/master/logs/stand-sit-walk-stand-sit/20130404_111852/_accumulated_annotated.png"&gt;here&lt;/a&gt;. Without
any knowledge about the performed activities, a (slightly trained) human eye
can recognize different activities, or at least transitions between them. The
annotations are added manually to aid the interpretation of the data for the
reader. The noisy measurements and the beginning and ending are the result of
starting and stopping the logging while the phone is in the hand. The goal of
this research project to automatically create the segments, i.e. the cut-
points between activities.&lt;/p&gt;

&lt;p&gt;With this setup of quick data gathering we are able to get a feeling about the
data and test them in a visual manner, without the need of an (superfluous)
extensive laboratory setup. This way of implementation enables fast feedback
from a theoretical idea and wish for data to practical measurements which are
easily interpretable for humans.&lt;/p&gt;

&lt;h2 id="source-code"&gt;Source code&lt;/h2&gt;

&lt;p&gt;All the used source code is available on a Github project &lt;a href="https://github.com/rvlasveld/accelerometer_plotting"&gt;Accelerometer Plotting&lt;/a&gt;. Please feel
free to use, reuse and modify the code to your needs. Included is one example
run of activities. The graphs in &lt;a href="https://github.com/rvlasveld/accelerometer_plotting/blob/master/logs/stand-sit-walk-stand-sit/20130404_111852/"&gt;logs/stand-sit- walk-stand-sit&lt;/a&gt; are the result of executing a
single run. The activities performed are, in executed order, standing,
sitting, walking with a 180 degree turn, standing and sitting. The file
_accumulated_annotated.png has manually annotations to provide the reading
some insight in the data.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2013/04/12/visualizing-accelerometer-data
                <guid>https://dobots.nl/2013/04/12/visualizing-accelerometer-data</guid>
                <pubdate>2013-04-12T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Roborouter</title>
                <description>
&lt;h1 id="robo-router"&gt;Robo-Router&lt;/h1&gt;

&lt;p&gt;In the “world according to DO”, people and organizations will soon be helped
with their daily chores by swarms of intelligent robots. Vacuum cleaning and
mopping robots clean the floors of hospitals, UAVs check for forest fires,
mobile security cameras keep a look-out for unwanted intruders at night.&lt;/p&gt;

&lt;h2 id="working-together"&gt;Working together&lt;/h2&gt;

&lt;p&gt;One of the prerequisites for this scenario is that the robots must be able to
collaborate; with each other, with other devices (doors, computers,
refrigerators) and with their human operators. We don’t want the vacuum
cleaner to be in our way when we walk through a room. We don’t want 10 robots
cleaning the same spot over and over again. And we don’t want to have to open
a door, every time the robot wants to clean a room. So the robots need to be
able to talk with each other.&lt;/p&gt;

&lt;p&gt;Unfortunately, the actors involved in these scenarios use a variety of
different languages, protocols and media. That is why we need a Robo-Router!
The Robo-Router would act as an interpreter between humans, robots, and other
devices, creating a robot internet.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/roborouter.png" alt="Robo-Router prototype" title="Robo-Router prototype" /&gt;&lt;/p&gt;

&lt;h2 id="the-roomba-vs-the-door"&gt;The Roomba vs the Door&lt;/h2&gt;

&lt;p&gt;Over the past six months, Enrico, one of our interns from the &lt;a href="http://www.hogeschoolrotterdam.nl/opleidingen/technische-informatica/voltijd"&gt;Hogeschool Rotterdam&lt;/a&gt;, worked on one aspect of this problem. His mission:
allowing a Roomba to open a door, by triggering the electronic door opener.
Since we prefer generically applicable solutions to specific ones, the
communication between these two devices would have to go through a Robo-
Router, which he helped develop.&lt;/p&gt;

&lt;p&gt;Our electronic door opener did not yet have any remote communication
capabilities, so we  fixed it up with an ATtiny45 with a 433 MHz receiver. Our
Roombas are already equipped with a RooTooth from Sparkfun, which gives them
Bluetooth capabilities.&lt;/p&gt;

&lt;h2 id="robo-router-1"&gt;Robo-Router&lt;/h2&gt;

&lt;p&gt;Next step was developing the Robo-Router. For the microcontroller, we chose an
Arduino MEGA, since the Robo-Router only needs a minimal amount of memory. And
because many Dutch domotica devices use the KaKu (Klik-aan-Klik-uit) protocol,
supporting that was one of our first priorities.&lt;/p&gt;

&lt;h2 id="graduation-and-further-development"&gt;Graduation and further development&lt;/h2&gt;

&lt;p&gt;Enrico finished his internship this month, leaving us with a great proof-of-
concept. Our Roomba can now open the door to our meeting room on its own, by
sending a command through the Robo-Router.&lt;/p&gt;

&lt;p&gt;This does not mean the end of the project, though. We already found a new
intern, Michiel, to help us carry on the development. The next step in our
mission to connect all robots and devices to the cloud, is to expand our Robo-
Router to support other communication protocols, such as 315 MHz and USB.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2013/03/14/robo-router
                <guid>https://dobots.nl/2013/03/14/robo-router</guid>
                <pubdate>2013-03-14T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Maker Inspiration</title>
                <description>
&lt;h1 id="maker-inspiration"&gt;Maker Inspiration&lt;/h1&gt;

&lt;p&gt;When browsing through the various blogposts, I noticed that the ideas of DO
have many parallels with the &lt;a href="http://en.wikipedia.org/wiki/Maker_culture"&gt;maker culture&lt;/a&gt;. The maker movement has
quickly been gaining popularity over the past years, and in that respect DoBots is
a typical example of modern engineering culture: we like rapid prototyping, we
build upon existing hardware, use software where appropriate (contrary to all-
hardware solutions), and embrace the open-source principles. Because of these
parallels, I decided to write a short blogpost about two maker-inpired ideas
that are on my (infinitely long) to-do list: coffee cups with RFID tags and
bringing a Nabaztag (version 1) back to live.&lt;/p&gt;

&lt;h3 id="nabaztag"&gt;Nabaztag&lt;/h3&gt;

&lt;p&gt;&lt;img src="/attachments/nabaztag.jpg" alt="Nabaztag" title="Nabaztag" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;We keep two comatose &lt;a href="http://en.wikipedia.org/wiki/Nabaztag"&gt;Nabaztag&lt;/a&gt; rabbits
in our office from an old project. They still look cute, but their stillness
is sometimes a tad saddening. Therefore, I would like to bring them back to
live and give them a sense of purpose again. Now, this is not a very easy
task, as the number of I/O options on the Nabaztag is rather limited: it has
moving ears, some LEDs, a speaker, and finally, one button. So far, I came up
with the following ideas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Track my computer or internet use in some way and let the Nabaztag give me feedback: encourage me to work harder, or take a break, in a non-obtrusive way&lt;/li&gt;
  &lt;li&gt;Let me know when there is something important to read or when a deadline in my calendar is due&lt;/li&gt;
  &lt;li&gt;Suggest fussball matches or react on the scores&lt;/li&gt;
  &lt;li&gt;Let me know when it might or might not be a good idea to cycle home (rain)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additional ideas of uses for office-rabbits are of course welcome!&lt;/p&gt;

&lt;h3 id="coffee-cups"&gt;Coffee Cups&lt;/h3&gt;

&lt;p&gt;Working in an office where a significant part of the daily physical exercise
consists of maintaining the flow of coffee from the ground floor to the second
floor, we came up with the idea of embedding RFID tags in coffee cups. This
idea is of course not new, as it is for example used to &lt;a href="http://www.validfill.com"&gt;keep track of
consumptions&lt;/a&gt; in coffee shops and restaurants.
However, the office is a much more interesting application space! The main
idea is this: put your cup with pretty (maybe even customized) RFID tag in the
coffee machine, and you get your standard brew without touching any buttons.
Just think of the possibilities and advantages!&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/mug.jpg" alt="Mug" title="robot mug" class="float-right" /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Getting coffee for colleagues becomes a breeze&lt;/li&gt;
  &lt;li&gt;No germ spreading through coffee machine buttons&lt;/li&gt;
  &lt;li&gt;Pretty, identifying RFID tags on every cup (dishwasher-proof of course)&lt;/li&gt;
  &lt;li&gt;Maybe even &lt;a href="http://soft.vub.ac.be/soft/edu/thesis/proposals1213/thingdb"&gt;find the cup when it is lost&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Automatic statistics gathered by the &lt;a href="http://andrewbrobinson.com/2011/12/27
/hacking-the-keurig-b40-coffee-maker-part-1-hardware/"&gt;Arduino &amp;amp; RFID reader in the coffee machine&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Connect it to a webserver and have a live coffee consumption leaderboard&lt;/li&gt;
  &lt;li&gt;Automatic machine status messages and requests for refilling, etc. The coffee machine becomes a truly autonomous entity!&lt;/li&gt;
  &lt;li&gt;Overrule standard settings for a particular cup through the internet or coffee machine interface. Use machine learning to predict and learn these deviations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Of course, one could argue that the same could be achieved with old-fashioned
techniques like color-coded cups, or even barcodes. However, who would want to
switch to a different cup after becoming attached to your own this-coffee-
will-make-me-lucky-drinking equipment? Who could stand the sight of an ugly
barcode stuck on your best desk-friend? And finally, who would use something
as crude as an optical sensor if an elegant and more advanced RF technique is
available? Clearly, it is only a matter of time until such a system will be
employed in every office around the world!&lt;/p&gt;

</description>
                <link>https://dobots.nl/2013/03/11/maker-inspiration
                <guid>https://dobots.nl/2013/03/11/maker-inspiration</guid>
                <pubdate>2013-03-11T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>3D printing at DoBots</title>
                <description>
&lt;h1 id="3d-printing"&gt;3D printing&lt;/h1&gt;

&lt;p&gt;The &lt;a href="/2012/07/26/snakebot-project/"&gt;Snakebot Project&lt;/a&gt; showed us the benefits of 3D printing when designing a
robot. In particular, 3D printing allows for quick prototyping of parts with
great accuracy and strength. This lead us to look for a 3D printer of our own,
to aid us in building Dotty.&lt;/p&gt;

&lt;h2 id="available-printers"&gt;Available printers&lt;/h2&gt;

&lt;p&gt;Due to the fact that more and more people are into designing and making things
for themselves, 3D printers have become a widespread phenomenon among makers.
Open-source printers and schematics can easily be found online for anyone who
is interested in building one.&lt;/p&gt;

&lt;p&gt;The &lt;a href="http://www.reprap.org"&gt;reprap&lt;/a&gt; community is one of the largest, dedicated
to open-source 3D printing and provided us with numerous insights in
performance criteria, robustness and printing volumes. The idea behind reprap
is that people can help eachother by printing parts for new 3D printers. For
anyone eager to learn about 3D printing, the Reprap wiki is one of the best
places to start.&lt;/p&gt;

&lt;p&gt;In the end we found a local printer manufacturer
&lt;a href="http://www.felixprinters.com"&gt;felixprinters&lt;/a&gt; that could supply us with all
necessary parts and instructions to build our own.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/3d_printer_0.jpg" alt="Felix printer" title="Felix 3D printer" /&gt;&lt;/p&gt;

&lt;h2 id="printing"&gt;Printing&lt;/h2&gt;

&lt;p&gt;One of the first things we noticed when printing is that calibration is
everything. The smallest of offsets in any one of the axes will result in
subpar prints or even total failures.&lt;/p&gt;

&lt;p&gt;Over the course of printing our first parts, we saw gradual improvement. Both
due to our own tweaking to the calibration mechanics and adjustments to the
printing parameters in the software host.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/3d_printer_1.jpg" alt="Printer calibration" title="Fial!" /&gt;&lt;/p&gt;

&lt;p&gt;Parameters such as bed temperature, fill ratio and nozzle temperature all
influence the outcome of the print job. The bed temperature in particular
caused a couple of botched printing jobs. Set the bed temperature too low and
the print will contort due to the heat gradient. Set it too high and the print
will sink a little due to sagging at the bottom. In the end, a bed temperature
of 65°C provided us with good results. Tweaking all printing parameters is a
good way to gain insight in the working of the machine.&lt;/p&gt;

&lt;p&gt;Another thing we needed to get used to was the printing speed. Depending on
the complexity, even small objects can take up to half an hour to complete.
Printing this cat for instance took upwards of two hours.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/3d_printer_2.jpg" alt="Printed cat" title="Printed cat" /&gt;&lt;/p&gt;

&lt;h2 id="cad-programs"&gt;CAD Programs&lt;/h2&gt;

&lt;p&gt;To create our own parts, we need to model them in a 3D CAD program. Luckily
there are multiple open-source CAD programs available. You should distinguish
between solid modeling or shell modeling, the former being more geared towards
modeling real solid objects.&lt;/p&gt;

&lt;p&gt;One of the more commonly used solid modelers is “openSCAD”. Instead of editing
a 3D object by direct manipulations, this modeler uses an input that describes
the model form by lines of code. This has as added bonus that it is easy to
alter any object easily to fit different specifications, or to add options
with the change of a single variable.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/3d_printer_3.jpg" alt="Dotty components" title="Dotty components" /&gt;&lt;/p&gt;

&lt;p&gt;This approach is particularly useful to people that are used to programming,
of which we have plenty here at the office. Other popular modeling options
include “Blender” and Google’s “Sketchup”, though these are shell modeling
programs and require model closing before it can be used as a printable file.&lt;/p&gt;

&lt;h2 id="dotty"&gt;Dotty&lt;/h2&gt;

&lt;p&gt;We intend to use our newly found printing toy to make casings and frames for
the &lt;a href="/2012/08/26/the-dotty-robot"&gt;Dotty&lt;/a&gt;. Right now we’re trying out multiple variants, so be sure
to check back here when we have more to show you.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2013/03/07/3d-printing-at-do-bots
                <guid>https://dobots.nl/2013/03/07/3d-printing-at-do-bots</guid>
                <pubdate>2013-03-07T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>FireSwarm operator console</title>
                <description>
&lt;h1 id="fireswarm-operator-console"&gt;FireSwarm operator console&lt;/h1&gt;

&lt;p&gt;The UAV operator console for the fire drones in the FireSwarm project
(&lt;a href="http://www.fireswarm.nl"&gt;http://www.fireswarm.nl&lt;/a&gt;) is created using node.js, of which the code can be
found on &lt;a href="https://github.com/mrquincle/uav-console"&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src="https://raw.github.com/mrquincle/uav-console/master/doc/uav_console.png" alt="FireSwarm Console" style="width: 800px" /&gt;&lt;/p&gt;

&lt;p&gt;This software is in an alpha-stage, but it demonstrates the power of node.js
and the community behind it. The convenience of the node package manager, npm,
is unprecedented (although, it is of course similar to apt-get, and package
managers that exists for distros).&lt;/p&gt;

</description>
                <link>https://dobots.nl/2013/02/28/fireswarm-operator-console
                <guid>https://dobots.nl/2013/02/28/fireswarm-operator-console</guid>
                <pubdate>2013-02-28T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>UAV path planning</title>
                <description>
&lt;h1 id="uav-path-planning"&gt;UAV path planning&lt;/h1&gt;

&lt;p&gt;The mission of the UAVs is to find a fire in an area. This leads to the problem of covering an area as fast as possible.
An optimal path for each UAV could be calculated in advance, such plans however are not very flexible.
UAVs cannot follow planned paths perfectly and there is no room for rechecking a possible fire.
An interesting method is to use potential fields, resulting in a landscape where the UAVs are repelled by hills and attracted by valleys.
As the landscape changes over time, the path planner keeps updating the plan.
This method enables us to incorporate various other influences in the plan.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Coverage: Repellers are placed on places that UAVs already observed, so that those places are not visited again.&lt;/li&gt;
  &lt;li&gt;Collision avoidance: Repellers are placed on future path of UAVs, so that UAVs won’t fly in the way of other UAVs.&lt;/li&gt;
  &lt;li&gt;Edges: Repellers are placed on the edges, so that the UAVs won’t leave the allowed area.&lt;/li&gt;
  &lt;li&gt;Fires: Attractors are placed on possible fires, so that the UAVs can recheck to be sure. Once rechecked the attractor can be removed.&lt;/li&gt;
  &lt;li&gt;Battery: An Attractor can be placed at the base when the battery is low, so that the UAV is ready to land.&lt;/li&gt;
  &lt;li&gt;Connectivity: Either attractors or repellers can be placed to keep all UAVs and the base connected.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Simple simulations are used to get statistics about coverage.
For these simulations, the flight dynamics, radio transmission and other things are simplified.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/nSNsEQN4LKQ?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;p&gt;Simulations with the software that will be on the UAVs are performed to check the simple simulations and to see how collision avoidance and landing works out.
With this software we can also perform hardware in the loop simulations, so that timing becomes visible.
A later blog post will reveal more details about the software. In this video, all UAVs fly at a different height, so collision avoidance will not be observed.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/-EJqMIBsEt0?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;p&gt;It should be possible for the operator to call all UAVs back at the same time, while the UAVs cannot all land at the same time.
By a simple queue system, each UAV will wait for lower flying UAVs in queue to land first.
In this video, the landing is made visible by a sudden turn towards the ground station, which in reality will be a landing performed by the auto pilot.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/HxtyKUiYGrw?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;
</description>
                <link>https://dobots.nl/2013/01/21/uav-path-planning
                <guid>https://dobots.nl/2013/01/21/uav-path-planning</guid>
                <pubdate>2013-01-21T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Swarm Control - Dancing Robots</title>
                <description>
&lt;h1 id="swarm-control---dancing-robots"&gt;Swarm Control - Dancing Robots&lt;/h1&gt;

&lt;p&gt;In order to illustrate the Robot Dancing feature in our &lt;a href="https://play.google.com/store/apps/details?id=org.dobots.swarmcontrol"&gt;Swarm Control App&lt;/a&gt;, we decided to show it in action.&lt;/p&gt;

&lt;p&gt;In the following two videos you can see several robots doing a dance together.
In the video on the left, we control two LEGO Mindstorms NXT, an iRobot Roomba
with Bluetooth, a Wowwee RoboScooper with Brainlink and our home made robot
&lt;a href="/2012/08/26/the-dotty-robot"&gt;Dotty&lt;/a&gt;. In the video on the right, we removed the RoboScooper and added an AR Drone 2.0 instead.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/U0sYzpzBetA?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;
&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/ZgrdEz7YqEM?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

</description>
                <link>https://dobots.nl/2013/01/11/swarm-control-dancing-robots
                <guid>https://dobots.nl/2013/01/11/swarm-control-dancing-robots</guid>
                <pubdate>2013-01-11T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Swarm Control Update</title>
                <description>
&lt;h1 id="swarm-control-update"&gt;Swarm Control Update&lt;/h1&gt;

&lt;p&gt;Continuing our development of the &lt;a href="https://play.google.com/store/apps/details?id=org.dobots.swarmcontrol"&gt;Swarm Control app&lt;/a&gt; we now
released an updated version v0.2. In this version we included more robots to
control and started to tackle the goal to control a swarm of robots. In
addition, we tried to make the remote control more user friendly and added a
joystick element to the user interface for driving.&lt;/p&gt;

&lt;h2 id="new-robots"&gt;New Robots&lt;/h2&gt;

&lt;p&gt;In order to reach our goal to control a heterogeneous swarm of robots we first
had to add more robots to our swarm; the more different their capabilities and
sensors the better.&lt;/p&gt;

&lt;h3 id="meccano-spykee"&gt;Meccano Spykee&lt;/h3&gt;

&lt;p&gt;With the Spykee we extended our collection of robots by the first WiFi robot.
It comes with a docking station and includes a USB webcam, a microphone, a
speaker and 5 leds. With the Spykee we are now able to stream video from the
robot to the phone.&lt;br /&gt;
For a more detailed review of the Spykee see our blog post
&lt;a href="/2011/12/21/review-of-meccano-spykee-spy-robot/"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id="prerequisites"&gt;Prerequisites&lt;/h4&gt;

&lt;p&gt;By default the Spykee creates an AdHoc network and waits for connections but
it has also the functionality to connect to any AccessPoint. Because Android
prevents the phones from connecting to AdHoc networks the settings have to be
adjusted so that it connects to the same network that the phone is using. Next
in the app, when connecting to the Spykee the settings have to be entered (IP
Address of the Spykee, Port (default 9000), user name and password).&lt;/p&gt;

&lt;h4 id="control"&gt;Control&lt;/h4&gt;

&lt;p&gt;With this setup we are now able to control the Spykee remotely:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Drive the Spykee with one of the available control options (Arrow Keys, Accelerometer, Joystick)&lt;/li&gt;
  &lt;li&gt;Turn the leds on and off&lt;/li&gt;
  &lt;li&gt;Start the automatic Docking (if it is undocked) and start Undock (if it is docked)&lt;/li&gt;
  &lt;li&gt;Let the Spykee play one of it’s pre-recorded sound effects&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id="sensors"&gt;Sensors&lt;/h4&gt;

&lt;p&gt;As sensors, the Spykee provides us with the battery level and the docking
state (Docked / Undocked) and in addition sends the video of the USB webcam as
a stream to be displayed on the phone.&lt;/p&gt;

&lt;h3 id="wowwee-roboscooper"&gt;Wowwee RoboScooper&lt;/h3&gt;

&lt;p&gt;The RoboScooper is a remote controllable robot that can autonomously pick up
small objects from the floor. It is equipped with IR sensors to detect and
avoid objects as well as detect objects between it’s hands to pick up. In
addition, it has bumper sensors in it’s index fingers to detect if it hits an
obstacle. The RoboScooper comes with an IR remote control to drive it around
manually but has no means to connect directly to a phone or computer.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/roboscooper_brainlink.png" alt="RoboScooper equiped with Brainlink" title="RoboScooper equiped with Brainlink" /&gt;&lt;/p&gt;

&lt;h4 id="prerequisites-1"&gt;Prerequisites&lt;/h4&gt;

&lt;p&gt;In order to control the RoboScooper from an Android phone we used a
&lt;a href="http://www.brainlinksystem.com/"&gt;brainlink&lt;/a&gt;. This handy and lightweight
device is equipped with Bluetooth, an IR Receiver and Emitter and additional
IO connectors. Strapped to the head of the Roboscooper, we can now connect it
with any Android phone supporting Bluetooth and send it any signal available
from the remote control.&lt;/p&gt;

&lt;h4 id="control-1"&gt;Control&lt;/h4&gt;

&lt;p&gt;Thanks to the brainlink we are able to control the RoboScooper remotely:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Drive the Spykee with one of the available control options (Arrow Keys, Accelerometer, Joystick)&lt;/li&gt;
  &lt;li&gt;Enable / Disable Vision&lt;/li&gt;
  &lt;li&gt;Enter one of the Play Modes: Clean Sweep, Pick-up, Talk or Whack&lt;/li&gt;
  &lt;li&gt;Pick-up / Dump objects&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id="sensors-1"&gt;Sensors&lt;/h4&gt;

&lt;p&gt;The RoboScooper itself doesn’t provide us with any of it’s sensory feedback.
However we can display the sensors of the brainlink which are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Battery Level (of the Brainlink)&lt;/li&gt;
  &lt;li&gt;Light Sensor&lt;/li&gt;
  &lt;li&gt;Accelerometer (x, y and z-Axis in Gs)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id="brainlink"&gt;Brainlink&lt;/h3&gt;

&lt;p&gt;With the Brainlink we can now add any IR controlled robot to our app. More
precisely, with the utility tool ‘Signal Analyzer’ we can capture, analyze and
store any IR signal as long as we have a remote control available. Once the
signals are stored, the device file has to be placed on the phone and the
program will transmit the respective signal to the brainlink which will
convert the signal to IR pulses and emit them from the IR emitter.&lt;br /&gt;
The same way the utility tool ‘Signal Analyzer’ is using the brainlink to
capture IR signals, we could also use it programatically to detect IR signals
and respond to them accordingly.&lt;/p&gt;

&lt;p&gt;Note that this concept can also be applied to any other IR controlled device
and is not restricted to robots only.&lt;/p&gt;

&lt;h3 id="parrot-ardrone"&gt;Parrot AR.Drone&lt;/h3&gt;

&lt;p&gt;&lt;img src="/attachments/swarmcontrol_ardrone.png" alt="Parrot AR.Drone" title="Parrot AR.Drone" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;One of the more interesting in our choice of robots
is the AR Drone, a fully autonomous, radio controlled quadrocopter. It
connects to a device over WiFi and has two cameras which can be displayed: a
wide angle camera pointing forward and a high speed camera pointing down. A 6
degrees of freedom inertial measurement unit provides pitch, roll and and yaw
measures which are used for stabilization and assisted tilting control. In
addition, the AR Drone uses its vertical camera to detect ground movement
which is used together with the gyroscope to stabilize the drone in hover
mode. An ultrasound altimeter provides altitude and vertical speed control.
All in all an excellent choice for a flying robotic platform. A detailed
review can be found &lt;a href="http://www.rchelicopterfun.com/parrot-ar-drone.html"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The AR Drone comes in to versions. The AR Drone and the AR Drone 2.0 which are
both supported in our app.&lt;/p&gt;

&lt;h4 id="prerequisites-2"&gt;Prerequisites&lt;/h4&gt;

&lt;p&gt;The AR Drone 2.0 creates an AccessPoint and is ready to for connecting with
any Android phone. The AR Drone however creates an AdHoc Network by default
and cannot be used with an Android phone out of the box. To overcome this, the
firmware of the Parrot needs to be updated to 1.7.4 in order to turn the WiFi
to Access Point mode. The firmware and a tutorial can be found
&lt;a href="http://ardrone.parrot.com/parrot-ar-drone/uk/support/update"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id="control-2"&gt;Control&lt;/h4&gt;

&lt;p&gt;With ths we can now control the AR Drone remotely:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Take Off / Land&lt;/li&gt;
  &lt;li&gt;Execute the following basic moves with Arrow keys:
    &lt;ul&gt;
      &lt;li&gt;Forward / Backward&lt;/li&gt;
      &lt;li&gt;Left / Right&lt;/li&gt;
      &lt;li&gt;Rotate Left / Right&lt;/li&gt;
      &lt;li&gt;Up / Down&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Set Altitude&lt;/li&gt;
  &lt;li&gt;Switch Camera (Front / Down)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id="sensors-2"&gt;Sensors&lt;/h4&gt;

&lt;p&gt;The AR Drone provides us with measurement and navigational data such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Battery level&lt;/li&gt;
  &lt;li&gt;Acceleration Values (x, y, z-Axis)&lt;/li&gt;
  &lt;li&gt;Pitch / Roll / Yaw Values&lt;/li&gt;
  &lt;li&gt;Altitude&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, we can turn on and display the video stream received from the AR
Drone.&lt;br /&gt;
For the AR Drone, this consists of a 93° wide angle front camera with a
resolution of 640x480 VGA and a high speed 64° vertical camera with 60 fps.&lt;br /&gt;
For the AR Drone 2.0 the front camera was replaced with a 720p HD camera
providing 30 fps.&lt;/p&gt;

&lt;h3 id="dobots-bot-aka-dotty"&gt;DoBots Bot aka Dotty&lt;/h3&gt;

&lt;p&gt;&lt;img src="/attachments/dotty.png" alt="Dotty, our home-made robot" title="Dotty, our home-made robot" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;Last but not least &lt;a href="/2012/08/26/the-dotty-robot"&gt;Dotty&lt;/a&gt;, our home-made robot was included in this update. Dotty is our prototype for an autonomously recharging swarm robot.
It’s bluetooth module enables us to directly connect it with an Android phone.
It has a two motors which can be controlled separately and several sensors
which can be streamed and displayed on the phone.&lt;/p&gt;

&lt;h4 id="control-3"&gt;Control&lt;/h4&gt;

&lt;p&gt;At the current state, the robot can be driven around remotely from the phone
and the streaming interval of the sensors can be adjusted.&lt;/p&gt;

&lt;h4 id="sensors-3"&gt;Sensors&lt;/h4&gt;

&lt;p&gt;In the current version, Dotty includes the following sensors which can be
displayed on the phone:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Infrared&lt;/li&gt;
  &lt;li&gt;Battery&lt;/li&gt;
  &lt;li&gt;Light&lt;/li&gt;
  &lt;li&gt;Microphone&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src="/attachments/swarmcontrol_robotlist.png" alt="Robot list for dancing swarm behaviour" title="Robot list for dancing swarm behaviour" class="float-right" /&gt;&lt;/p&gt;

&lt;h2 id="swarm-behaviour"&gt;Swarm Behaviour&lt;/h2&gt;

&lt;p&gt;To start with a simple showcase of controlling a set of different robots we
added the dancing swarm action to the app. Here any of the available robots
can be added to a list of dancing partners. In a next step, a Dance can be
created out of a set of predefined Moves. For now we started with the basic
moves consisting of: 1) forward / backward 2) left / right 3) rotate left /
right. Once the dance is started, every added robot will go through each of
the moves until the dance is over.&lt;br /&gt;
In addition, behind the scenes, a start-up and a shutdown phase were added
before and after the dance so that each robot can be prepared for remote
control. For example the AR Drone has to take off before the dance can start
and land again once it is finished.&lt;br /&gt;
To account for the differences between the robots without it reflecting back
to the control algorithm, a robot will only execute a move if it is available.
Otherwise it will skip the step and wait for the next (for example only the AR
Drone can move left and right, the other robots can only rotate left and
right).&lt;/p&gt;

&lt;h3 id="robot-list"&gt;Robot List&lt;/h3&gt;

&lt;p&gt;On starting up the dancing swarm action the list of robots has to be set up.
After adding a robot to the list, it has to be connected. If the connection
was successful, the status will change from Disconnected to Connected and the
address of the selected robot will be shown next to its name. Once the
connection is established, the screen for the individual robot can be
displayed and its settings can be adjusted.&lt;/p&gt;

&lt;h3 id="remote-control"&gt;Remote Control&lt;/h3&gt;

&lt;p&gt;Once all robots are connected, the group can be remote controlled the same way
as a single robot, using either the arrow keys or the joystick.&lt;/p&gt;

&lt;p&gt;Note: the joystick is not available for the AR Drone&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/swarmcontrol_dancelist.png" alt="Created dance list" title="Created dance list" class="float-right" /&gt;&lt;/p&gt;

&lt;h3 id="dance-list"&gt;Dance List&lt;/h3&gt;

&lt;p&gt;To create a dance, one of the available moves can be selected from the drop
down menu. If desired, the move duration can be adjusted and the move added to
the list. The current dance list will be displayed in the list at the top.&lt;br /&gt;
Once the list is completed, a press on Start will let the robots execute the
dance. (The current step will be highlighted in red.)&lt;/p&gt;

&lt;h2 id="outlook"&gt;Outlook&lt;/h2&gt;

&lt;p&gt;The app is still under heavy development and although a lot of our current
stock of robots were added already there is lots of interesting stuff to come&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;More Robots! A swarm can never be too big! There is still the Surveyor, the SpyGear Trakr, the Finch and the Terrain Twister But we are always looking for more, so if you know of any other robots with sensor which could be added to our collection let us know!&lt;/li&gt;
  &lt;li&gt;We just started to tickle the iceberg of swarm control behaviours and our next ventures will be in the field of Multi Robot SLAM&lt;/li&gt;
  &lt;li&gt;Interface robots with home automation systems, thus enabling them to interact with their surroundings and/or benefiting from sensors in their environment.&lt;/li&gt;
  &lt;li&gt;And not forgetting, the robots want to be hooked up to the Internet and the cloud&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are interested you should definitely keep an eye on the blogs of this 
&lt;a href=""&gt;website&lt;/a&gt;, where we will keep track of the development.
But also the app is completely open source; you can find the code
at &lt;a href="https://github.com/eggerdo/swarm-control"&gt;https://github.com/eggerdo/swarm-control&lt;/a&gt; and the ready app on &lt;a href="https://play.google.com/store/apps/details?id=org.dobots.swarmcontrol"&gt;Google Play&lt;/a&gt;.
Feel free to contribute and add your own robots to the list or let us know
which ones you want to see added!&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/12/04/swarm-control-update
                <guid>https://dobots.nl/2012/12/04/swarm-control-update</guid>
                <pubdate>2012-12-04T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Particle filter for tracking</title>
                <description>
&lt;h1 id="particle-filter-to-track-robots"&gt;Particle filter to track robots&lt;/h1&gt;

&lt;p&gt;Particle filter demo of our home-made robot, the dotty. The IP webcam used in
this demo is quite old, so the quality is not that good. You will see that the
pockets of the pool table are quite hard (the particle filter is tempted to
track those instead of the robot when the robot gets close). You can find the
code for the particle filter at
&lt;a href="https://github.com/mrquincle/particlefilter"&gt;github&lt;/a&gt;, feel free to improve
it.&lt;/p&gt;

&lt;p&gt;There is not much time spend on this implementation. It is only meant as an
exercise to get familiar with particle filtering techniques. There are
approximate and generalised forms of belief propagation on networks of robots
that require such expertise. Subsequently this can be used for either
simultaneous localisation and mapping, tracking, or certain common visual
tasks.&lt;/p&gt;

&lt;p&gt;With respect to the latter, it would be cool if an image segmentation task
across a multi-agent system would indeed be solved as one general problem
where the structure is exploited, rather than some ad-hoc user-defined
decomposition depending on the coffee quantity in the morning.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/LmxHQdzytRw?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;p&gt;For other algorithms do also not hesitate to visit the website
&lt;a href="http://almende.github.com/nets/"&gt;NETS&lt;/a&gt; dedicated to algorithms only!&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/11/11/particle-filter-for-tracking
                <guid>https://dobots.nl/2012/11/11/particle-filter-for-tracking</guid>
                <pubdate>2012-11-11T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Localization</title>
                <description>
&lt;h1 id="localization"&gt;Localization&lt;/h1&gt;

&lt;p&gt;This is a follow-up on the earlier post about
&lt;a href="/2012/02/28/a-brief-story-of-slam/"&gt;SLAM&lt;/a&gt;. Localization
is inherent to SLAM if the system is working perfectly, but that usually requires
expensive sensors like laser range finders and lots of computing power. As we
envision cheap and expendable robots, we would like to know whether reliable
localization is also possible in a simpler way.&lt;/p&gt;

&lt;p&gt;For outdoor robots, the solution is easy: existing maps with GPS (and in the
near future, &lt;a href="http://www.esa.int/esaNA/galileo.html"&gt;Galileo&lt;/a&gt;). This is the
approach that is used for &lt;a href="http://fireswarm.nl/"&gt;Fireswarm&lt;/a&gt;. For indoor
robots, such a simple, almost luxurious solution is not available,
unfortunately. Thus, we have to rely on a number of sensors, like a compass,
gyroscope and distance sensors. There is one sensor that can really give a lot
of information for almost no money though: a camera. Can we use something like
a webcam for robust SLAM?&lt;/p&gt;

&lt;h2 id="visual-slam"&gt;Visual SLAM&lt;/h2&gt;

&lt;p&gt;In fact, this has already been achieved with
&lt;a href="http://www.robots.ox.ac.uk/~gk/PTAM/"&gt;PTAM&lt;/a&gt;. Unfortunately, PTAM and its
dependencies have not been maintained the past two years. The result is
software that is hard to get working (especially on non-standard platforms
such as smartphones) and does not include the latest developments in SLAM
algorithms, especially regarding loop closing and bundle adjustment.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/2zZ4MrcAqNQ?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;p&gt;This means we have work to do! First of all, we have to decide whether we can
make adaptations to the environment. Markers and so on make the task easier,
but may not be acceptable in every application. A map that is known beforehand
also solves part of the task: most of the mapping part of the SLAM algorithm.
Of course, we should solve the localization task in the worst-case scenario of
an unknown dynamic environment.&lt;/p&gt;

&lt;h2 id="ceiling-odometry"&gt;Ceiling odometry&lt;/h2&gt;

&lt;p&gt;As a start, we are building a simple 2d demo. We can use a camera to look at
the ceiling and use this video stream to estimate where we have walked. The
first step is to detect and track features on the ceiling. This may seem easy,
but to find stable features for tracking is easier said than done. To track
these features and find out where they moved, we can use the iterative Lucas-
Kanade method*. &lt;a href="http://docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html"&gt;This method&lt;/a&gt; is built into OpenCV and
gives good results.&lt;/p&gt;

&lt;p&gt;Then, we can calculate the
&lt;a href="http://en.wikipedia.org/wiki/Homography"&gt;homography&lt;/a&gt; between the old and new
locations of the features. The homography is a matrix describing the
transformation between the coordinates. Because the features are all on the
same stationary plane, the homography matrix actually describes how the camera
has moved. Again, OpenCV has &lt;a href="http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html?highlight=findhomography#cv.FindHomography"&gt;a function&lt;/a&gt; to do all the dirty work for us, including
&lt;a href="http://en.wikipedia.org/wiki/RANSAC"&gt;RANSAC&lt;/a&gt; for outlier rejection. If I get
around to working on this a bit more, this demo will soon be available as an
app for Android!&lt;/p&gt;

&lt;p&gt;*Alternatively, we can use &lt;a href="http://en.wikipedia.org/wiki/Phase_correlation"&gt;phase correlation&lt;/a&gt;, which is based on a Fourier transform over the old and new images. OpenCV only includes a function to use this method for translations, but it has been shown that it can also be used to find rotations. While initial tests did not give great results, I would like to look into this further, as it seems to be a good method to quickly use all the information in an image to estimate rotation and translation.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/11/01/localization
                <guid>https://dobots.nl/2012/11/01/localization</guid>
                <pubdate>2012-11-01T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>The Dotty robot</title>
                <description>
&lt;h1 id="the-dotty"&gt;The Dotty&lt;/h1&gt;

&lt;p&gt;&lt;img src="/attachments/Dotty_open_hardware_printed_circuit_board_small.jpg" alt="The Dotty PCB" title="Open Hardware PCB" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;Dotty is the name we’ve given to our prototype swarm robot. Dotty is a simple, extremely cheap robot.&lt;/p&gt;

&lt;p&gt;Dotty is an open hardware platform for experimental and educational purposes. We use our Dotties to create swarming behaviour. Using machine learning algorithms, the swarm of Dotties learns to carry out group tasks.&lt;/p&gt;

&lt;p&gt;Dotty is able to automatically recharge itself, by searching out an induction charger platform. Further, she can easily be connected to a smartphone, adding ‘brains’ and connectivity to our cheap robot.&lt;/p&gt;

&lt;h2 id="components"&gt;Components&lt;/h2&gt;

&lt;p&gt;Dotty is made up of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a LilyPad Arduino&lt;/li&gt;
  &lt;li&gt;a printable circuit board of our own design, with:
    &lt;ul&gt;
      &lt;li&gt;a Bluetooth chip&lt;/li&gt;
      &lt;li&gt;light sensor&lt;/li&gt;
      &lt;li&gt;infrared sensor&lt;/li&gt;
      &lt;li&gt;microphone&lt;/li&gt;
      &lt;li&gt;current sensor&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href="http://www.arexx.com/arexx.php?cmd=goto&amp;amp;cparam=p_robot_chassis"&gt;AREXX frame&lt;/a&gt; with 3 wheels, 2 motors and a battery.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All in all, Dotty we aim to keep the material costs for Dotty under 100 euros (we actually aim for 70 euros).&lt;/p&gt;

&lt;p&gt;Dotty components head&lt;/p&gt;

&lt;h2 id="open-hardware"&gt;Open hardware&lt;/h2&gt;

&lt;p&gt;&lt;img src="/attachments/Dotty_head.jpg" alt="Dotty's head" title="Dotty's head" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;DoBots is a software company, and our main aim is to develop software for swarm robot applications. Dotty is an exception to this rule. Since we could not find an open hardware platform to suit our needs, we designed our own.&lt;/p&gt;

&lt;p&gt;We do not intend to sell or produce any hardware, though. The specs for Dotty (printable circuit board, 3D-models) will be made available via our website, so roboticists can build their own Dotties.&lt;/p&gt;

&lt;p&gt;If any party would like to mass produce Dotty, don’t hesitate to contact us!&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/Dotty_head_components.jpg" alt="Dotty's head with parts" title="Dotty's head with components" class="float-right" /&gt;&lt;/p&gt;

&lt;h2 id="alternatives"&gt;Alternatives&lt;/h2&gt;

&lt;p&gt;Fortunately, we are no longer the only company aiming to develop a smart robot in this price class. Some robots that come pretty close to our needs are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href="http://www.gctronic.com/shop.php"&gt;Elisa-3&lt;/a&gt; (Gctronic, 280 euros without charger, Arduino-compatible, various sensors, IR receiver, RF radio, USB)&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://www.mobsya.org/en-shop"&gt;Thymio II&lt;/a&gt; (EPFL/ECAL, 80 euros, USB, various sensors, no wireless connectivity or charging)&lt;/li&gt;
&lt;/ul&gt;
</description>
                <link>https://dobots.nl/2012/08/26/the-dotty-robot
                <guid>https://dobots.nl/2012/08/26/the-dotty-robot</guid>
                <pubdate>2012-08-26T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Swarm Control App</title>
                <description>
&lt;h1 id="swarm-control-app"&gt;Swarm Control App&lt;/h1&gt;

&lt;p&gt;&lt;img src="/attachments/swarmcontrol_main.png" alt="Swarm Control" title="Swarm Control" class="float-right" /&gt;&lt;/p&gt;

&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In our attempt to control a swarm of robots we came up with a first draft of
an app that will let you connect to a wide range of different robots,
including but not limited to toy robots, cleaning robots, flying robots and
our self developed arduino robot aka “Dotty”. The app provides remote control
for the different robots and provides the user with their sensor information.
In a second stage the app will make use of the available sensors of each
individual robot and control the robots either individually or as a swarm,
depending on the task.&lt;/p&gt;

&lt;p&gt;Another goal of the app will be to connect the robots to the Internet and
interface them with the cloud. This will serve several purposes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The robots can be connected with each other even if they are physically far apart and communicate among them without the need of direct links&lt;/li&gt;
  &lt;li&gt;The robots can store their sensor information in the cloud avoiding the need of storage capacity on the robot itself and facilitating the sharing of data between robots&lt;/li&gt;
  &lt;li&gt;Computation- intensive tasks such as image processing or navigation can be offloaded to the cloud.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The swarm behaviours available in the app will cover subjects like playing
(eg. play tag, hide and seek, etc), moving (eg. marching, following, dancing
etc), chatting (eg. count each other) and charting (eg. searching, rescuing,
guarding etc).&lt;/p&gt;

&lt;h3 id="release-version-10"&gt;Release: Version 1.0&lt;/h3&gt;

&lt;p&gt;The app can be found on the Android market as the &lt;a href="https://play.google.com/store/apps/details?id=org.dobots.swarmcontrol"&gt;Swarm Control app&lt;/a&gt;. In the first
draft we added the following two robots:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/swarmcontrol_roomba.png" alt="iRobot Roomba" title="iRobot Roomba" class="float-right" /&gt;&lt;/p&gt;

&lt;h2 id="irobot-roomba"&gt;IRobot Roomba&lt;/h2&gt;

&lt;p&gt;The iRobot Roomba 521 is equipped with a Bluetooth module
(&lt;a href="https://www.sparkfun.com/products/684?"&gt;RooTooth&lt;/a&gt; from Sparkfun) in order to
connect it to the smartphone.&lt;/p&gt;

&lt;h3 id="control"&gt;Control&lt;/h3&gt;

&lt;p&gt;With this wireless connection we can control the Roomba remotely:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Start autonomous behaviour actions provided by the Roomba such as Clean, Spot and Dock&lt;/li&gt;
  &lt;li&gt;Activate the brushes and vacuum cleaner&lt;/li&gt;
  &lt;li&gt;Move the Roomba manually forward and backward as well as rotate the Roomba on the spot using arrow buttons&lt;/li&gt;
  &lt;li&gt;Drive the Roomba around using the smartphone’s accelerometer&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id="sensors"&gt;Sensors&lt;/h3&gt;

&lt;p&gt;We can also query all of the Roomba’s sensors and display them on the
smartphone. This means we can see if the Roomba detects a cliff, bumps in an
obstacle or detects a wall. We can also monitor the power consumption, the
remaining battery charge and the charging state.&lt;/p&gt;

&lt;p&gt;For now this information is only displayed on the smartphone and not used
further. But in a later stage, this sensory information will allow us to
develop behaviour for a swarm of Roombas. One possible scenario could be a
team of two or more robots which coordinate the cleaning of a floor and use
the same charging station but in such a way that there will never be one robot
waiting for another to finish.&lt;/p&gt;

&lt;h2 id="mindstorms-nxt"&gt;Mindstorms NXT&lt;/h2&gt;

&lt;p&gt;&lt;img src="/attachments/swarmcontrol_nxt.png" alt="Mindstorms NXT" title="Mindstorms NXT" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;The Mindstorms NXT 2.0 comes equipped with a Bluetooth module which made it
perfect for a fast incorporation. We fixed the design of the robot to a
differential robot for now and connected the left wheel motor to output port A
and the right wheel motor to output port B.&lt;/p&gt;

&lt;h3 id="control-1"&gt;Control&lt;/h3&gt;

&lt;p&gt;As with the Roomba, the NXT can be controlled remotely:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Move the NXT forward and backward as well as rotate it on the spot using arrow buttons&lt;/li&gt;
  &lt;li&gt;Drive the NXT around using the smartphone’s accelerometer&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id="sensors-1"&gt;Sensors&lt;/h3&gt;

&lt;p&gt;In contrary to the Roomba we can choose the type of sensors we want to add to
the NXT from a wide variety. Moreover, we are not fixed to a certain design of
the robot but want to be able to add robots with different sets of sensors. To
reflect this we can define in the app for each input port what type of sensor
is attached and enable only the sensors we want to observe.&lt;/p&gt;

&lt;p&gt;Besides the sensors we can also monitor the encoder of the motors which
provide us with a tacho count and gives us a sense of direction and distance
that the robot has travelled.&lt;/p&gt;

&lt;h2 id="upcoming"&gt;Upcoming&lt;/h2&gt;

&lt;p&gt;Since the app is just starting to take shape and is under heavy development
there are a lot of things that need to be done still.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Several Robots are waiting to be included in the app: Surveyor, Meccano Spykee, SpyGear Trakr, FinchRobot, and many more&lt;/li&gt;
  &lt;li&gt;Multiple robots should be able to be paired into a swarm and then controlled simultaneously&lt;/li&gt;
  &lt;li&gt;Swarm Behaviours need to be implemented, starting with simple flock movements and dancing patterns&lt;/li&gt;
  &lt;li&gt;And not forgetting, the robots want to be hooked up to the Internet and the cloud&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are interested you should definitely keep an eye on the blogs of this 
&lt;a href=""&gt;website&lt;/a&gt;, where we will keep track of the development. But also the app is completely open source; you can find the code
at &lt;a href="https://github.com/eggerdo/swarm-control"&gt;https://github.com/eggerdo/swarm-control&lt;/a&gt;. Feel free to contribute and add
your own robots to the list or let us know which ones you want to see added!&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/08/09/swarm-control-app
                <guid>https://dobots.nl/2012/08/09/swarm-control-app</guid>
                <pubdate>2012-08-09T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Snakebot Project</title>
                <description>
&lt;h1 id="snakebot-project"&gt;Snakebot Project&lt;/h1&gt;

&lt;p&gt;&lt;img src="/attachments/snake_example.png" alt="Snakebot animation" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;Over the past semester, we have been working with a group of students from the Hogeschool Rotterdam (University of Applied Sciences).&lt;/p&gt;

&lt;p&gt;The assignment: design a robot that is capable of locomotion on land, in shallow water, on sea beds and on river banks.&lt;/p&gt;

&lt;p&gt;The result: a functioning 1,5m SnakeBot prototype.&lt;/p&gt;

&lt;h2 id="why-a-snake"&gt;Why a snake?&lt;/h2&gt;

&lt;p&gt;One of the use cases that we have in mind for our swarm robots is
environmental monitoring. Swarms of unobtrusive, expendable robots canvassing
forests, rivers and seas, keeping an eye on water levels and air quality.&lt;/p&gt;

&lt;p&gt;One acute problem to solve in The Netherlands is the overgrowth of
cyanobacteria (blue-green algae) that poison recreational waters. These
bacteria can multiply very quickly, so it is important to keep a close eye on
lakes, and quickly detect and fight growing bacterial colonies.&lt;/p&gt;

&lt;p&gt;So our swarm robots must be able to move through difficult terrain, such as
shallow water. Wheeled vehicles or big boats are out of the question. Why not
take a hint from mother nature, and copy the snake design?&lt;/p&gt;

&lt;h2 id="3d-printing"&gt;3D printing&lt;/h2&gt;

&lt;p&gt;&lt;img src="/attachments/snake_printing1.jpg" alt="3D Printing" class="float-right" /&gt;)&lt;/p&gt;

&lt;p&gt;The students designed a
snake robot which is made up of 11 segments. Each segment (exept for the
head/tail) contains either a battary, or two servos. Gears were added to
transfer power from the relatively light servos to move the relatively heavy
snake joints. The head segment very appropriately contains the ‘brain’, in the
shape of an Arduino Uno.&lt;/p&gt;

&lt;p&gt;The segmented approach makes it possible to assemble longer and shorter
snakes.&lt;/p&gt;

&lt;p&gt;The segments were 3D printed, and covered with a plastic coating to make them
water proof. None of the DoBots employees had ever used a 3D printer before, so
this project was also a valuable learning experience for us! We’re now
seriously considering using the 3D printer to expand some of the off-the-shelf
robots that we use in our experiments.&lt;/p&gt;

&lt;p&gt;For pictures, videos and and account of the printing-and-assembling process, &lt;a href="http://jelkevandersande.nl/projecten/snakebot-project/"&gt;see the project blog&lt;/a&gt; by one of the students.&lt;/p&gt;

&lt;p&gt;The whole snakebot now cost us around 500 euros, but we’re aiming to bring to
price down even further, so that it becomes financially possible to introduce
(swarms of) snakes in natural areas.&lt;/p&gt;

&lt;h2 id="next-steps"&gt;Next steps&lt;/h2&gt;

&lt;p&gt;&lt;img src="/attachments/snake_printing2.jpg" alt="Snake components" /&gt;
Over the next months,
we will work hard to improve the prototype. We want to make the segments
smaller, so that the robot itself becomes smaller, too. We would like to place
the battery inside the servo-segment, so that there is less variation between
the segments, making mass-production easier.&lt;/p&gt;

&lt;p&gt;Further, our snake still needs sensors. We are currently testing all sorts of
small sensors. In the next version, we’ll decide which sensors we install.&lt;/p&gt;

&lt;p&gt;Also, we wil teach the robot to move in an effective way. We’re researching
reward mechanisms, so that the robot can learn by itself which movements are
useful in which situation.&lt;/p&gt;

&lt;p&gt;Keep an eye on our blog for updates!&lt;/p&gt;

&lt;p&gt;Do you think that working on our snakebot might be an interesting project for
you? We can always use good interns with a background in computer science,
(technical) informatics, electrical or mechanical engineering, so don’t
hesitate to give us a call!&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/07/26/snakebot-project
                <guid>https://dobots.nl/2012/07/26/snakebot-project</guid>
                <pubdate>2012-07-26T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Echo State Networks</title>
                <description>
&lt;h1 id="echo-state-networks"&gt;Echo State Networks&lt;/h1&gt;

&lt;p&gt;Just a quick post for people that are interested in Echo State Networks.&lt;/p&gt;

&lt;p&gt;The past months I spent my time at DoBots to investigate Echo State Networks
(ESNs) as part of my studies Cognitive Neuroscience. ESNs are a type of
recurrent neural network that is easier to train than conventional recurrent
neural networks. Those conventional networks require gradient-based learning
algorithms, such as backpropagation through time (BPTT), and can have problems
with convergence. In order to circumvent this problem, the recurrent
connections are not trained at all in the ESN approach. Instead, the recurrent
neural network is used as a reservoir of non-linear combinations of the input
data, and this reservoir is used to train a simple perceptron output node with
regression. The resulting network gives especially impressive results in
timeseries prediction.&lt;/p&gt;

&lt;p&gt;Here is a scholarpedia link with a better (and short) explanation of the idea
and two good review articles:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href="http://www.scholarpedia.org/article/Echo_state_network"&gt;http://www.scholarpedia.org/article/Echo_state_network&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;M. Lukosevicius and H. Jaeger. Reservoir computing approaches to recurrent neural network training. Computer Science Review, 3(3):127-149, 2009.&lt;/li&gt;
  &lt;li&gt;B. Schrauwen, D. Verstraeten, and J. Van Campenhout. An overview of reservoir computing: theory, applications and implementations. In proceedings of the 15th European Symposium on Arti cial Neural Networks. 2007.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="classification-of-context"&gt;Classification of context&lt;/h2&gt;

&lt;p&gt;I investigated classification with ESNs. Specifically, classification of
environments and abstract context. This can be useful in a couple of
situations. First, you may want to create a robot controller based on a
recurrent neural network, but get stuck at the training procedure. As
explained earlier, an ESN can solve this problem. Another possibility is to
create a behavioral robot controller and use the output nodes of the ESN to
inhibit or enhance specific behaviors, depending on the context. For example,
you could use a small ESN to process battery status and light sensor
information to create a sleeping pattern or simply to make a robot look for
energy when the battery is almost empty. Unfortunately, I did not get around
to implementing the ESNs in an actual robot for demonstration. Nevertheless, I
adapted the basic ESN concept to deal with different timescales, by using a
continuous times reservoir instead of a discrete reservoir. This system can
solve some artificial but non-trivial abstract tasks. The main challenges for
a robot implementation will be the use of continuous input signals instead of
binary input and the timing of the input signals: in my system, the input is
given every 15 timesteps and at the end of every such trial, the output is
read. In a robot, this will probably not be so simple.&lt;/p&gt;

&lt;p&gt;For more detailed information and the matlab code for the ESNs, see
&lt;a href="https://github.com/RemcoTukker/PFC-ESN"&gt;https://github.com/RemcoTukker/PFC-ESN&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For reference, the most common application of ESNs in robots are for motor
control and inverse modelling of robot control. Here’s some articles about
these applications, in chronological order:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;P. Ploger, A. Arghir, T. Gunther, and R. Hosseiny. Echo state networks for mobile robot modeling and control. RoboCup 2003: Robot Soccer World Cup VII, pages 157-168, 2004.&lt;/li&gt;
  &lt;li&gt;M. Salmen and P.G. Ploger. Echo state networks used for motor control. In Robotics and Automation, 2005. ICRA 2005. Proceedings of the 2005 IEEE International Conference on, pages 1953-1958. IEEE, 2005.&lt;/li&gt;
  &lt;li&gt;A.F. Krause, V. Durr, B. Blasing, and T. Schack. Evolutionary optimization of echo state networks: multiple motor pattern learning. 2010.&lt;/li&gt;
  &lt;li&gt;C. Hartland and N. Bredeche. Using echo state networks for robot navigation behavior acquisition. In Robotics and Biomimetics, 2007. ROBIO 2007. IEEE International Conference on, pages 201-206. IEEE, 2007.&lt;/li&gt;
  &lt;li&gt;T. Waegeman, E. Antonelo, F. Wyels, and B. Schrauwen. Modular reservoir computing networks for imitation learning of multiple robot behaviors. In Computational Intelligence in Robotics and Automation (CIRA), 2009 IEEE International Symposium on, pages 27-32. IEEE, 2009.&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>https://dobots.nl/2012/07/06/echo-state-networks
                <guid>https://dobots.nl/2012/07/06/echo-state-networks</guid>
                <pubdate>2012-07-06T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Swarm behavior with a group of small robots</title>
                <description>
&lt;h1 id="part-1-dotty"&gt;Part 1: Dotty&lt;/h1&gt;

&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;

&lt;p&gt;If you have read the &lt;a href="http://www.dobots.nl/blog/-/blogs/building-a-swarm-bot"&gt;“Building a swarm robot”&lt;/a&gt; post, you
already know that we at DoBots are aiming to develop a cheap platform for
swarming experiments. The Dotty is the result of our first attempts. As
mentioned before, we are interested in equipping our small bots with machine
learning algorithms to assist the swarm in fulfilling the given task and
providing the robot with a better understanding of its location and its
characteristics.&lt;/p&gt;

&lt;p&gt;The physical frame which we selected for the Dotty is the &lt;a href="http://www.arexx.com/arexx.php?cmd=goto&amp;amp;cparam=p_robot_chassis"&gt;AREXX frame&lt;/a&gt;, which
contains 2 motors, some experiment space in the form of two breadboard-like
areas for electroniccomponents, and a battery compartment. The motors drive
two wheels, the third is a passive turning wheel.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://www.arexx.com/data/images/dp/robo_chassis.jpg" alt="" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;One of our wishes is to attach a number of different sensors to the robot that
are not optimized individually, but which will be used as a composite by the
algorithms, trying to catch one sensor’s weaknesses with the other one’s
strengths.&lt;/p&gt;

&lt;p&gt;The same applies to the actors, also aiming to enable both communication
between bots as communication with human observers.&lt;/p&gt;

&lt;p&gt;We have used an Arduino board as the brain of our little robot and added
different circuits and sensors to empower the AREXX frame and alter it to a
smarter robot.&lt;/p&gt;

&lt;p&gt;The actual design of the electronics and circuits has already gone through a
number of revisions. As we could not find solutions readily available for all
of our wishes, electronics design became a necessity, and revisiting the
knowledge required essential.&lt;/p&gt;

&lt;p&gt;At the end we aim to have a Dotty equipped with sensors and electronics
like: IR sensor, Microphone, Speaker, Light sensor, Motor sensor, Battery
sensor and Gas sensor.&lt;/p&gt;

&lt;h2 id="light-power--collision-avoidance"&gt;Light-power- collision avoidance&lt;/h2&gt;

&lt;p&gt;One of our first experiments was a “Light-power- obstacle avoidance” scenario
in which the Dotty is equipped with a light sensor and an IR sensor. The IR-
sensor is responsible for the obstacle avoidance task and the light sensor
data is used for adjusting the robot’s speed. As the light level sensed by the
bot is higher, it moves faster.&lt;/p&gt;

&lt;p&gt;Below, you will find the hardware characteristics, a short video and a link to
the code.&lt;/p&gt;

&lt;h2 id="microcontroller-and-additional-electronics"&gt;Microcontroller and additional electronics&lt;/h2&gt;

&lt;p&gt;For this project we used a &lt;a href="http://arduino.cc/en/Main/ArduinoBoardLilyPad"&gt;LilyPad Arduino&lt;/a&gt; board as
microcontroller for the Dotty. We also added a &lt;a href="http://www.sparkfun.com/products/9457"&gt;motor controller&lt;/a&gt; to the AREXX frame, in
order to control two DC motors, for moving forward or backward. The speed of
the motors can be adjusted by the Arduino PWM output.&lt;/p&gt;

&lt;p&gt;&lt;img src="\[$dl-reference=/groups/10157/portlets/20/file-entries/23526/1.0.xml$\]" alt="" /&gt;&lt;img src="\[$dl-reference=/groups/10157/portlets/20/file-entries/23539/1.0.xml$\]" alt="" /&gt;&lt;/p&gt;

&lt;h2 id="the-infrared-sensor"&gt;The Infrared sensor&lt;/h2&gt;

&lt;p&gt;&lt;img src="\[$dl-reference=/groups/10157/portlets/20/file-entries/23554/1.0.xml$\]" alt="" /&gt;&lt;/p&gt;

&lt;p&gt;The Infrared sensor can be bought easily; we got it through
&lt;a href="http://iprototype.nl/products/components/sensors/ir-sensor-4-30"&gt;iprototype.nl&lt;/a&gt;. It has 3 connections, ground, supply voltage and sensor out. It
produces a voltage proportional to the measured obstacle distance.&lt;/p&gt;

&lt;h2 id="the-light-sensor"&gt;The Light sensor&lt;/h2&gt;

&lt;p&gt;A challenge frequently addressed in Artificial Intelligence is the extraction
of periodicity in a series of observations. Figuring out which frequency fits
the observed data, enabling an accurate forecast, is far from trivial,
especially if multiple modulation sources with each their own repeat cycle are
involved. The choice for the light sensor is motivated by this question, as
the cycle of daylight (modulated by season and weather) is an excellent
example of the type of data central to this problem.&lt;/p&gt;

&lt;p&gt;The circuit we use is very simple and very cheap. It contains a single
transistor, a &lt;a href="http://iprototype.nl/products/components/sensors/ldr"&gt;Light Dependent Resistor (LDR)&lt;/a&gt; and two
additional resistors.&lt;/p&gt;

&lt;p&gt;&lt;img src="\[$dl-reference=/groups/10157/portlets/20/file-entries/23462/1.0.xml$\]" alt="" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Schematic of Light sensor circuit for the DoBots Bot One&lt;/em&gt;&lt;/p&gt;

&lt;h2 id="links"&gt;Links:&lt;/h2&gt;

&lt;p&gt;Below, you can see a short video, recorded during the experiment.&lt;/p&gt;

&lt;p&gt;You also can &lt;a href="http://dev.almende.com/svn/coquetry/avoidance/light_powered_obstacle_avoidance_LED.ino"&gt;download the code&lt;/a&gt; written for this experiments from our SVN
repository.&lt;/p&gt;

&lt;h2 id="looking-for-the-light-source"&gt;Looking for the light source&lt;/h2&gt;

&lt;p&gt;Using the same hardware, electronics and sensors (mentioned above), we tried
out another experiment to see if the Dotty can find a source of light by
comparing the received data from the light sensor. Using a simple feedback
algorithm, the received light levels are compared every 0.5 seconds. If the
sensor data indicates that the Dotty is receiving less light than before, it
turns with a random angle and moves in a different direction, searching for a
light source. If the Dotty receives a higher light level than before, it
continues moving forward with the same direction.&lt;/p&gt;

&lt;p&gt;Below you can see how Dotty succeeds in finding the light source (in this
case a desk lamp).&lt;/p&gt;

&lt;p&gt;The &lt;a href="http://dev.almende.com/svn/coquetry/Head2light.ino"&gt;piece of code&lt;/a&gt;
wiritten for this experiment can also be found in our SVN repository.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/06/19/implementing-swarm-behavior-with-a-group-of-small-robots
                <guid>https://dobots.nl/2012/06/19/implementing-swarm-behavior-with-a-group-of-small-robots</guid>
                <pubdate>2012-06-19T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Android and rosjava</title>
                <description>
&lt;h1 id="android-and-rosjava"&gt;Android and rosjava&lt;/h1&gt;

&lt;h2 id="open-source"&gt;Open source&lt;/h2&gt;

&lt;p&gt;&lt;img src="/attachments/rosjava_image.jpg" alt="Random shot of desk" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;coders and employees from Google have joined forces to create rosjava. This is
a pure Java implementation of &lt;a href="http://www.willowgarage.com/pages/software/ros-platform"&gt;ROS&lt;/a&gt; (from Willow Garage). ROS shouldn’t be seen as a solution for
all and everything. So, for what can it be useful? Under other things, it
provides:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;utilities for robots to make it easier to visualize, compress and communicate data&lt;/li&gt;
  &lt;li&gt;basic data processing software&lt;/li&gt;
  &lt;li&gt;drivers for devices of which you might like to address their data (a famous example is the Kinect).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Enough to be really useful! However, there is something that is a little bit a
negative point. ROS might be overwhelming if you just start with your first
robot or Android project. It can be quite a pain to figure everything out. So
here we go with a sort tutorial where we will guide you around (undocumented)
issues that will come up.&lt;/p&gt;

&lt;h2 id="documentation"&gt;Documentation&lt;/h2&gt;

&lt;p&gt;First of all, you definitely need to look at the &lt;a href="http://docs.rosjava.googlecode.com/hg/android_core/html/building.html"&gt;online documentation of the android_core&lt;/a&gt; where you will discover that a lot of documentation online is outdated. The
android core for example does not make use of the “rosmake” utility, but of
the “gradlew” build utility. If you have not installed ROS yet, feel free to
do so. The directions in this blog post will target Ubuntu Precise Penguin
(12.10). You can install using &lt;code&gt;sudo aptitude install ros-fuerte-ros&lt;/code&gt;. The
first thing I have to warn against is installing stuff yourself using
mercurial.&lt;/p&gt;

&lt;h2 id="setup"&gt;Setup&lt;/h2&gt;

&lt;p&gt;Feel free to setup the system how you want of course. I tend to use multiple
Eclipse workspaces to keep code together that belongs together. So, a
&lt;code&gt;~/myworkspace/nonequilibrium&lt;/code&gt; for statistical physics code, a
&lt;code&gt;~/myworkspace/android&lt;/code&gt; for Java code for Android, a &lt;code&gt;~/myworkspace/arduino&lt;/code&gt;
for Arduino related code, a &lt;code&gt;~/myworkspace/nodejs&lt;/code&gt; for node.js code, and
naturally for ROS: mkdir ~/myworkspace/ros &amp;amp;&amp;amp; cd ~/myworkspace/ros&lt;/p&gt;

&lt;p&gt;Please, follow closely the &lt;a href="http://www.ros.org/doc/api/rosinstall/html/rosws_tutorial.html"&gt;rosws
tutorial&lt;/a&gt; to
create the proper ROS workspace:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sudo apt-get install python-pip&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sudo pip install -U rosinstall&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rosws init&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;source setup.bash &lt;/code&gt;&lt;/p&gt;

&lt;p&gt;And copy that line indeed to your &lt;code&gt;~/.bashrc&lt;/code&gt; file as recommended&lt;/p&gt;

&lt;p&gt;Using rosws we can “merge” code from different locations. This will only add
references to the given code or repositories, not the code itself.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rosws merge http://android.rosjava.googlecode.com/hg/.rosinstall&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rosws merge http://rosjava.googlecode.com/hg/.rosinstall&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rosws merge /opt/ros/electric/.rosinstall&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now to get the code actually and check the results:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rosws update&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rosws info&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;# Expected output:  
workspace: /home/anne/myworkspace/ros  
ROS_ROOT: /opt/ros/fuerte/ros  
Localname       &amp;amp; nbsp;        S SCM  Version-Spec UID  (Spec)  URI  (Spec)
[https://...]  
\\--------- &amp;amp;n; bsp;            &amp;amp;n; bsp; - ---- ------------ -----------
\-------------------------  
ros java_core           &amp;amp;nb; sp;   hg   default      7cfb92045e17
code.google.com/p/rosjava  
/opt/ros/electric/stacks &amp;amp;nb; sp;            &amp;amp;nb; sp;            &amp;amp;nb; sp;  
/opt/ros/electric/ros    ;              ;              ;  
google   &amp;amp;nbs; p;            &amp;amp;nbs; p;    hg   default      736ac32a2ede
code.google.com/p/rosjava.google  
android_core  &amp;amp;n; bsp;            hg   default      9327ca89c140
code.google.com/p/rosjava.android&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Pay attention that &lt;code&gt;ROS_ROOT&lt;/code&gt; is not empty. Eventually check everything using
&lt;code&gt;env | grep ROS&lt;/code&gt;. You can also use &lt;code&gt;rospack list-names&lt;/code&gt; to return a list of
all packages.&lt;/p&gt;

&lt;h2 id="hurdle-1-forgetting-to-source-script"&gt;Hurdle 1: Forgetting to source script&lt;/h2&gt;

&lt;p&gt;We will try to build our first code:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;roscd rosjava_core&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;roscd: No such package ‘rosjava_core’&lt;/p&gt;

&lt;p&gt;If an error like this happens and &lt;code&gt;rosjava&lt;/code&gt; is actually in the &lt;code&gt;rosws info&lt;/code&gt;
list, then you will need to call the script in the ROS workspace.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;source setup.sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;roscd rosjava_core&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./gradlew install&lt;/code&gt;&lt;/p&gt;

&lt;h2 id="hurdle-2-refer-to-android"&gt;Hurdle 2: Refer to Android&lt;/h2&gt;

&lt;p&gt;And then similar the other code:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;roscd google&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./gradlew install&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;roscd android_core&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./gradlew debug&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Execution failed for task ‘:android_acm_serial:updateProject’. A problem
occurred starting command ‘android’.&lt;/p&gt;

&lt;p&gt;This fails for me because it cannot find &lt;code&gt;android. &lt;/code&gt;Now you will think that
you only need to update your environmental $PATH, however, it is not just bash
that does not find the android binary, but it is &lt;code&gt;ANT&lt;/code&gt;, the Java build system.
Hence you will need to add this to your ~/.bashrc file:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;export PATH=$PATH:/opt/android-sdk- linux_x86/tools&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;export CLASSPATH=$CLASSPATH:/opt/android-sdk-linux_x86/tools&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sudo killall java &lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Make sure the &lt;code&gt;JVM&lt;/code&gt; is dead. Close all neat Java programs such as Eclipse and
JabRef beforehand. And now run &lt;code&gt;./gradlew clean debug&lt;/code&gt; again.&lt;/p&gt;

&lt;p&gt;On my system it still complains about the fact that a &lt;em&gt;target needs to be
specified&lt;/em&gt;, but for now we will build one project that does not have so many
dependencies:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;./gradlew android_tutorial_pubsub:clean android_tutorial_pubsub:debug&lt;/code&gt;&lt;/p&gt;

&lt;h2 id="hurdle-3-set-android-target"&gt;Hurdle 3: Set Android target&lt;/h2&gt;

&lt;p&gt;There is a problem about the absence of the &lt;code&gt;build.xml&lt;/code&gt; file on building the
files straight from the repository.&lt;/p&gt;

&lt;p&gt;Buildfile: build.xml does not exist!&lt;/p&gt;

&lt;p&gt;The solution is simple:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;cd android_honeycomb_mr2&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;android list targets&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Pick the one that is recent enough for honeycomb, in my case that is number 3.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;android update project --target 3 --path .&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You can pick &lt;code&gt;android_tutorial_teleop&lt;/code&gt; afterwards for example for which you
will need to do the same. I am curious in streaming the camera from my
smartphone to my computer for now: &lt;code&gt;android_tutorial_camera&lt;/code&gt;. Installing on
your smartphone is easy by just connecting your smartphone via USB and after
the build &lt;code&gt;ant installd&lt;/code&gt; (where the d stands for the debug version).&lt;/p&gt;

&lt;h2 id="hurdle-4-display-image"&gt;Hurdle 4: Display image&lt;/h2&gt;

&lt;p&gt;According to the &lt;a href="http://www.ros.org/wiki/image_view"&gt;ROS wiki&lt;/a&gt; image_view
resides apparently in the pipeline package. We also install rxgraph which is a
nice utility to visualize our architecture and last, but not least we will
need to be able to get compressed images (transport plugins):&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sudo aptitude install ros-fuerte-image- common ros-fuerte-image-pipeline&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sudo aptitude install ros-fuerte-rx &lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sudo aptitude install ros-fuerte-image- transport-plugins&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now on your PC start the ROS core and note down your IP address:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;roscore&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sudo ifconfig&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Start the &lt;strong&gt;CameraTutorial app&lt;/strong&gt; on your smartphone and connect to the IP
address you just wrote down, e.g. http://10.10.1.133:11311. You can check
beforehand if you can reach your PC from your smartphone using the browser on
your phone (however, Opera Mobile couldn’t find my server for some reason, so
don’t trust such a check too much). Needless to say, use an IP address and not
some local name only known to your computer. Finally, start the viewer on the
PC side:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rosrun image_view image_view image:=/camera/image compressed&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Done! Proof can be found in the form of the picture (from the camera on my
smartphone) at the top of this blog post obtained by right-clicking in the
image viewer on my PC.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/06/19/android-and-rosjava
                <guid>https://dobots.nl/2012/06/19/android-and-rosjava</guid>
                <pubdate>2012-06-19T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Visual fire detection</title>
                <description>
&lt;h1 id="visual-fire-detection"&gt;Visual fire detection&lt;/h1&gt;

&lt;p&gt;May 22 we had a second sensor test flight.
With this flight, we could test the improved payload modules and this time we did get usable images from the downwards facing camera.
With these images we could try out some visual fire detection algorithms.&lt;/p&gt;

&lt;h2 id="color"&gt;Color&lt;/h2&gt;

&lt;p&gt;The first algorithm simply filters by color.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/visual_fire_detection_color1.png" alt="Fire detected by color at the edges of the fire. The center is so bright that the pixels are over saturated (white)." title="Fire detected by color at the edges of the fire. The center is so bright that the pixels are over saturated (white)." style="width: 800px" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/visual_fire_detection_color2.png" alt="Fire detected by color at the top center (few pixels at the helicopter). The yellow truck (down left) is also marked as fire and thus a false positive." title="Fire detected by color at the top center (few pixels at the helicopter). The yellow truck (down left) is also marked as fire and thus a false positive." style="width: 800px" /&gt;&lt;/p&gt;

&lt;p&gt;The color filter will result in many false positives and we will never be able to trust on color alone.&lt;/p&gt;

&lt;h2 id="flicker"&gt;Flicker&lt;/h2&gt;

&lt;p&gt;The second algorithm tries to find flickering pixels: pixel values that go up and down over time.
The camera recorded images in bursts of 8 images, here is such a burst of a very small fire:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/visual_fire_detection_flicker1.gif" alt="Burst of 8 images of a small fire (at the helicopter)." title="Burst of 8 images of a small fire (at the helicopter)." style="width: 400px" /&gt;&lt;/p&gt;

&lt;p&gt;When we check for flicker and threshold the results, we get:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/visual_fire_detection_flicker2.png" alt="Flicker detection found the fire, but also many false positives." title="Flicker detection found the fire, but also many false positives." style="width: 800px" /&gt;&lt;/p&gt;

&lt;p&gt;The flicker detection also has many false positives.
This is possibly caused by the motion of the plane, resulting in movement of pixels over the image.
The moved pixel can be replaced by a pixel that has very different values.
If this happens more often over the burst, it will be seen as a flickering pixel.&lt;/p&gt;

&lt;h2 id="optic-flow"&gt;Optic flow&lt;/h2&gt;

&lt;p&gt;To be able to compensate for the movement, we need to calculate the optic flow.
We followed the algorithm as described in &lt;a href="http://flohauptic.googlecode.com/svn/trunk/optic_flow/docs/articles/Camus%20-%2097%20-%20Real-Time%20Quantized%20Optical%20Flow.pdf"&gt;Real-Time Quantized Optical Flow&lt;/a&gt;.
This resulted in very noisy flow fields, which was caused by lack of texture in certain parts of the images.
We filtered out those flow vectors by checking the difference of the possible displacements.
If the difference is large enough, we assumed one displacement matched a lot better than the others and thus there must be enough texture.
The result is shown in the image below.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/visual_fire_detection_flow.gif" alt="Calculated optical flow of a big fire. Top left: original image, top center: textureness, top right: optic flow, bottom: optic flow drawn as arrows" title="Calculated optical flow of a big fire. Top left: original image, top center: textureness, top right: optic flow, bottom: optic flow drawn as arrows" style="width: 800px" /&gt;&lt;/p&gt;

&lt;p&gt;There is still some noise in the flow vectors, especially if the exposure time of the camera suddenly changes.
However, once the exposure is under control, the noise will be low enough to use the optic flow to compensate and maybe even find moving smoke or fire.&lt;/p&gt;

&lt;h2 id="motion-compensation"&gt;Motion compensation&lt;/h2&gt;

&lt;p&gt;In the images below you can see results when we compensated for average motion.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/visual_fire_detection_motion.gif" alt="Image burst compensated for average motion." title="Image burst compensated for average motion." style="width: 400px" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/visual_fire_detection_combined.gif" alt="Color and flicker detection on images compensated for motion. This removes a lot of noise for the flicker detection." title="Color and flicker detection on images compensated for motion. This removes a lot of noise for the flicker detection." style="width: 800px" /&gt;&lt;/p&gt;

&lt;p&gt;There are still more features that we can extract from the images, so far the results are promising and we trust that we can detect fire with the color camera.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/06/12/visual-fire-detection
                <guid>https://dobots.nl/2012/06/12/visual-fire-detection</guid>
                <pubdate>2012-06-12T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Robot-to-robot communication</title>
                <description>
&lt;p&gt;&lt;img src="/attachments/replicator_2prototypes_square.png" alt="Replicator robot prototypes" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;In
the near future smartphones will become important as the brains for robots.
How can we best setup phone-to-phone communication? Or in our case, robot-to-
robot communication. We do not only want to have robot-in-the-cloud
functionality, but want to be able to stream sensor data from one robot to the
other. One of the important criteria for us is the availability on
contemporary smartphones. The requirements of the solutions we find should not
be judged on the fact that they support a MIPS or ARM computer architecture,
but on the fact that it is works fine with Android 2.3. Robot-to-robot
communication is especially useful in settings where the robots are physically
close. However, for remote presence applications it would be nice to have
mobile-to-robot communication which is not limited to one (wireless) network.
This opens up possibilities of remote control and remote monitoring. The
ability to bridge the firewall that separates a home or company from the
internet is called &lt;a href="http://en.wikipedia.org/wiki/NAT_traversal"&gt;NAT traversal&lt;/a&gt;, so it will be really
nice if a library or tool makes this easy for us. One of our last requirements
is about what we want to communicate. We want to be able to stream data on the
level and quality as video chat applications. This means that text-based
approaches using JSON or XML per pixel ;-) might not be the best way to go.
Tools that try to achieve some quality of service will have a big plus.
Especially for communicating accelerometer data between users it is important
that this goes very fast. There should be a small delay between a command to a
robot and the action by the robot.&lt;/p&gt;

&lt;h2 id="cross-platform-tools"&gt;Cross-platform tools&lt;/h2&gt;

&lt;p&gt;There are tools available that makes it possible to program for both Android
and iOS at the same time. These tools differ in how much they make use of web
functionality. The extreme is an “app” which is only a browser that displays
pages from an HTML5 website. Such an app does not look like native
applications on your smartphone and has many difficulties in accessing the
sensors on board. As Jonas Lind &lt;a href="http://www.visionmobile.com/blog/2011/06/platform-x-how-cross-platform-tools-can-end-the-os-wars/"&gt;tells us&lt;/a&gt;, it might very well be that the final solution for
this platform “war” will be more important than the platforms themselves. To
us, it seems inevitable to have a solution that can run natively on the
smartphone and has access to many of the sensors, attached devices, and as a
side-effect has a high performance.
&lt;a href="http://www.madewithmarmalade.com/"&gt;Marmalade&lt;/a&gt; and
&lt;a href="http://www.mosync.com/"&gt;Mosync&lt;/a&gt; seem to be such cross-platform APIs.&lt;/p&gt;

&lt;p&gt;We are not so much interested in these cross-platform capabilities as such: we
want to have &lt;em&gt;phone-to-phone streaming&lt;/em&gt; of sensor data. Although, it looks
like there must be a huge market for multi-user peer-to-peer applications,
there does not seem to be many frameworks yet that make it easy for developers
to create multi-user games. The streaming capabilities seem not be up to par.
The newest Mosync API (3.0.1) might have audio streaming now, but there is
nothing in place that really targets peer-to-peer data streaming.&lt;/p&gt;

&lt;h2 id="middleware-solutions-and-standards"&gt;Middleware solutions and standards&lt;/h2&gt;

&lt;p&gt;In home networking there is UPnP (of which e.g.
&lt;a href="http://4thline.org/projects/cling"&gt;Cling&lt;/a&gt; seems to be big) and DLNA which you
will probably recognize from a sticker on your televion set. There do not seem
many libraries for Android for UPnP. One of the most professional looking ones
is &lt;a href="http://www.openhome.org/wiki/OhNet"&gt;ohNet&lt;/a&gt; from
&lt;a href="http://openhome.org/"&gt;OpenHome&lt;/a&gt;. The UPnP technology does not use any
authorisation schemes and is hence only useful in the house itself. That’s why
there is an OpenHome hub to which you can connect to your home and control
everything. Besides looking for multimedia streaming solutions, we can also
look at the parties that implement phone conversation. So, we have to look at
&lt;a href="http://en.wikipedia.org/wiki/Session_Initiation_Protocol"&gt;SIP&lt;/a&gt; (voice) or &lt;a href="http://en.wikipedia.org/wiki/Extensible_Messaging_and_Presence_Protocol"&gt;XMPP&lt;/a&gt; (chat) or extensions on that. There is for
example the &lt;a href="http://sensor.andrew.cmu.edu/xep/sox-xep.html"&gt;SOX- XEP&lt;/a&gt;
extension. It stands for Sensor-Over-XMPP and an ejabberd server will be work
fine. Jingle adds peer-to-peer session control to XMPP. Hence, maybe something
can be cooked up to combine this and create something that works for robot-to-
robot communication, but it will be a lot of effort! There is peerdroid and
&lt;a href="http://code.google.com/p/sip2peer"&gt;sip2peer&lt;/a&gt;, but both projects do not seem
to be backed up by a company or diligent programmers. SIP is a signaling
protocol that comes with a real-time transport protocol. The RTCP is a control
protocol that is needed besides the data transfer protocol (RTP), which is
about quality of service, etc. Audio and video streams have typically separate
RTP sessions (and hence different ports). What’s most interesting to us is
that there is an experimental Control Data Profile for RTP for machine-to-
machine communications. Regretfully, it doesn’t seem anyone picked up on that
either. Then, there is
&lt;a href="http://en.wikipedia.org/wiki/Data_distribution_service"&gt;DDS&lt;/a&gt;, Data
Distribution Service middleware, which seems to be promoted by at least one
company (Twin Oaks) for use on smartphones and which has a
&lt;a href="http://blogs.rti.com/2011/03/02/new-video-data-centric-integration-demo-android/"&gt;demo&lt;/a&gt;. And from a middleware perspective there is
&lt;a href="http://www.zeromq.org/"&gt;zeromq&lt;/a&gt;, which also takes messaging between devices
more seriously, but which is fairly general (and does not have examples of
streaming from &lt;a href="http://www.zeromq.org/build:android"&gt;Android&lt;/a&gt; to iOS for
example). To summarize all these solutions seem to have good ingredients for
what we want, but to stream for example images and acccelerometer data from
phone-to-phone in a nice plug-and-play and reconfigurable manner seems to be
not there yet.&lt;/p&gt;

&lt;h2 id="recommendation"&gt;Recommendation&lt;/h2&gt;

&lt;p&gt;For a roboticist or computer scientist it might now be most natural to roll
your own solution, but we are convinced something will show up that meets our
demands completely in not a very long time. It is most logical to jump the
bandwagon with one of the solutions that solves basically everything except
for the NAT traversal and only requires an additional hub similar to the
OpenHome strategy. Contrary, to the latter though, we would like to pick
something that makes development on the smartphone side extremely easy. One
choice could have been &lt;a href="https://bu.mp/"&gt;bump&lt;/a&gt;, which has an API that can make
a connection on physically bumping devices together. Most promising though
seems to be &lt;a href="https://www.alljoyn.org/"&gt;Alljoyn&lt;/a&gt;, backed up by Qualcomm and
open-source. It works for both Wifi and Bluetooth. You can find the source
code &lt;a href="http://alljoyn.github.com/build.html"&gt;here at github&lt;/a&gt;. Happy coding!&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/06/11/robot-to-robot-communication
                <guid>https://dobots.nl/2012/06/11/robot-to-robot-communication</guid>
                <pubdate>2012-06-11T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Smartphone add-ons</title>
                <description>
&lt;p&gt;&lt;img src="/attachments/romo.jpg" alt="Romo" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;There are more and more add-ons available for smartphones.
We are eagerly awaiting for example the
&lt;a href="http://www.kickstarter.com/projects/peterseid/romo-the-smartphone-robot"&gt;Romo&lt;/a&gt; funded via &lt;a href="http://www.kickstarter.com"&gt;kickstarter.com&lt;/a&gt;. However,
because of European regulations with respect to volume restrictions of
smartphones, the Romo robot had to be redesigned for the European market and
the Romo will arrive somewhere in early June.&lt;/p&gt;

&lt;p&gt;Most of the add-ons are sensors which do not have to create a graphical
interface for the user because now they can hitchhike on the nice AMOLED, etc.
screens of a smartphone which the customer does have anyway already. Such as
for example the &lt;a href="http://www.everistgenomics.com/content/cardiodefender/summary.htm"&gt;Cardio Defender&lt;/a&gt; from Everist Genomics, the
&lt;a href="http://www.zensorium.com/gallery/gallery.html"&gt;Tinké&lt;/a&gt; from Zensorium, the
&lt;a href="http://www.dexcom.com/seven-plus"&gt;Zeven Plus&lt;/a&gt; glucose meter from Dexcom, the
&lt;a href="http://www.valencell.com/"&gt;v-linc&lt;/a&gt; heart rhythm sensor from Valencell, the
&lt;a href="http://www.adidas.com/nl/micoach/"&gt;micoach&lt;/a&gt; smart shoes from Adidas.&lt;/p&gt;

&lt;p&gt;Slowly, however there are are also actuators (things with a motor) attached to
the smartphone as an add-on. There is for example the &lt;a href="http://www.xapprgun.com/"&gt;Xappr gun&lt;/a&gt;, the &lt;a href="http://www.techinasia.com/vibease-pre-order/"&gt;Vibease
vibrator&lt;/a&gt;, the &lt;a href="https://www.cobra.com/detail/cobra-phonetag.cfm"&gt;Cobra key finder&lt;/a&gt; and the
&lt;a href="http://www.bikn.com/index.php"&gt;bikn&lt;/a&gt;, the Panasonic &lt;a href="http://www.cnet.com/8301-13553_1-57400077-32/smartphone-talks-to-panasonic-rice-cooker/"&gt;rice cooker&lt;/a&gt;, several &lt;a href="http://www.lightinthebox.com/unique-design-h-264-wireless-ptz-ip-camera-smartphone-control-msn-server_p238092.html"&gt;PTZ IP cameras&lt;/a&gt;, numerous helicopters, the &lt;a href="http://ardrone.parrot.com/parrot-ar-drone/"&gt;Parrot
quadrocopter&lt;/a&gt;, the
&lt;a href="http://www.gosphero.com/"&gt;sphero&lt;/a&gt;, an underwater robot, the Hydroview (3000
dollar) from Aquabotix, the &lt;a href="http://samuelwilkinson.com/biome/"&gt;Biometerrarium&lt;/a&gt;, the &lt;a href="http://www.orb.com/en/store.html"&gt;Orb music player&lt;/a&gt;, etcetera, etcetera.&lt;/p&gt;

&lt;p&gt;This world of smartphone-enabled devices is booming. A personal computer has
always been, notwithstanding its name, kind of “impersonal”. Your computer
does almost have no idea about you or your wishes and plans of the day.
Smartphones however start to cross this bridge. It becomes possible, at least
from a technical point of view, not to let your phone ring if you are at a
specific location or when you are having a date. It is possible to tell you
which route to take on the highway, to advice you about health issues, to
measure your achievements as a sportsman, etc. However, until now your
smarthphone has not so many options to “reach out” to you in the real world.
Fairly basic things like turning the heat and the lights on, giving water to
the plants, vacuum cleaning are still done by people themselves. Devices
should be able to help us better and better. This will become easier the more
devices are attached to the internet. Hence, smartphone-powered devices are
interesting to us. Not in the first place because they can be turned on by you
and me, but especially because they can then be turned on by our robots. Our
vacuum cleaner can turn on the lights to make it easier to the cleaning, which
is on the moment a far-fetched scenario, but definitely in our reach from a
technological point of view. Most important of all, our robots will need to
know us to be really good personal assistants. The time of impersonal
computers is over…&lt;/p&gt;

&lt;p&gt;So, if you sell or find an interesting device, feel free to contact us about
the possibilities to make it internet-enabled.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/05/13/smartphone-add-ons
                <guid>https://dobots.nl/2012/05/13/smartphone-add-ons</guid>
                <pubdate>2012-05-13T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Building a swarm bot</title>
                <description>
&lt;h1 id="building-circuitry-for-a-small-robot-swarm"&gt;Building circuitry for a small robot swarm&lt;/h1&gt;

&lt;p&gt;The quest for a simple swarm has brought us 10 assembled Arexx frames.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/dotty_arexx.jpg" alt="Arexx frame for the Dotty" /&gt;&lt;/p&gt;

&lt;p&gt;We do not require anything sophisticated. An Arduino board with a tiny microcontroller is already enough. Sparkfun also provides motor controllers, so nothing sophisticated using H-bridges is required anymore. The motor controller handles both DC motors and has a current handling capability of 1 A for each side, more than enough for our bots, that do not typically exceed 70 mA while running.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/arduino1.png" alt="Arduino" /&gt;
&lt;img src="/attachments/arduino2.png" alt="Arduino" /&gt;&lt;/p&gt;

&lt;p&gt;Our basic idea is to include inductive charging platforms to allow the bots to
charge their batteries. We have bought these game-controller battery packs plus charger a number of times, including one of a different form just
for variety.&lt;/p&gt;

&lt;p&gt;To control the movement of the bots we use this (fig 4)&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/induction.jpg" alt="Charging station" /&gt;&lt;/p&gt;

&lt;p&gt;To detect trouble it is useful to know if the motors are running smoothly or
are being blocked, or maybe running without any friction, as this can prompt
the bot to change its behaviour if the current strategy does not yield the
required result.&lt;/p&gt;

&lt;p&gt;After surfing the web to learn which solutions are available to do this (&lt;a href="http://ww1.microchip.com/downloads/en/AppNotes/00894a.pdf"&gt;http://ww1.microchip.com/downloads/en/AppNotes/00894a.pdf&lt;/a&gt;) the following circuit has emerged which primarily excells at being very cheap with parts that everyone that has built any electronics whatsoever probably already owns. If not, Velleman (a Begian firm) has a standard set of parts available in shops and on the internet.&lt;/p&gt;

&lt;p&gt;The diagram is shown here:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/current_sensing_circuit.png" alt="Motor operation circuit" /&gt;&lt;/p&gt;

&lt;p&gt;Here is a picture of the test setup 8-)&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/test_setup.png" alt="Test setup" /&gt;&lt;/p&gt;

&lt;p&gt;As could be expected a circuit that simple did surprise us with a few quirks.
First of which was that the voltage drop observed in the testbed did not
manifest itself in the prototype. The swing, originally from 2.3 to 4.4 Volts,
only showed 3.1 to 4.3 Volts, with a lot of noise present in the sample data.&lt;/p&gt;

&lt;p&gt;A little test with a potentiometer and a reference voltage showed very stable
results, so a more elaborate test setup was built including the motor
controller this time.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/adc_input.png" alt="ADC Input" /&gt;&lt;/p&gt;

&lt;p&gt;This diagram shows the results coming from the adc input of arduino, and the
noise is quite apparent. The lower generic level is measured when the one
motor is blocked.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/04/20/building-a-swarm-bot
                <guid>https://dobots.nl/2012/04/20/building-a-swarm-bot</guid>
                <pubdate>2012-04-20T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>In the spotlight, Linux!</title>
                <description>
&lt;h1 id="fireswarm-internals"&gt;FireSwarm internals&lt;/h1&gt;

&lt;p&gt;&lt;img src="/attachments/tux.jpg" alt="Tux" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;The FireSwarm drones developed
to detect fire as quick as possible, making use of their large numbers, have a
series of sensors and processors on-board. The autopilot from MAVlab - known
from the &lt;a href="http://www.delfly.nl/"&gt;DelFly&lt;/a&gt;, the micro-aerial vehicle lab at the
TUDelft - is a mature solution for navigating airborn vehicles through the
air. This autopilot communicates with a low-power processor from &lt;a href="http://sallandelectronics.nl/"&gt;Salland Electronics&lt;/a&gt; which functions as the hub for
sensors on-board such as a thermopile array, a CO sensor, and a 3MP camera for
streaming hi-res images to the fire brigade (after the fire has been detected
autonomously).&lt;/p&gt;

&lt;p&gt;The autonomous detection of the fire is impossible without a powerful
processor on-board. Our solution uses a small form-factor computer, the so-
called Gumstix, the size of a stick of gum. The Gumstix is often used in the
world of UAVs and uses the OMAP3 SoC (system-on-chip) solution by Texas
Instruments build around a Cortex A8 processor. For example, the Overo Earth
contains only this ARM processor (OMAP3503), while the Overo Water (OMAP3530)
has an ARM plus a GPU, the PowerVR SGX530. By the way, the Overo Water has the
same OMAP as in the well-known Beagleboard.&lt;/p&gt;

&lt;h2 id="linux-33"&gt;Linux 3.3&lt;/h2&gt;

&lt;p&gt;In the latest version of Linux, the Linux core developers came to terms with
the Android developers integrating all kind of software that the latter group
wrote. This wasn’t the only thing. Important for us, the mainline kernel (from
Linus) now supports the Caspa FS camera natively. Hence, we would like to have
in the spotlight this month: &lt;strong&gt;Linux, and especially Laurent Pinchart&lt;/strong&gt;, who
has been a great help to us. To reduce a bit the number of questions that
these people get, we will try to summarize our findings here. Feel free to
contact us for questions to use the Caspa with the newest version of Linux.
The only thing that is missing is board code support for the Caspa module
which you can find at &lt;a href="http://git.linuxtv.org/pinchartl/media.git/commit/3b6af8682bf3c3275d02ae2aa48daf582650c1f7"&gt;pinchart’s git repository&lt;/a&gt; in the form of a patch. This link changes all
the time, search for “board- overo: Add Caspa camera support” if the link is
broken. Laurent told us that the ISP driver by Gumstix is a bit buggy and
limited in features, so it is recommended to use the driver from the mainline
kernel. The main reason we want to use the updated driver is the very
interesting media architecture with which we get &lt;strong&gt;access to the RAW camera
data&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Normally the output of the camera is YUYV while the original input to the
driver is in the form of a Bayer pattern (see
&lt;a href="http://en.wikipedia.org/wiki/Bayer_filter"&gt;wikipedia&lt;/a&gt;). A Bayer pattern has 4
subpixels per pixels of which there is one red, one blue, and two green. It
has been invented by Bayer to mimic the physiology of the human retina which
is more sensitive to green.&lt;/p&gt;

&lt;p&gt;The media framework set up by Laurent makes it possible to make an image
processing pipeline where there is a list of /dev/video* devices with
different output. The pipeline can be configured such that one of these
streams raw images, another monochrome images, and yet another YUYV images.&lt;/p&gt;

&lt;h2 id="get-the-code"&gt;Get the code&lt;/h2&gt;

&lt;p&gt;First of all you will need to get the code of the new kernel. Now the 3.3
kernel is officially out, so you can best get it from the mainline kernel at
&lt;a href="http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=summary"&gt;http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=summary&lt;/a&gt;. However,
we choose back then for Laurent’s repository at git://&lt;a href="http://git.linuxtv.org/pinchartl/media.git"&gt;git.linuxtv.org/pinchartl/media.git&lt;/a&gt; and then
the unstable - &lt;strong&gt;so not recommended&lt;/strong&gt;! - branch: omap3isp-omap3isp-next. I
won’t detail git commands here, you will find out how to clone and checkout
branches easily elsewhere. Do not forget to apply the patch from above! This
patch not included, not even in the omap3isp-omap3isp-next branch. It is most
natural to have the media framework and the drivers in the form of modules of
course, so make sure you install the modules. In the next code section you see
$OVEROTOP for the directory where the cross-compiler resides (build by
OpenEmbedded / bitbake, installable by aptitude install bitbake on Ubuntu),
and $FIRESWARM_KERNEL and $FIRESWARM_KERNEL_MODULES for the root of the build
and the place where the modules will be installed.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export PATH=${OVEROTOP}/tmp/sysroots/ x86_64-linux/usr/armv7a/bin:${PATH}
make ARCH=arm CROSS_COMPILE=arm-angstrom-linux-gnueabi- menuconfig  
make ARCH=arm CROSS_COMPILE=arm-angstrom-linux-gnueabi- uImage modules  
export FIRESWARM_KERNEL_MODULES=${FIRESWARM_KERNEL}/build/modules  
mkdir -p ${FIRESWARM_KERNEL_MODULES}  
make ARCH=arm INSTALL_MOD_PATH=${FIRESWARM_KERNEL_MODULES} modules_install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are .config files included in the kernel (of course). So you will have
to browse the webs for something that is usable. For your convenience I put a
.config here that is build from Sakoman’s 3.2 .config version and an
omap3-overo .config file, plus some changes from ourselves for Linux 3.3:
&lt;a href="documents/10530/0/.config"&gt;.config&lt;/a&gt;. You will of course need to check them,
it might have keyboard/mouse disabled for example. You will benefit from two
utility programs, namely media-ctl and yavta. You can find them at
&lt;a href="http://git.ideasonboard.org"&gt;git.ideasonboard.org&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To cross-compile yavta: &lt;code&gt;make ARCH=arm CROSS_COMPILE=arm-angstrom-linux-gnueabi-&lt;/code&gt; And to compile media-ctl make sure you set the cross-
compiler similarly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export FIRESWARM_MEDIA_CTL=/opt/fireswarm/media-ctl  
mkdir ${FIRESWARM_MEDIA_CTL}/build  
export CC=arm-angstrom-linux- gnueabi-gcc  
export CPP=arm-angstrom-linux-gnueabi-cpp  
ARCH=arm CROSS_COMPILE=arm-angstrom-linux-gnueabi- autoreconf --install  
ARCH=arm CROSS_COMPILE=arm-angstrom-linux-gnueabi- ./configure \
  --with-kernel-headers=${OVEROTOP}/tmp/sysroots/overo-angstrom-linux-gnueabi/kernel \
  --build=arm --host=i686 --prefix=${FIRESWARM_MEDIA_CTL}/build  
ARCH=arm CROSS_COMPILE=arm- angstrom-linux-gnueabi- make  
make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id="in-operation"&gt;In operation&lt;/h2&gt;

&lt;p&gt;Make sure everything is attached nicely and you copied over the image, the
kernel modules, and the yavta and media-ctl utilities to the Gumstix, via
ethernet or via the USB card itself. The camera should be automatically
detected, even if you hotplug it. If not, the only module you need to insert
is the ISP one, by e.g. modprobe omap3-isp. This will automatically load
media.ko, videodev.ko, v4l2-common.ko, mt9v032.ko, and omap-iovmm.ko. Check it
with lsmod for yourself. If you don’t see a &lt;code&gt;/dev/video&lt;/code&gt; device, make sure you
applied the patch of above. Then after you see the &lt;code&gt;/dev/video&lt;/code&gt; devices listed,
it’s time to make work of the media-ctl utility.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./media-ctl -v -l '"mt9v032 3-005c":0-&amp;gt;"OMAP3 ISP CCDC":0[1]'  
./media-ctl -v -l '"OMAP3 ISP CCDC":1-&amp;gt;"OMAP3 ISP CCDC output":0[1]'  
./media-ctl -v -f '"mt9v032 3-005c":0 [SGRBG10 752x480]'  
./media-ctl -v -f '"OMAP3 ISP CCDC":1 [SGRBG10 752x480]'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And use yavta to capture some images:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yavta -p -f SGRBG10 -s 752x480 -n 4 --capture=20 --skip 10 \  
  $(./media-ctl -e "OMAP3 ISP CCDC output") --file=img-#.bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make sure the “capture” number is larger than the “skip” number, or no frames
will actually be captured. The dash # is a mask and will be replaced by
increasing numbers. This media-ctl setting - as you can see - captures raw
GRBG images with 10 bit subpixel values. Be careful in converting this raw
data to images. The 10 bits values are not packed, they are written to 16bit
values. Also, realize that the image sensor is 752x480 subpixels, and not
entire pixels. So raw Bayer gives you only 752x480 16-bit values.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/04/03/in-the-spotlight-linux
                <guid>https://dobots.nl/2012/04/03/in-the-spotlight-linux</guid>
                <pubdate>2012-04-03T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Formation tests with Paparazzi simulator</title>
                <description>
&lt;h1 id="formation-tests-with-paparazzi-simulator"&gt;Formation tests with Paparazzi simulator&lt;/h1&gt;

&lt;p&gt;A swarm of UAVs has to find one or more fires in a dune area by processing the images taken with the cameras directed downwards on the UAVs.
It is a legal requirement that during flight, the UAVs keep a communication link with the ground station.
This can either be a direct link or an indirect link through hopping. When a UAV turns or changes its altitude,
the camera directed downwards shall have a limited view of the area beneath.
Therefore, the amount of pitch and roll changes should by minimised.&lt;/p&gt;

&lt;p&gt;In literature, many methods were found that handle similar problems. Here follows a  selection of the most interesting ones:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Levy flight: flying randomly but with varying time steps&lt;/li&gt;
  &lt;li&gt;Reynolds Boids / Potential fields: through attraction and repulsion between UAVs, several different behaviours can be established, e.g. swarming or formation flight.&lt;/li&gt;
  &lt;li&gt;Pheromones: Using markings in the environment to guide robots, e.g. ant algorithms.&lt;/li&gt;
  &lt;li&gt;Frontier-based: robots look for the frontier between known and unknown space and decide where the trade-off between the cost of going there and the amount of expected new information is best.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to test these pattern formation and swarming algorithms for the swarm of UAVs, the Paparazzi simulator is used.
Paparazzi is an open source autopilot, see the &lt;a href="http://paparazzi.enac.fr/wiki/Main_Page"&gt;Paparazzi Wiki&lt;/a&gt;.
It includes the hardware and software for the autonomous aircraft as well as the complete ground station mission planning and monitoring software.
Here you see an example of the behaviour of 10 UAVs when simulated with Levy flight combined with a repulsive force between UAVs.
In this example no communication restrictions were used. The square is the area to be covered and spans 5 by 5 km.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/formation_tests_repul_nocom.png" alt="Repulsive forces, no communication" title="Repulsive forces, no communication" /&gt;&lt;/p&gt;

&lt;p&gt;In Paparazzi, the easiest way to program the flight schedule for the individual UAVs is to set way points for each UAV.
Then, each autopilot lays down the route to a way point. The carrots are the direction indicators of the UAVs.&lt;/p&gt;

&lt;p&gt;The tested algorithms shall be assessed using the following function:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/formation_tests_formule.png" alt="Evaluation formula" title="Evaluation formula" /&gt;&lt;/p&gt;

&lt;p&gt;Where:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;F is the function to be minimized,&lt;/li&gt;
  &lt;li&gt;t is the flight time in which the swarm has covered 90% of the area,&lt;/li&gt;
  &lt;li&gt;N is the number of UAVs,&lt;/li&gt;
  &lt;li&gt;Dcomm is a discount for having no communication link,&lt;/li&gt;
  &lt;li&gt;Wcomm is a weight factor,&lt;/li&gt;
  &lt;li&gt;Dturn is a discount for large pitch and roll,&lt;/li&gt;
  &lt;li&gt;Wturn is a weight factor.&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>https://dobots.nl/2012/04/03/formation-test-with-paparazzi-simulator
                <guid>https://dobots.nl/2012/04/03/formation-test-with-paparazzi-simulator</guid>
                <pubdate>2012-04-03T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Self-organisation Literature</title>
                <description>
&lt;h1 id="self-organization-literature"&gt;Self-organization literature&lt;/h1&gt;

&lt;p&gt;This list contains a series of papers which we consider as tightly related to
the concept of &lt;strong&gt;self-organization&lt;/strong&gt;. DoBots is a spin-off of
&lt;a href="http://www.almende.com"&gt;Almende&lt;/a&gt;, a company that performs research on this
specific topic and DoBots is moderately “infected” by it. :-) Reading the
following literature might bring you a bit closer to the general thought
package of Almende, which was originally inspired by the &lt;a href="http://www.santafe.edu"&gt;Santa Fé
Institute&lt;/a&gt; on the other side of the ocean. We will try
to say in two or three lines why we think a specific paper is interesting. We
add (pdf) links, but please, use google scholar if the links are outdated.&lt;/p&gt;

&lt;h2 id="embodied-intelligence"&gt;Embodied intelligence&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;New robotics: design principles for intelligent systems [Pfeifer, Iida,
Bongard], 2003 (&lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.236&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;pdf&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Brooks creating his purely reactive agents, said things like “intelligence
without representation” and “intelligence without reason”, or more eloquently:
“the world is its own model.” Very concrete: if a robot changes something in
the world, it does not need to remember it as such, it can just look it up.
The authors come up with a developmental paradigm to solve the problem of how
a robot even starts to know its own body. In a simulation “block pushers” are
evolved in which body morphology and brain are coevolved using gene regulatory
networks.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Developmental robotics: a survey [Lungarella, Metta, Pfeifer, Sandini], 2003 (&lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.83.7615&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;pdf&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;From unknown sensors and actuators to actions grounded in sensorimotor perceptions [Ollson, Nehaniv, Polani], 2006 (&lt;a href="http://homepages.feis.herts.ac.uk/~comqdp1/publications/files/olssonl_cs.pdf"&gt;pdf&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="empowerment"&gt;Empowerment&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Empowerment: a universal agent-centric measure of control [Klyubin, Polani, Nehaniv], 2005 (&lt;a href="http://homepages.feis.herts.ac.uk/~comqdp1/publications/files/cec2005_klyubin_polani_nehaniv.pdf"&gt;pdf&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Empowerment is an attempt to define in an information theoretic sense the
concept of autonomy. It is the amount of information an agent can potentially
inject into the environment via its actuators and _capture later _via its
sensors. The principle of empowerment maximization has an agent act in such
way, that its perception later is the richest experience imaginable. This is
self-organization in a sensorimotor space.&lt;/p&gt;

&lt;p&gt;See also:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Predictive information and explorative behavior of autonomous robots [Ay, Bertschinger, Der, Güttler, Olbrich], 2008 (&lt;a href="http://www.santafe.edu/media/workingpapers/08-02-006.pdf"&gt;pdf&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Information theory of decisions and actions [Tishby, Polani] (&lt;a href="http://eprints.pascal-network.org/archive/00005915/01/IT-PAC.pdf"&gt;pdf&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="language-grounding"&gt;Language grounding&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Origins of communication in evolving robots [Marocco, Nolfi], 2006 (&lt;a href="http://gral.ip.rm.cnr.it/Pubblicazioni/English/Book%20Chapters/marocco_sab9.pdf"&gt;pdf&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The authors simulate four robots that undergo (neuro)evolution where they - in
the end - distribute themselves over two target areas in the arena. Their
output neurons in a neural networks are connected to communication sensors.
The paper is interesting because the authors show how the communication
patterns evolve over generations. It shows not only that motor behaviour can
co-evolve with communication skills, but also how the latter improve over
time. First the robots can only communicate that they already occupy an area,
later on they can communicate with each other such that they can distinguish
between one or multiple robots in a given area. Most remarkable, they really
come up with this representation entirely by themselves!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Evolving communication without dedicated communication channels [Quinn], 2001 (&lt;a href="http://www.isrl.illinois.edu/~amag/langev/localcopy/pdf/quinn01evolvingCommunication.pdf"&gt;pdf&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="contraction-theory"&gt;Contraction theory&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;A study of synchronization and group cooperation using partial contraction theory [Slotine, Wang], 2005 (&lt;a href="http://web.mit.edu/people/wangwei/CooperativeControl.pdf"&gt;pdf&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Contraction theory tells you something that is very important to synthesize
systems. If two systems are contracting, then the system composed out of these
two subsystems will also be contracting. In other words, contraction theory
studies convergence between two arbitrary system trajectories. Partial
contraction theory extends this and describes convergence to something beyond
a unique trajectory, for example a manifold. This might not fall under the
conventional concept of self-organization, but the ability to say something
reasonable about systems connected to each other by arbitrary feedback loops,
is extremely valuable.&lt;/p&gt;

&lt;h2 id="associative-learning"&gt;Associative learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Cognitive robotics, enactive perception, and learning in the real world [Morse, Ziemke], 2007 (&lt;a href="http://csjarchive.cogsci.rpi.edu/Proceedings/2007/docs/p485.pdf"&gt;pdf&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This paper and the one called “dynamic liquid association” introduce a new
type of artificial intelligence called &lt;em&gt;reservoir computing&lt;/em&gt;. It makes use of
a static reservoir of neurons with fading activity. The latter is enforced by
scaling the network weight till the spectral radius is small enough.
Subsequently a simple network is used to read-out the network activity. This
paper is remarkable in the sense that it uses an old technique for that,
adaptive resonance theory (Grossberg). A simple auto-associative memory like
Kohonen networks which is worth to look at it for its own reasons.&lt;/p&gt;

&lt;h2 id="polychronization"&gt;Polychronization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Polychronization: computation with spikes [Izhikevich], 2006 (&lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.125.5672&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;pdf&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Random networks can produce time-locked patterns that are &lt;em&gt;not&lt;/em&gt; synchronous.
When delays exist between neurons specific patterns emerge when input is
provided. Simplifying, suppose a post-synaptic neuron gets activitated if
&lt;em&gt;two&lt;/em&gt; pre-synaptic neurons fire, when their spikes _after _two different
delays through the synaptic cleft, arrives at their destination. It is not
important when the neurons fire, but when their activity comes together at the
same time! In this way networks of neurons are formed. Very interesting, the
number of such polychronous groups far exceed the number of neurons.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Delay learning and polychronization for reservoir computing [Paugam-Moisy, Martinez], 2008 (&lt;a href="http://liris.cnrs.fr/Documents/Liris-3399.pdf"&gt;pdf&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>https://dobots.nl/2012/03/20/self-organisation-literature
                <guid>https://dobots.nl/2012/03/20/self-organisation-literature</guid>
                <pubdate>2012-03-20T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Barbecue test for FireSwarm</title>
                <description>
&lt;h1 id="barbecue-test"&gt;Barbecue test&lt;/h1&gt;

&lt;p&gt;To test the camera that will be on board the UAV, we lit up a barbecue in the garden of Almende and took some pictures.
This was a good opportunity to tweak camera settings, test startup scripts and of course: see how well we can recognize fire.&lt;/p&gt;

&lt;p&gt;The box you see on the picture will also be the box that is going up in the air for the sensor test flight.
In that box the breakout board (Summit) will be replaced by a board made by Salland Electronics,
on which more sensors will be connected (a thermopile array, a 3M pixel camera and a gas sensor).&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/barbecue_test_1.JPG" alt="Barbecue" title="Barbecue" style="width: 500px" /&gt;
&lt;img src="/attachments/barbecue_test_2.JPG" alt="Box top" title="Box top" style="width: 500px" /&gt;&lt;/p&gt;

&lt;p&gt;In order to simulate the movement of the plane,
the camera was rotating constantly from left to right by a simple construction of lego.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/barbecue_test_3.JPG" alt="Box front" title="Box front" style="width: 500px" /&gt;
&lt;img src="/attachments/barbecue_test_4.JPG" alt="Box back" title="Box back" style="width: 500px" /&gt;&lt;/p&gt;

&lt;p&gt;This was a nice test to see how sensitive the camera is for hot objects.
When you combine a picture taken with an IR pass filter and a picture taken with an IR block filter,
you can already distinguish the barbecue.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/barbecue_test_5.png" alt="Barbecue color" title="Barbecue color" style="width: 500px" /&gt;
&lt;img src="/attachments/barbecue_test_6.png" alt="Barbecue near infrared" title="Barbecue near infrared" style="width: 500px" /&gt;&lt;/p&gt;

&lt;p&gt;With these sample images we can already test out some fire detection algorithms.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/03/09/barbecue-test
                <guid>https://dobots.nl/2012/03/09/barbecue-test</guid>
                <pubdate>2012-03-09T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>SLAM (Simultaneous Localization And Mapping)</title>
                <description>
&lt;h1 id="what-is-this-slam"&gt;What is this SLAM?&lt;/h1&gt;

&lt;p&gt;SLAM (Simultaneous Localization And Mapping) tries to answer two key questions
in Robotics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Robot Localization: “Where is the Robot?”&lt;/li&gt;
  &lt;li&gt;Robot Mapping: “What does the world around the Robot look like?”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SLAM can help you solve the problem of building up a map within an unknown
environment (without &lt;em&gt;a priori&lt;/em&gt; knowledge), or to update a map within a known
environment (with &lt;em&gt;a priori&lt;/em&gt; knowledge from a given map), while at the same
time keeping track of the current location of the robots in the environment.&lt;/p&gt;

&lt;h2 id="some-possible-slam-applications"&gt;Some possible SLAM applications&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Oil pipeline inspection&lt;/li&gt;
  &lt;li&gt;Ocean surveying and underwater navigation&lt;/li&gt;
  &lt;li&gt;Mine exploration&lt;/li&gt;
  &lt;li&gt;Coral reef inspection&lt;/li&gt;
  &lt;li&gt;Military applications&lt;/li&gt;
  &lt;li&gt;Crime scene investigation&lt;/li&gt;
  &lt;li&gt;Earthquake surveillance procedures&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="requirements"&gt;Requirements&lt;/h2&gt;

&lt;p&gt;In order to implement SLAM, you need a mobile robot and a range measurement
device, such as a laser scanner, a sonar device or a camera (visual SLAM).&lt;/p&gt;

&lt;h2 id="how-to-implement-slam"&gt;How to implement SLAM&lt;/h2&gt;

&lt;p&gt;The goal of the SLAM process is to use the data obtained by the robot’s
ranging sensors in order to update the position of the robot and the map of
the world around it. A very basic localisation process consists of a numbers
of steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Move&lt;/li&gt;
  &lt;li&gt;Estimate position (&lt;a href="http://en.wikipedia.org/wiki/Odometry"&gt;odometry&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Sense features&lt;/li&gt;
  &lt;li&gt;Map and localization update&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is indeed a naive approach, because robot odomety of an unknown
environment is very error-prone and the result of such an approach would be an
inconsistent map. Therefore as an corrected version, SLAM uses a probabilistic
approach consisting of the following steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Move&lt;/li&gt;
  &lt;li&gt;Predict position (odometry)&lt;/li&gt;
  &lt;li&gt;Sense features&lt;/li&gt;
  &lt;li&gt;Recognize landmarks (data association) ⇒ loop closure&lt;/li&gt;
  &lt;li&gt;Correct position (probability theory)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this new approach, the ranging sensor data and odometry data are combined
to correct the perception of the robot about its location and the position of
environmental feature.&lt;/p&gt;

&lt;p&gt;This prediction and correction process is described with the aid of some
figures below:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/slam1.png" alt="Robot odometry: prediction of location and correction" /&gt;&lt;/p&gt;

&lt;p&gt;In the figures, the robot is represented by the triangle. The stars represent
landmarks (environmental features). The robot initially measures the location
of the landmarks, using its sensors (sensor measurements illustrated by the ‘lightning’).&lt;/p&gt;

&lt;p&gt;The robot moves. Based on robot odometry, the robot thinks that is it located
here.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/slam2.png" alt="Robot odometry" /&gt;&lt;/p&gt;

&lt;p&gt;Once again the robot measures its distance to the landmarks using its range
measurement sensors. What if the odometry and sensor measurements don’t match?
The answer is: The robot is not at the location where it thinks it is
(odometry error).&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/slam3.png" alt="Robot odometry error" /&gt;&lt;/p&gt;

&lt;p&gt;The robot has more trust in sensor data than in odometry. So, the new location
of the robot considering the landmarks distances is here. (The dashed triangle
is where the robot thought it was using the odometry data).&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/slam4.png" alt="Robot odometry" /&gt;&lt;/p&gt;

&lt;p&gt;The actual location of the robot is here. You can see that the sensors are not
perfect but their measurements are more reliable than odometry. The lined
triangle is the actual position of the robot, the dotted triangle is its
estimated location based on the sensor data and the dashed triangle is its
estimated location based on odometry.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/slam5.png" alt="Robot odometry and sensor data" /&gt;&lt;/p&gt;

&lt;h2 id="visual-slam"&gt;Visual SLAM&lt;/h2&gt;

&lt;p&gt;At DoBots we are mostly interested in projects involving Visual SLAM, where
the range measurement sensor is simply a camera. Using camera as range
measurement device has some advantages such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It is fast.&lt;/li&gt;
  &lt;li&gt;It has longer range than many other sensors.&lt;/li&gt;
  &lt;li&gt;The sensor data contains more information about the environment rather than the distances.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The basic procedure of Visual SLAM is described in the following figures:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/visual_slam1.png" alt="Visual SLAM" /&gt;&lt;/p&gt;

&lt;p&gt;The camera localization &lt;script type="math/tex"&gt;y_i^C&lt;/script&gt; depends on three parameters: camera orientation
and camera position (which are given by odometry) and the range measurements
(which is provided by the camera). The yiW&lt;/p&gt;

&lt;p&gt;is the camera distance from the landscape from measurement &lt;script type="math/tex"&gt;z_i&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id="predictor-corrector"&gt;Predictor-Corrector&lt;/h3&gt;

&lt;p&gt;At any moment(t) the robot localization can be predicted by the robot location
at(t-1), the robot orientation and the robot position. This prediction will be
corrected by the aid of range measurements as it is shown in below figure:&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/visual_slam2.png" alt="Visual SLAM" /&gt;&lt;/p&gt;

&lt;p&gt;Three stages of the SLAM(Prediction, Observartion and Update) need to be done
based on a recursive algorithm which can derive the Robot state from the noisy
sensor data. One the best known algorithms which is implemented to solve the
SLAM problem is &lt;a href="http://en.wikipedia.org/wiki/Kalman_filter"&gt;Kalman filter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the following links you can see some SLAM projects realization, done by
different research groups.&lt;/p&gt;

&lt;h2 id="videos"&gt;Videos&lt;/h2&gt;

&lt;p&gt;The following YouTube videos give a nice insight into using visual SLAM:&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/bq5HZzGF3vQ?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/gNxmgQ2gofE?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/geQOxFiOj_0?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;h2 id="references"&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href="http://www.cs.bris.ac.uk/Research/Vision/Realtime/bmvctutorial/bmvcVslamTut07ptI.pdf"&gt;Visual SLAM (2007), Andrew Davison, Andrew Calway, Walterio Mayol, BMVC 2007 Visual SLAM Tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://www.doc.ic.ac.uk/~ajd/Robotics/RoboticsResources/SLAMTutorial1.pdf"&gt;Simultaneous Localization and Mapping: Part 1(2006), Hugh Durrant-Whyte and Tim Baile, IEEE Robotics &amp;amp; Automation Magazine, P 99-108.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://www.cse.yorku.ca/~hogue/qual_slides.pdf"&gt;Simultaneous Localizatioand Mapping – SLAM (2005), Andrew Hogu, York University, Toronto, Ontario&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://ocw.mit.edu/courses/aeronautics-and-astronautics/16-412j-cognitive-robotics-spring-2005/projects/1aslam_blas_repo.pdf"&gt;SLAM for Dummies: A Tutorial Approach to Simultaneous Localization and Mapping, Søren Riisgaard and Morten Rufus Blas.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>https://dobots.nl/2012/02/28/a-brief-story-of-slam
                <guid>https://dobots.nl/2012/02/28/a-brief-story-of-slam</guid>
                <pubdate>2012-02-28T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>The renormalization group on networks, an introduction</title>
                <description>
&lt;h1 id="the-renormalization-group-on-networks"&gt;The renormalization group on networks&lt;/h1&gt;

&lt;p&gt;Modern physics is actually one big quest to solve the many-body problem. Most
often stated in soundbites invented by famous people from for example the
influential Santa Fé school, but sometimes as old as Aristotle, such as “the
whole is greater than the sum of the parts”. In physics this often came to
mean: the whole is the sum of parts plus all their interactions. These
interactions can sometimes be neatly organised in the form of Feynmann
diagrams, the mysterious drawings you might sometimes see on the cover of
physics books.&lt;/p&gt;

&lt;h2 id="blocks"&gt;Blocks&lt;/h2&gt;

&lt;p&gt;Not exactly a new kid on the block, we are somewhere in the 50s-70s, group
renormalization became a tool in particle physics to cope with the many
possible interactions between elementary particles. First of all, the
physicists borrowed from the old idea of considering a system at different
scales. Think of the different resolutions of for example Google maps.
Secondly, the physicist gave up on the idea that a system could be described
by a finite number of interactions, but instead of that realised that an
infinite number of interactions would not be such a big problem if the
relation of the “interaction landscape” (my words) on one scale would have a
simple reation with the “interaction landscape” on another scale. The example
often used in statistical physics is that of the Ising model which is a 2D
grid with so-called spins, that can point upwards or downwards. The spins
influence their neighbours: a spin aligns its orientation according to the
majority of its neighbours. By considering the larger scale of so-called block
spins, we take blocks of say 3x3 spins and assign a spin direction to this
larger block. Now, we have to come up with a description for the interactions
between the block spins on this higher level. Not only that, but these
interactions should also somehow be a coarse grained description of the
interactions on the lower level. The idea of group renormalisation is that by
repeating this pattern we end up with a flow from one “interaction landscape”
to another: a flow through the space of coupling constants. We now miss one
important piece of the puzzle to make sense of all this. This “flow” can
namely be very chaotic. However, there is one specific phenomenon that is
deeply related to the concept of scale invariance, which is the second order
phase transition. A second order phase transition is a critical phenomenon
that makes a system transition from one state to another in a very abrupt
manner. Think earthquakes, crackling noise, etc. A system that is “poised at
this edge of chaos” corresponds to a group renormalisation flow that is
directed towards a fixed point! We suddenly have a way to describe the global
behaviour - through a huge series of scales - from local interactions!&lt;/p&gt;

&lt;h2 id="network"&gt;Network&lt;/h2&gt;

&lt;p&gt;&lt;img src="/attachments/renormalization_newman_and_watts.jpg" alt="Renormalization on networks" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;This is all
very well, but nothing makes much sense without examples. We can give an
example of the Ising model, or of self-organised criticality with sandpiles
(Bak), but for swarm robotics, we are most interested in graphs. We have one
remark here, most graph-based research disregards the spatial embedding of
this graph in the real world, however, in our case of a network of robots,
this is instead one of the most interesting and challenging future directions
of “complex systems” research. The scientific world has been talking the last
decade about networks with the so-called small world property. Imagine a
network of people, where people correspond to the nodes on a graph and people
that know each other are represented by edges. Imagine these nodes on a
circle, and organise them a bit and you will see a structure as in Fig. 1
(from Newman and Watts publicly available paper [1]). The hypothesis is that
this small world property is a critial phenomenon, and hence group
renormalization can be applied.&lt;/p&gt;

&lt;h2 id="parameters-l-k-and-p"&gt;Parameters L, k and p&lt;/h2&gt;

&lt;p&gt;We just summarize Newman and Watts findings here… The authors want to show
that if the number of “shortcuts” goes to zero, the system exhibits a second-
order phase transition observing the parameter &lt;script type="math/tex"&gt;l&lt;/script&gt;, the average distance between
vertices. Besides the system size &lt;script type="math/tex"&gt;L&lt;/script&gt;, and a parameter &lt;script type="math/tex"&gt;k&lt;/script&gt; which stands for some
fixed range for which every node is connected to all its neighbours, there is
a random parameter &lt;script type="math/tex"&gt;p&lt;/script&gt;. This &lt;script type="math/tex"&gt;p&lt;/script&gt; stands for a rewiring probability for every &lt;script type="math/tex"&gt;kL&lt;/script&gt;
connection (and is “quenched”, that means, it is only done once). The
renormalization procedure can be divided into two different cases, which is
very typical for the procedure. For &lt;script type="math/tex"&gt;k=1&lt;/script&gt;, their blocking procedure takes two
subsequent vertices on the ring and replaces them by one vertex. A “shortcut”
to one of the vertices is preserved and will now point to the new vertex. The
number of vertices &lt;script type="math/tex"&gt;L&lt;/script&gt; becomes &lt;script type="math/tex"&gt;L/2&lt;/script&gt;. The number of edges goes from &lt;script type="math/tex"&gt;L&lt;/script&gt; to &lt;script type="math/tex"&gt;L/2&lt;/script&gt; and
hence the probability to pick a given edge goes from &lt;script type="math/tex"&gt;p&lt;/script&gt; to &lt;script type="math/tex"&gt;2p&lt;/script&gt;. For the case of
&lt;script type="math/tex"&gt;k&gt;1&lt;/script&gt;, the system size goes from &lt;script type="math/tex"&gt;L&lt;/script&gt; to &lt;script type="math/tex"&gt;L/k&lt;/script&gt;, the probability from &lt;script type="math/tex"&gt;p&lt;/script&gt; to &lt;script type="math/tex"&gt;k^2 p&lt;/script&gt;, the
range parameter goes from &lt;script type="math/tex"&gt;k&lt;/script&gt; to &lt;script type="math/tex"&gt;1&lt;/script&gt;, and the mean distance stays the same. Hence,
every system with &lt;script type="math/tex"&gt;k&gt;1&lt;/script&gt; can be converted in a system with &lt;script type="math/tex"&gt;k=1&lt;/script&gt; and then a group
renormalisation procedure can be applied over and over again.&lt;/p&gt;

&lt;h2 id="the-transition"&gt;The transition&lt;/h2&gt;

&lt;p&gt;&lt;img src="/attachments/renormalization_newman_and_watts_results.jpg" alt="Renormalization on networks, results" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;The authors write down an expression for &lt;script type="math/tex"&gt;l&lt;/script&gt;, the
average distance between vertices. Let us not repeat the exact form of this
equation, but explain between which regimes the mentioned transition would
take place. If a system is really small, then for a given fixed &lt;script type="math/tex"&gt;p&lt;/script&gt;, there is on
average less than one shortcut in the graph. This means that the average
distance between vertices just happens over the ring and hence scales linearly
with the system size &lt;script type="math/tex"&gt;L&lt;/script&gt;. However, when the system becomes larger, and &lt;script type="math/tex"&gt;p&lt;/script&gt; is
still fixed, in the end shortcuts will show up, and the distance between
vertices will scale logarithmic with respect to &lt;script type="math/tex"&gt;L&lt;/script&gt;. The transition between
these two regimes is the one that interests us.&lt;/p&gt;

&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In Fig. 2 you can see that the authors indeed were able to show this continous
phase transition in the system. So, what do we have here!? We have a relation
between system size and the famous average number of “degrees of separation”
between nodes on a network. Now, you would be able to be all cocky on parties
and tell fellow nerds that it’s not just six degrees of separation between you
and some famous person, but that this depends on &lt;script type="math/tex"&gt;L&lt;/script&gt;, &lt;script type="math/tex"&gt;p&lt;/script&gt; and &lt;script type="math/tex"&gt;k&lt;/script&gt;: the system size,
the number of shortcuts, and the range of neighbours someone has.&lt;/p&gt;

&lt;p&gt;This text introduced the renormalization group in networks. Interesting would be take these results and check how it can be applied to swarm robotics!&lt;/p&gt;

&lt;p&gt;[1] Renormalization group analysis of the small-world network model, 2009, T. Payne&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/02/27/the-renormalization-group
                <guid>https://dobots.nl/2012/02/27/the-renormalization-group</guid>
                <pubdate>2012-02-27T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Reprogrammable Remote Control dongles</title>
                <description>
&lt;h1 id="reprogrammable-remote-control-dongles"&gt;Reprogrammable Remote Control dongles&lt;/h1&gt;

&lt;p&gt;&lt;img src="/attachments/toybien_dongle.jpg" alt="Toybien dongle" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;Interested as we are in swarming, our efforts include building sort of an army
of moving bots. To keep cost down the available toys are our first port-of-
call. We now have a collection consisting of flying (helium-filled
zeppelin/balloon), driving (a few cars and robots) and swimming (submarines)
units that are all remote controlled. Most of them use 27 MHz based RF, a few
are on 49MHz, or even 2.4GHz (BT/Wifi/etc.). IR is also a frequently chosen
solution.&lt;/p&gt;

&lt;p&gt;To get communication up with all of them, enabling our software to become part
of the swarm, we need a more or less &lt;strong&gt;universal solution&lt;/strong&gt; connecting sensors
and actors.&lt;/p&gt;

&lt;p&gt;On the market there are a number of dongle suppliers that use the headphone
connector to add an IR or RF extension to Android (and/or iPhone) smartphones.
What makes life difficult is that the toys mostly use very cheap encoding
solutions based on chips like RX2B / TX2B  which are inflexible and difficult
to combine with other coding options, either single chip based or not.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;So, what is our master plan!? Make a dongle - or have a dongle made - that enables reprogramming it through the headphone connection!&lt;/strong&gt; It would allow us to combine the advantage of off-loading the encoding to a dedicated chip (a PIC166XX comes to mind) and still retaining the flexibility for application programmers to adapt to the required protocol.&lt;/p&gt;

&lt;h2 id="joybien"&gt;Joybien&lt;/h2&gt;

&lt;p&gt;In the picture you see the JoyXtix dongle from Joybien [1]. There are three
ways in which this friendly manufacturer of dongles, based in Taiwan, can open
up their API for application developers on Android or iPhone:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Provide a &lt;strong&gt;limited API&lt;/strong&gt; with say 6 till 10 commands for toys for which Joybien reverse engineered the wireless protocol. This allows Android phone developers to create their own apps for the toys that Joybien did target, under which are a speedcar sold also by Joybien and for example the Air Swimmer blimps. Taping a smartphone to this blimp and staying away for more than 5 meters from the others, would allow us to cover an area from the air in an extremely simple way. And many more applications are possible. Joybien seems to be willing to provide this API, good news!&lt;/li&gt;
  &lt;li&gt;Provide a complicated API with many commands to adjust modulation, ASK, FSK, timing, etc. etc. for Android/iPhone app developers. No, we are not gonna recommend that! Instead, we would recommend a more or less &lt;strong&gt;straight translation&lt;/strong&gt; from the analog values over audio towards the dongle to the actual wireless domain. So, for example a 27 kHz modulation becomes a 27 MHz modulation but most of the signal will be preserved.&lt;/li&gt;
  &lt;li&gt;Provide a &lt;strong&gt;programmable device&lt;/strong&gt; (the solution of above) that takes input from the headphone output and translates it into hex code for the microcontroller in the dongle. Instead of programming over serial, or over USB, or over Bluetooth, we program this time over audio! This of course is of much more use than only the “dongle industry”. Any device attached to a smartphone (via the headphone connector) might profit from reprogrammability by users, which can be over-the-air and even come with its “own app store for smartphone add-ons”.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We had the idea first. :-)&lt;/p&gt;

&lt;p&gt;[1] &lt;a href="http://www.joybien.com/product/Product_JoyXtix.html"&gt;http://www.joybien.com/product/Product_JoyXtix.html&lt;/a&gt;&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/02/20/reprogrammable-remote-control-dongles
                <guid>https://dobots.nl/2012/02/20/reprogrammable-remote-control-dongles</guid>
                <pubdate>2012-02-20T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>NXT and ROS</title>
                <description>
&lt;h1 id="ros"&gt;ROS&lt;/h1&gt;

&lt;p&gt;&lt;img src="/attachments/lego_mindstorms_nxt_disassembled.jpg" alt="Lego Mindstorms NXT disassembled" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;ROS, Robot Operating System, is more and more used in research organisations
to quickly be able to connect to robots. This is a very down to earth blog
post which explains you on how to install ROS on Ubuntu (11.10) and how
subsequently to connect to the Lego Mindstorms NXT robot.&lt;/p&gt;

&lt;p&gt;Make sure your computer recognizes the Lego Mindstorm NXT brick, this will always be the same procedure for all USB devices on Ubuntu. For Lego you can check it’s udev instructions at &lt;a href="http://www.ros.org/wiki/nxt/Installation"&gt;http://www.ros.org/wiki/nxt/Installation&lt;/a&gt;. The installation procedure of ROS is pretty standard and can be found at &lt;a href="http://www.ros.org/wiki/electric/Installation/Ubuntu"&gt;http://www.ros.org/wiki/electric/Installation/Ubuntu&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You first need to update a file in /etc/apt/sources.list.d that references the proper
Ubuntu repository. After adding a key and an apt-get update you can install
the most recent version of ROS. The version “fuerte” is already available, but
official release is in April 2012, so stick with “electric” for now. The ROS
install is pretty heavy, make sure your remaining diskspace is plenty (&amp;gt; 1GB) by &lt;code&gt;df -h&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id="ros--nxt"&gt;ROS &amp;amp; NXT&lt;/h2&gt;

&lt;p&gt;The rosinstall utility takes care of installing ROS modules. You can change the installation directories that are used by rosinstall through changing the file &lt;code&gt;/opt/ros/electric/setup.sh&lt;/code&gt;. Installation of the NXT module goes like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rosinstall ~/myworkspace/ros/nxtros /opt/ros/electric \
"http://www.ros.org/wiki/nxt/Installation?action=AttachFile&amp;amp;do;=get&amp;amp;target;=nxt-0.1.0.rosinstall"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Under the hood this basically does something like this for you (cloning a mercurial repository):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hg clone https://nxt.foote-ros-pkg.googlecode.com/hg --insecure nxt-ros
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However! You can now also just get the stuff from the Ubuntu repository directly, very easy:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;aptitude search ros | grep -i nxt
sudo aptitude install ros-electric-nxtall
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have problems with libcurl4-nss-dev versus libcurl4-openssl-dev (very
common if you use 32-bit libraries on a 64-bit system) it might help to use
apt-get instead of aptitude. Never say yes to something that asks to remove
almost everything on your system of course. :-)&lt;/p&gt;

&lt;p&gt;You can check with dpkg where actually everything is installed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dpkg -L ros-electric-nxt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And you will find out that most things are placed in &lt;code&gt;/opt/ros/electric/stacks/nxt&lt;/code&gt;&lt;/p&gt;

&lt;h2 id="start-ros--nxt"&gt;Start ROS &amp;amp; NXT&lt;/h2&gt;

&lt;p&gt;Now you can start roscore. If there is a message like “cannot ping itself”,
execute not only the command that you will see there, like “ping yourmachine”.
But also check if /etc/hosts actually contains a reference to the hostname for
127.0.0.1.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rosmake nxt_python
roscore
rosrun nxt_python touch_sensor_test.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you can check if it works by plugging the touch sensor in port 1 and you
should see “TOUCH: True” versus “TOUCH: False” on the command line. Nice! We
now can program our NXT robot without doing anything on the robot itself, but
just by sending commands from our laptop. A very non-autonomous situation of
course, but useful for prototyping.&lt;/p&gt;

&lt;h2 id="google-and-rosjava"&gt;Google and rosjava&lt;/h2&gt;

&lt;p&gt;Another reason we are interested in this route is because of &lt;code&gt;rosjava&lt;/code&gt;, which
does have a Google supported Android port. This means that a smartphone can
operate as a ROS node and - potentially - send commands to a NXT connected
through it through Bluetooth. The ROS code for this is not yet written. It has
already been demonstrated by the MindDroid app (and a derivative called
MindOpen from DoBots) that the NXT can be controlled from an Android phone.
As soon as such ROS code would be in place, it would be logical to write a ROS
wrapper for the distributed computing solution
&lt;a href="http://www.cs.vu.nl/ibis/index.html"&gt;Ibis&lt;/a&gt; for example. We can have large
groups of robots connected to the cloud where data processing occurs on the
robot and in the cloud, depending on the application demands.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/02/16/nxt-and-ros
                <guid>https://dobots.nl/2012/02/16/nxt-and-ros</guid>
                <pubdate>2012-02-16T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Moway Robot Disassembled</title>
                <description>
&lt;h1 id="moway-robot-disassembled"&gt;Moway Robot Disassembled&lt;/h1&gt;

&lt;p&gt;One of the Moway robots ended up on my desk, an ill fate for the poor thing,
but a nice moment for me! The Moway robots can be programmed with nice little
programs using the GUI delivered by Moway itself. We are of course interested
in writing code on a lower level, so we can just upload our C programs that we
have lying around. The Moway looks like a computer mouse, it has two round
wheels, some LEDs, a USB connector, a flat 3.7V LiPo battery and a Microchip
processor, the PIC18F87J50. You can see the robot disassembled in the picture.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/moway_disassembled_0.jpg" alt="Moway_disassembled" title="Moway_disassembled" /&gt;
&lt;img src="/attachments/moway_disassembled_1.jpg" alt="Moway_disassembled" title="Moway_disassembled" /&gt;&lt;/p&gt;

&lt;p&gt;As you will immediately recognize, the poor thing is on the operation table.
It turned out to be the case that using the Linux facility “picprog” which
should be able to upload a new hex file to the robot was indeed able to
program something on the robot. Regretfully, it turned out to be some
essential parts in its tiny little memory. The Moway was not accessible
anymore via USB. By the way, as always in Linux, the way to make something
recognized by the operating system is by adding a “udev” rule:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat /etc/udev/rules.d/52-moway.rules

# Moway rules
SUBSYSTEMS=="usb", ATTRS{idVendor}=="04d8", ATTRS{idProduct}=="003c",MODE="0666", SYMLINK+="ttyUSB%n"
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id="pickit"&gt;PICkit&lt;/h2&gt;

&lt;p&gt;&lt;img src="/attachments/moway_disassembled_2.jpg" alt="PICkit" title="PICkit" /&gt;Thanks to the superb support staff at Moway this was
not the end of the robot (yet)! The Moway has 5 pins at the right-side of the
PIC chip. The support stuff was kind enough to provide the pin layout to
revive the robot. From top to bottom: a.) VSS, b.) PGC, c.) PGD, d.) MCLR’,
e.) VDD. A PICkit 2 programmer can be bought for under a 50 bucks at
&lt;a href="http://www.microchip.com/Developmenttools/ProductDetails.aspx?PartNO=DV164121"&gt;microchip&lt;/a&gt;. The pin layout of the PICkit goes from
triangle on: 1.) MCLR’, 2.) VDD, 3.) VSS, 4.) PGD, 5.) PGC and the 6th doesn’t
need to be used. It is perfectly fine to power the chip from the LiPo battery,
so the VDD you don’t need to connect. You can download the software to use the
PICkit from microchip, the flash utility is called pk2cmd. One of the first
commands you likely want to try out, is pk2cmd -I -P. Don’t do this, it will
try to discover which device you attached to the PICkit 2 debugger, but in
that process it will use higher and higher voltages for the different PIC
families. It is much safer to use the device type if you know it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./pk2cmd -I -Ppic18f87j50
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Regretfully in my case the device does not respond properly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Device ID = 0000`
Revision  = 0000`
Device Name = &amp;lt;no device&amp;gt;`

Operation Succeeded`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This does not look good!&lt;/p&gt;

&lt;h2 id="logic-analyser"&gt;Logic analyser&lt;/h2&gt;

&lt;p&gt;Time to get another fancy gadget in the spotlight, the &lt;a href="http://www.saleae.com/Logic16"&gt;Logic Analyser from
Saleae&lt;/a&gt;. This neat small form factor device
can sample up to 16 channels with a frequency of up to 100 MHz. The results
can be seen in the following screenshot&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/moway_disassembled_3.png" alt="Logic analyser" title="Logic analyser" /&gt;&lt;/p&gt;

&lt;p&gt;You can see that all channels seem to do what they are supposed to do. The VDD
is on a level of 3.7V, and there seems some data clocked back and forth. The
only thing left to do now is to check if we need some additional circuitry to
be able to program with PICkit 2. There are some rumours that the PICkit 2
needs some pins like MCLR’ with a pull-up resistor (which might not be
necessary when using other programmers). So, we have to wait on the final word
from Spain. One thing is for sure, regardless if this specific robot can be
revived, the support from Moway has been excellent!&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/02/09/moway-robot-disassembled
                <guid>https://dobots.nl/2012/02/09/moway-robot-disassembled</guid>
                <pubdate>2012-02-09T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Robot Club Visit school</title>
                <description>
&lt;h1 id="robot-club-visit-school"&gt;Robot Club Visit school&lt;/h1&gt;

&lt;p&gt;Yesterday we welcomed a group of 25 kids from a local primary school. These 12
-year-olds had already made their first steps towards becoming robotics
engineers: in their school’s Robot Project they scavenged their parents’ old
cell phones, bicycle led lights and toothbrushes and created their own working
bristle bots!&lt;/p&gt;

&lt;p&gt;Now, they wanted to see what we were up to at DoBots.&lt;/p&gt;

&lt;h2 id="masterclass"&gt;Masterclass&lt;/h2&gt;

&lt;p&gt;The kids were introduced to our company in two sessions. In the meeting room,
our CTO Peet van Tooren gave them a quick masterclass on swarm robotics,
learning, evolution, and even self-organisation.&lt;/p&gt;

&lt;p&gt;To our surprise, the kids were really quick to understand even these
complicated topics, and the discussion soon became more philosophical: what is
a robot? Is a simulated robot really a robot, or just a computer programme?
What rules do we need to teach our robots? How can it become independent and
evolve? What does a robot like; can it have feelings?&lt;/p&gt;

&lt;h2 id="robot-lab"&gt;Robot Lab&lt;/h2&gt;

&lt;p&gt;But most fun was to be had in the attic, normally our break room, now
transformed into a kids’ robotics lab. One of our junior researchers, Bart van
Vliet, explained the basics of programming: teaching your robot rules, so it
behaves the way you want it.&lt;/p&gt;

&lt;p&gt;Luckily, we had some boxes of Lego Mindstorm building blocks lying around, a
great tool for researchers, but also a great starting kit for kids. The boys
and girls quickly started assembling their cars, bi- and tricycles, and got to
work getting them to react to sensor data from push, light and ultrasonic
sensors.&lt;/p&gt;

&lt;p&gt;Funny to see how some of them really think their project through, while others
just start building things and pressing buttons, with many great and
&lt;a href="http://www.flickr.com/photos/dobots/sets/72157629143184807/"&gt;surprising results&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id="demos"&gt;Demos&lt;/h2&gt;

&lt;p&gt;To top the day off, we showed the kids some of the more serious bots that we
are working with; the impressive smartphone-controlled Parrot, the
programmable Caterpillar. Also a bit hit: some of the cheap RC-helicopters
that they were allowed to smash into the ceiling themselves.&lt;/p&gt;

&lt;p&gt;After three hours, we said goodbye to our visitors. Perhaps we’ll welcome them
back in a year or ten, as junior researcher?&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/02/03/robot-club-visit
                <guid>https://dobots.nl/2012/02/03/robot-club-visit</guid>
                <pubdate>2012-02-03T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>The WowWee JoeBot</title>
                <description>
&lt;h1 id="the-wowwee-joebot"&gt;The WowWee JoeBot&lt;/h1&gt;

&lt;p&gt;The WowWee JoeBot is a humanoid robot that can walk, talk, sing and shoot. It
makes cheeky remarks and it can play simple games. But you can’t program it
and there doesn’t seem to be a hack for it (yet) either.&lt;/p&gt;

&lt;h2 id="features"&gt;Features&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Joebot has preprogrammed voice commands.&lt;/li&gt;
  &lt;li&gt;His hands function as buttons to invoke dancing, walking or singing behaviour.&lt;/li&gt;
  &lt;li&gt;He can also play back beats that you just played for him.&lt;/li&gt;
  &lt;li&gt;When walking, he can avoid obstacles.&lt;/li&gt;
  &lt;li&gt;By setting joebot in different poses you can program a dance.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/qOMlniXUhc0?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;p&gt;Overall Joebot is unable to hold your attention for longer than half an hour.
Even as a kids toy it is rather boring. The possibilities to create something
or to use your own imagination are extremely limited.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/01/19/review-wowwee-joebot
                <guid>https://dobots.nl/2012/01/19/review-wowwee-joebot</guid>
                <pubdate>2012-01-19T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Review of Arexx Caterpillar</title>
                <description>
&lt;h1 id="review-of-arexx-caterpillar"&gt;Review of Arexx Caterpillar&lt;/h1&gt;

&lt;p&gt;The caterpillar is an excellent robot to start off with in robotics. It has
servos and sensors, you can change the hardware and you can program it
yourself.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/ttwfjcpefDs?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;p&gt;&lt;img src="/attachments/AREXX_Caterpillar.jpg" alt="Assembled caterpillar" title="Such sensors, wow, very legs" /&gt;&lt;/p&gt;

&lt;h2 id="hardware"&gt;Hardware&lt;/h2&gt;

&lt;p&gt;The package contains many parts which you have to assemble yourself. This
consists mostly out of screwing things together, a bit of soldering and
connecting cables. The instructions are easy to follow (pay attention to which
screws you use though). The soldering was a bit harder, especially the touch
sensors have to be soldered exactly. Connecting the cables was a bit of a
guessing game, the manual wasn’t very clear about it. The total assembly took
about a day.&lt;/p&gt;

&lt;h2 id="software"&gt;Software&lt;/h2&gt;

&lt;p&gt;The included software includes a nice GUI to connect to the robot. You can
read out values, use the commandline and upload, start and stop programs.
Unfortunately the GUI did not work on my system (ubuntu 11.10), so I used
windows. I’m sure though that there are other programs to connect to the
robot, since it uses UART.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/AREXX_Caterpillar_gui" alt="Caterpillar GUI" title="Caterpillar GUI" /&gt;&lt;/p&gt;

&lt;p&gt;The robot comes with some example programs, with source code, so it’s easy to
modify and start making your own programs. At first, it is a bit tricky to
combine the servo commands in such a way that you get a smooth movement, but
later you do this with ease. In the video you can see my own programmed
sidewards movement. The biggest problem I had was that the servos are a bit
weak. Since the board has some room for extra sensors, you can add some and
make it more autonomous. Or you could even completely change the hardware and
build a completely different robot (a walking robot for example).&lt;/p&gt;

&lt;h2 id="specs"&gt;Specs&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ATmega16A processor: 16 MHz, 16 KB flash, 512 B EEPROM&lt;/li&gt;
  &lt;li&gt;Actuators: 8 servos, 4 LEDs, beep speaker&lt;/li&gt;
  &lt;li&gt;Sensors: IR sensor, 3 touch sensors, 2 roll sensors&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="links"&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href="http://www.arexx.com/caterpillar/html/en/index.htm"&gt;Official website&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>https://dobots.nl/2012/01/09/review-of-arexx-caterpillar
                <guid>https://dobots.nl/2012/01/09/review-of-arexx-caterpillar</guid>
                <pubdate>2012-01-09T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Brookstone Rover AC13 United States</title>
                <description>
&lt;h1 id="brookstone-rover-ac13"&gt;Brookstone Rover AC13&lt;/h1&gt;

&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;img src="/attachments/Brookstone_Rover_AC13_200.png" alt="Brookstone Rover AC13" title="Brookstone Rover AC13" /&gt;&lt;/p&gt;

&lt;p&gt;The Brookstone® Rover is sold exclusively (for $99,-)
in the United States and can be controlled by ipod, iphone, or other Apple
device. It has a camera at front, a microphone, and an IR led which can be
used to “see in the dark”. It runs on 6 AA batteries which is directly the
only “minus” of the robot. Different from for example the iRobot Create®,
recharging the thing can only be done manually. If you want to stalk your cats
if you are on holidays, you will have to turn on the robot before you leave
and it will have run out of power before it’s time to check if your neighbour
did do his job. But, for now, it’s a perfect toy!&lt;/p&gt;

&lt;h2 id="disassembly"&gt;Disassembly&lt;/h2&gt;

&lt;p&gt;Last month, we got one from under the christmas tree, and hence, a challenge
is born! How to control this interesting robot from an android device. First
we use &lt;strong&gt;nmap&lt;/strong&gt; to see which ports are open. The result: one TCP port open at
80, and two UDP ports at 67 and 10000. We connect to port 80 and see that
there is a HTTP page served which requires username and password. After
hitting ESC for three times, it shows a misspelled “Device Embeded Web UI
Version” phrase, just as Foscam IP cameras do! Probably the same developers
have programmed the firmware for the Rover! Time to disassemble the robot to
check if this is true.&lt;/p&gt;

&lt;p&gt;&lt;img src="/attachments/Brookstone_Rover_AC13_0.jpeg" alt="Disassembled Rover" title="Disassembled Rover" /&gt;
&lt;img src="/attachments/Brookstone_Rover_AC13_1.jpeg" alt="Disassembled Rover" title="Disassembled Rover" /&gt;&lt;/p&gt;

&lt;p&gt;Yes! Remarkably enough there is a similar chip which reads spansion
s29gl032n90tfi040 (Flash) as in the Foscam line of IP cameras. The board looks
almost the same as the one at the
&lt;a href="http://www.computersolutions.cn/blog/2010/04/ip-cam-hacking-pt3/"&gt;computersolutions.cn&lt;/a&gt; blog. Checking the CGI API document from Foscam reveals that the Rover
responds to document cgi scripts such as “decoder_control.cgi”,
“camera_control.cgi”, “wifi_scan”, “get_params” and “set_params”.&lt;/p&gt;

&lt;h2 id="listening"&gt;Listening&lt;/h2&gt;

&lt;p&gt;We connect an iphone to the robot via the default ad-hoc network started up by
the rover. And we connect a mini laptop to it at the same time, now running
“sudo tcpdump -XXvvvs0 -i wlan0 port 80” reveals the network traffic. And we
just see that the iphone logs into the browser by issuing an HTTP GET request
“http://192.168.1.100/check_user.cgi?user=AC13&amp;amp;pwd=AC13”. That was easy, we
already know the username and password: AC13. Now, to quickly check if there
are (unencrypted) images going through the network we run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;"sudo driftnet -i wlan0" 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and yes, it displays the sequence of images transmitted by the robot.
So far, so good!&lt;/p&gt;

&lt;p&gt;The developers do not seem to use the cgi scripts at all. Just as on the
Foscam IP cameras, there is a binary protocol used to command the camera to
start streaming. It starts with “MO_O” or “MO_V” and then there is a sequence
of bytes which represents a command. After careful examining this binary
stream we extracted the commands going from the iphone towards the robot. A
python script that gets you one image is published on the
&lt;a href="http://androidcommunity.com/forums/f44/brookstone-rover-app-81730/index2.html"&gt;androidcommunity&lt;/a&gt; forum. There were nasty details such as the necessity
to close the socket and open the socket at the same port after authentication.&lt;/p&gt;

&lt;h2 id="remote-control"&gt;Remote control!&lt;/h2&gt;

&lt;p&gt;To actually be able to remotely control the robot we need to do more. Google
provided a nice SDK to build your own apps, integrated with Eclipse, debugging
by connecting your phone via USB. Almost everything works out of the box. We
need to transform our python code to Java to use it on Android. We wanted not
only to be able to stream images, but also to control the robot by moving the
cell phone. The result can be found in the Android market, as the &lt;a href="https://market.android.com/details?id=org.almende.roveropen"&gt;RoverOpen app&lt;/a&gt;. Contrary to
most apps, this one is completely open source. The code can be found at the
server of our mother company, &lt;a href="https://dev.almende.com/repositories/browse/roveropen"&gt;Almende Redmine&lt;/a&gt;. Feel free to
contribute to the code to make this app better! Enjoy!&lt;/p&gt;

</description>
                <link>https://dobots.nl/2012/01/09/brookstone-rover-ac13
                <guid>https://dobots.nl/2012/01/09/brookstone-rover-ac13</guid>
                <pubdate>2012-01-09T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Review of Meccano SPYKEE spy robot</title>
                <description>
&lt;h1 id="review-of-meccano-spykee-spy-robot"&gt;Review of Meccano SPYKEE spy robot&lt;/h1&gt;

&lt;p&gt;Introduced a couple of years ago, Spykee was received warmly. Especially the
promise of Meccano that the robot would be open-source was for many robot
enthusiasts reason to keep their hopes up. Though the source code is released
now, initially it was not and this is probably the reason that the Spykee
community never grew very big. This is a pity, because the robot has many
possibilities, as described in the modding section. I will first discuss the
out-of-the-box features and experience.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/-GwdixdD6ms?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;p&gt;Meccano has a long history of construction toys, and Spykee is no exception.
On delivery, you will find a box full of small plastic parts and a manual.
Three possible models are described in the manual, although the second and
third seem to be added as an afterthought. Also, while it is possible to
create your own model, do not expect too much from it, as many parts are
especially tailored towards the standard model. Nevertheless, this standard
model is good and will take about an afternoon to complete. The construction
is mostly comfortable and fun. Note that all the construction work is just for
decoration though; it is also possible to use the robot by just connecting the
camera to the combined battery and wheels.&lt;/p&gt;

&lt;p&gt;After charging the battery for a long time you can finally try Spykee!
Installing the software should be easy, but, due to the not-open- source-
after-all debacle, there is no linux client. Only Mac and Windows (and Iphones
by an independent party by now) are supported. On first use you will have to
connect with the robot using an ad-hoc wifi connection and launch the client.
This will allow you to see the video from the webcam and drive around with
Spykee: very much fun! There is also some other features like playing mp3’s, a
flashlight, and so on. The interface does the job, it’s not bad.&lt;/p&gt;

&lt;p&gt;The ad-hoc network mode is easy enough. The next step is to connect Spykee to
a router and access it from your local network or even the internet. The
configuration of this setup is a bit more complicated though and in many cases
will require tweaking of some router settings. Also, do know where the reset
button on the robot is located; I had to restore the factory settings a number
of times… However, it’s nothing a robot enthusiast or a smart kid couldn’t
handle.&lt;/p&gt;

&lt;p&gt;The hardware of the robot is very nice for a toy, but in case you are looking
for a really useful robot, it might not suffice. The two tracks for example
can turn the robot in place on the plus side; but the suspension is so basic
that a small bump can already block Spykee’s way. The battery is good enough
to make the robot work for about an hour, but the charging mechanism is not
very good and may quickly reduce the battery life if not fixed by a
modification. The robot has an autodocking feature when it has to charge, but
it is barely working. The resolution of the camera is okay for navigation, but
don’t expect to take nice pictures. All in all, out of the box the hardware
capabilities are not very impressive. Still, for 200 euro it offers a robot
that is a great toy and for example even good enough to use in an office
environment. Moreover, it is possible to make your own modifications to it and
make it a really stunning robot.&lt;/p&gt;

&lt;p&gt;By the way, Meccano has introduced some other Spykee-like robots by now. Check
them out if you’re interested in a robot that can change the channels on your
television or that can interface with phones and Ipods. A direct competitor of
Meccano’s Spykee is the WowWee Rovio, which seems to have about the same
possibilities out of the box. Making modifications to a Rovio robot may be
harder, but do your own research to find this out.&lt;/p&gt;

&lt;h2 id="modding"&gt;Modding&lt;/h2&gt;

&lt;p&gt;&lt;img src="/attachments/meccano_spykee_400.jpg" alt="Meccano Spykee" title="Meccano Spykee" class="float-right" /&gt;&lt;/p&gt;

&lt;p&gt;Even though Spykee turned out to be less friendly to modders than anticipated, there are many possibilities by now:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Writing your own software to control the robot. This is something that a couple of people did already, notably leading to SpykeeFX (found at the devmods site) and ISpykee. Unfortunately, SpykeeFX is not open- source. However, ISpykee is, so at least part of the communication protocol is available and reverse engineering is not obligatory.&lt;/li&gt;
  &lt;li&gt;Probably a whole lot easier is to replace the firmware on the robot with something that is more to your liking. As the toolchain for making the firmware became available (found at the official website), a number of replacement firmwares were created. One of the additions is for example the enabling of telnet on the robot. The URBI firmware by the French robotics company Gostai seems to be the most attractive: together with the URBI library (in C++) it should be an easy way to write your own computer programs to control Spykee. It supports track speed setting, led on/off (but only in pairs), camera readout and all other features that you’d expect.&lt;/li&gt;
  &lt;li&gt;Add hardware! Most people that add actuators use the LEDs to control them, either directly or via a controller (eg a Arduino board). One of the most attractive additions is an actuator to move the head up and down, in order to see where the robot is walking (although it is probably also possible to achieve this by changing the body). In theory it is possible to add whatever actuator you want though! It should also be possible to use the USB and serial connectors. This can be useful if you would like to add sensors, but probably will be quite involved on the software side. Adding a USB disk is easy though, see the devmods wiki.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="specs"&gt;Specs&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Price: ± 200 euro (It is still in production, but sometimes a cheaper offer can be found)&lt;/li&gt;
  &lt;li&gt;Computer: 200 MHz ARM9 processor, 32 Mb internal memory, 4 Mb flash memory for the firmware, WiFi connection to the rest of the world (either ad-hoc or through a router; note that the router name should not contain spaces because of some weird bug in Spykee), USB host (usually has the camera on it), serial port (which requires serious hacking to use)&lt;/li&gt;
  &lt;li&gt;Sensors: infrared (for docking, not standard available for readout), battery charge status, USB webcam, microphone&lt;/li&gt;
  &lt;li&gt;Actuators: 2 tracks, speaker, 4 leds (red and green) and 1 flash led (white)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="links"&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Official website: &lt;a href="http://www.spykeeworld.com/"&gt;http://www.spykeeworld.com/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Source code: &lt;a href="http://www.spykeeworld.com/spykee/UK/freeSoftware.html"&gt; http://www.spykeeworld.com/spykee/UK/freeSoftware.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;A CNET video review showcasing different models in action: &lt;a href="http://cnettv.cnet.com/?type=externalVideoId&amp;amp;value=6827518"&gt;http://cnettv.cnet.com/?type=externalVideoId&amp;amp;value;=6827518&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Urbi software by Gostai: &lt;a href="http://www.urbiforge.org/index.php/Robots/Spykee"&gt;http://www.urbiforge.org/index.php/Robots/Spykee&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Modding community: &lt;a href="http://www.spykee.devmods.org/"&gt;http://www.spykee.devmods.org/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The wiki of the above with a lot of information on the hard- and software: &lt;a href="http://spykee.duskofsolace.com/"&gt;http://spykee.duskofsolace.com/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The ISpykee website: &lt;a href="http://ispykee.com/"&gt;http://ispykee.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>https://dobots.nl/2011/12/21/review-of-meccano-spykee-spy-robot
                <guid>https://dobots.nl/2011/12/21/review-of-meccano-spykee-spy-robot</guid>
                <pubdate>2011-12-21T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Robot reviews</title>
                <description>
&lt;h1 id="robot-reviews"&gt;Robot reviews&lt;/h1&gt;

&lt;p&gt;We started to review some commercially off the shelf toy robots. Here are the
videos, later we will add more detailed information.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/yvy37u0sxs8?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/TS7QFA2zs6Q?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/ttwfjcpefDs?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;
</description>
                <link>https://dobots.nl/2011/10/21/robot-reviews
                <guid>https://dobots.nl/2011/10/21/robot-reviews</guid>
                <pubdate>2011-10-21T00:00:00+00:00</pubdate>
        </item>

        <item>
                <title>Research with the Surveyor robot</title>
                <description>
&lt;h1 id="research-with-the-surveyor-robot"&gt;Research with the Surveyor robot&lt;/h1&gt;

&lt;p&gt;The Surveyor robot is a moderately cheap robot with ad-hoc WIFI connection and a camera
on-board. We mounted a laser on front which projects a red line in the forward
direction. The default lasers are just point lasers, and do not project a
line. We subsequently test our software using camera and laser information
together (so-called sensor fusion).&lt;/p&gt;

&lt;p&gt;Normally the robot will not be able to make a distiction between a fake and a
real power outlet. However, with the additional distance information from the
laser, it can! You will hear the robot respond to the real power outlet.&lt;/p&gt;

&lt;div class="videowrapper"&gt;
		&lt;iframe width="560" height="420" src="//www.youtube.com/embed/ogFNDs1SgwY?color=white&amp;amp;theme=light"&gt;&lt;/iframe&gt;
		&lt;/div&gt;

&lt;p&gt;This research robot is available in our webshop because it is really good for
research that involves a robot camera. The robot contains a Blackfin
microcontroller which can run uCLinux. However, the person responsible for
this left the Surveyor Corporation, and so uCLinux support faded away. Still,
there is a lot of documentation online and it is not hard to flash the robot
with this OS. The WIFI on the robot is via a Lantronix chip. This solution
works fine for connecting to the robot wirelessly, however do not assume that
it will be easy to have two robots communicate with each other. Summarized:
buy this robot if you want to do image processing experiments on a single
robot.&lt;/p&gt;

</description>
                <link>https://dobots.nl/2011/10/20/research-with-the-surveyor-robot
                <guid>https://dobots.nl/2011/10/20/research-with-the-surveyor-robot</guid>
                <pubdate>2011-10-20T00:00:00+00:00</pubdate>
        </item>


</channel>
</rss>
